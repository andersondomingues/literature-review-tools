"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication_Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","License","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"FDT 2.0: Improving scalability of the fuzzy decision tree induction tool - integrating database storage","E. E. A. Durham; X. Yu; R. W. Harrison","Department of Computer Science, Georgia State University, Atlanta, USA","2014 IEEE Symposium on Computational Intelligence in Healthcare and e-health (CICARE)","20150115","2014","","","187","190","Effective machine-learning handles large datasets efficiently. One key feature of handling large data is the use of databases such as MySQL. The freeware fuzzy decision tree induction tool, FDT, is a scalable supervised-classification software tool implementing fuzzy decision trees. It is based on an optimized fuzzy ID3 (FID3) algorithm. FDT 2.0 improves upon FDT 1.0 by bridging the gap between data science and data engineering: it combines a robust decisioning tool with data retention for future decisions, so that the tool does not need to be recalibrated from scratch every time a new decision is required. In this paper we briefly review the analytical capabilities of the freeware FDT tool and its major features and functionalities; examples of large biological datasets from HIV, microRNAs and sRNAs are included. This work shows how to integrate fuzzy decision algorithms with modern database technology. In addition, we show that integrating the fuzzy decision tree induction tool with database storage allows for optimal user satisfaction in today's Data Analytics world.","","Electronic:978-1-4799-4527-6; POD:978-1-4799-4526-9","10.1109/CICARE.2014.7007853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7007853","Big Data;HIV protease;drug resistance prediction;fuzzy ID3;fuzzy logic","Accuracy;Algorithm design and analysis;Databases;Decision trees;Educational institutions;Training;Training data","database management systems;decision trees;fuzzy set theory;software tools;storage management","FDT 2.0;FID3 algorithm;HIV;MySQL;data analytics world;data engineering;data retention;data science;database storage;database technology;freeware FDT tool;freeware fuzzy decision tree induction tool;fuzzy decision algorithms;large biological datasets;large data handling;machine learning;microRNA;optimal user satisfaction;optimized fuzzy ID3;robust decisioning tool;sRNA;scalable supervised-classification software tool","","1","","6","","","","9-12 Dec. 2014","","IEEE","IEEE Conferences"
"High-performance geometric algorithms for sparse computation in big data analytics","P. Baumann; D. S. Hochbaum; Q. Spaen","Department of Business Administration, University of Bern, Switzerland","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","546","555","Several leading supervised and unsupervised machine learning algorithms require as input similarities between objects in a data set. Since the number of pairwise similarities grows quadratically with the size of the data set, it is computationally prohibitive to compute all pairwise similarities for large-scale data sets. The recently introduced methodology of “sparse computation” resolves this issue by computing only the relevant similarities instead of all pairwise similarities. To identify the relevant similarities, sparse computation efficiently projects the data onto a low-dimensional space where a similarity is considered relevant if the corresponding objects are close in this space. The relevant similarities are then computed in the original space. Sparse computation identifies close pairs by partitioning the low-dimensional space into grid blocks, and considering objects close if they fall in the same or adjacent grid blocks. This guarantees that all pairs of objects that are within a specified L<sub>∞</sub> distance are identified as well as some pairs that are within twice this distance. For very large data sets, sparse computation can have high runtime due to the enumeration of pairs of adjacent blocks. We propose here new geometric algorithms that eliminate the need to enumerate adjacent blocks. Our empirical results on data sets with up to 10 million objects show that the new algorithms achieve a significant reduction in runtime. The algorithms have applications in large-scale computational geometry and (approximate) nearest neighbor search. Python implementations of the proposed algorithms are publicly available.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8257970","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8257970","Big data;computational geometry;similarity-based machine learning;sparse computation;sparsification","Aerospace electronics;Approximation algorithms;Machine learning algorithms;Nearest neighbor searches;Object recognition;Runtime;Sparse matrices","Big Data;computational geometry;data analysis;parallel algorithms;pattern classification;pattern clustering;unsupervised learning","L<sub>∞</sub> distance;Python implementation;adjacent blocks;big data analytics;data set;high-performance geometric algorithms;input similarities;large-scale computational geometry;large-scale data sets;leading supervised machine learning algorithms;low-dimensional space;nearest neighbor search;pairwise similarities;sparse computation resolves this issue;unsupervised machine learning algorithms","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"ZenLDA: Large-scale topic model training on distributed data-parallel platform","B. Zhao; H. Zhou; G. Li; Y. Huang","National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China and Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing 210023, China","Big Data Mining and Analytics","20180125","2018","1","1","57","74","Recently, topic models such as Latent Dirichlet Allocation (LDA) have been widely used in large-scale web mining. Many large-scale LDA training systems have been developed, which usually prefer a customized design from top to bottom with sophisticated synchronization support. We propose an LDA training system named ZenLDA, which follows a generalized design for the distributed data-parallel platform. The novelty of ZenLDA consists of three main aspects: (1) it converts the commonly used serial Collapsed Gibbs Sampling (CGS) inference algorithm to a Monte-Carlo Collapsed Bayesian (MCCB) estimation method, which is embarrassingly parallel; (2) it decomposes the LDA inference formula into parts that can be sampled more efficiently to reduce computation complexity; (3) it proposes a distributed LDA training framework, which represents the corpus as a directed graph with the parameters annotated as corresponding vertices and implements ZenLDA and other well-known inference methods based on Spark. Experimental results indicate that MCCB converges with accuracy similar to that of CGS, while running much faster. On top of MCCB, the ZenLDA formula decomposition achieved the fastest speed among other well-known inference methods. ZenLDA also showed good scalability when dealing with large-scale topic models on the data-parallel platform. Overall, ZenLDA could achieve comparable and even better computing performance with state-of-the-art dedicated systems.","","","10.26599/BDMA.2018.9020006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8268736","Monte-Carlo;collapsed Gibbs sampling;graph computing;large-scale machine learning;latent Dirichlet allocation","Bayes methods;Big Data;Computational modeling;Data mining;Inference algorithms;Sparks;Training","","","","","","","","","","March 2018","","TUP","TUP Journals & Magazines"
"Conceptual design of proactive SONs based on the Big Data framework for 5G cellular networks: A novel Machine Learning perspective facilitating a shift in the SON paradigm","B. Keshavamurthy; M. Ashraf","Department of Electronics and Communication, BMS College of Engineering, Bangalore, India","2016 International Conference System Modeling & Advancement in Research Trends (SMART)","20170412","2016","","","298","304","Self-Organizing Networks (SONs) are being researched extensively in the existing 3G and 4G landscape primarily to facilitate a convenient yet cost-effective approach in the configuration, optimization and troubleshooting of networks. However, the existing SONs will be no match for the operational complexity of the envisioned 5G networks. The promise of 5G revolves around the premise of infinite capacity and zero latency. 5G networks are set for commercial availability by 2020 and these networks will be part of a flexible and dynamic telecom ecosystem supporting cross-domain integration and multi-RAT environments. Network Densification, Network Function Virtualization, Flexible spectrum allocation, E2E security and Massive MTC are a few of the features promised by 5G networks. With this assortment of numerous technologies, it is obvious that SONs have to evolve beyond the existing reactive paradigm without which the maintenance and management of 5G networks would prove to be a herculean task. Hence, we propose a novel Proactive SON methodology based on the Big Data framework to enable the shift in the SON paradigm. In this article we present a comprehensive Big-Data based SON framework involving innovative Machine Learning techniques which would cater to scalability and programmability of 5G networks with respect to availability, reliability, speed, capacity, security and latency.","","Electronic:978-1-5090-3543-4; POD:978-1-5090-3544-1","10.1109/SYSMART.2016.7894539","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7894539","Bitmaps;COD;Handover;Heuristic Analysis;KPIs;LTE;MTC;Regression;Root Cause Analysis;SONs;UMTS","3GPP;5G mobile communication;Automation;Big Data;Cellular networks;Complexity theory;Maintenance engineering","5G mobile communication;Big Data;cellular radio;learning (artificial intelligence);radio access networks;telecommunication computing;telecommunication network management;telecommunication network reliability;telecommunication security;virtualisation","3G landscape;4G landscape;5G cellular networks;Big-Data based SON framework;E2E security;conceptual proactive SON design;cross-domain integration;flexible dynamic telecom ecosystem;flexible spectrum allocation;machine learning;massive MTC;multiRAT environments;network densification;network function virtualization;self-organizing networks","","","","","","","","25-27 Nov. 2016","","IEEE","IEEE Conferences"
"Big data caching for networking: moving from cloud to edge","E. Zeydan; E. Bastug; M. Bennis; M. A. Kader; I. A. Karatepe; A. S. Er; M. Debbah","Turk Telekom","IEEE Communications Magazine","20160916","2016","54","9","36","42","In order to cope with the relentless data tsunami in 5G wireless networks, current approaches such as acquiring new spectrum, deploying more BSs, and increasing nodes in mobile packet core networks are becoming ineffective in terms of scalability, cost and flexibility. In this regard, context- aware 5G networks with edge/cloud computing and exploitation of big data analytics can yield significant gains for mobile operators. In this article, proactive content caching in 5G wireless networks is investigated in which a big-data-enabled architecture is proposed. In this practical architecture, a vast amount of data is harnessed for content popularity estimation, and strategic contents are cached at BSs to achieve higher user satisfaction and backhaul offloading. To validate the proposed solution, we consider a real-world case study where several hours worth of mobile data traffic is collected from a major telecom operator in Turkey, and big-data-enabled analysis is carried out, leveraging tools from machine learning. Based on the available information and storage capacity, numerical studies show that several gains are achieved in terms of both user satisfaction and backhaul offloading. For example, in the case of 16 BSs with 30 percent of content ratings and 13 GB storage size (78 percent of total library size), proactive caching yields 100 percent user satisfaction and offloads 98 percent of the backhaul.","0163-6804;01636804","","10.1109/MCOM.2016.7565185","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7565185","","5G mobile communication;Big data;Computer architecture;Mobile computing;Wireless networks","5G mobile communication;Big Data;cloud computing;learning (artificial intelligence);telecommunication computing;telecommunication traffic","5G wireless networks;Big data analytics;Big data caching;Turkey;backhaul offloading;cloud to edge;context- aware 5G networks;data tsunami;edge-cloud computing;machine learning;mobile data traffic;mobile packet core networks","","13","","","","","","September 2016","","IEEE","IEEE Journals & Magazines"
"Giving voice to office customers: Best practices in how office handles verbatim text feedback","M. Bentley; S. Batra","Office Product Group, Microsoft, Redmond, WA, USA","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3826","3832","Microsoft Office users submit hundreds of thousands of pieces of verbatim feedback per month. How can an engineer or manager in Office find the signal in this data to make business decisions? This paper presents an overview of the Office Customer Voice (OCV) system. OCV combines classification, on-demand clustering and other machine learning techniques with a rich web UI to solve this problem. In this paper, we describe the different types of feedback received. Next, we outline the architecture used to build OCV. We then detail the text processing, classification and clustering done to reason on the data. Finally, we present challenges, future plans, and best practices that may be relevant to other teams analyzing customer feedback. We argue that this multi-pronged approach to handling customer feedback presents a pattern that other organizations can use to mature their handling of customer feedback.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7841055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841055","Customer feedback;classification;clustering;machine learning;natural language processing;nlp","Best practices;Cascading style sheets;Customer services;Data models;Measurement;Software;Training data","learning (artificial intelligence);pattern classification;text analysis","Microsoft Office;OCV system;Office Customer Voice system;customer feedback analysis;machine learning technique;verbatim text feedback","","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conferences"
"Can We Group Similar Amazon Reviews: A Case Study with Different Clustering Algorithms","C. Fry; S. Manna","Comput. Sci. Dept., California State Polytech. Univ., Pomona, Pomona, CA, USA","2016 IEEE Tenth International Conference on Semantic Computing (ICSC)","20160324","2016","","","374","377","The amount of unstructured text data available is growing exponentially due to the proliferation of digital information such as emails, text messages, blogs, social media posts, and product reviews. For users of e-commerce websites such as Amazon, navigating thousands of reviews before buying a product can be a daunting task. Unsupervised machine learning techniques can be used to automatically analyze preprocessed data from these websites in order to provide consumers with an improved user experience before purchasing a product. In this work, we leverage two flat clustering algorithms on Amazon review data: K-means and Peak-searching to perform clustering of product reviews based on topic. The experimental results show that K-means clustering performs better than Peak-searching clustering in terms of grouping similar reviews based on topics.","","Electronic:978-1-5090-0662-5; POD:978-1-5090-0663-2","10.1109/ICSC.2016.71","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7439368","clustering;k-means;peak;reviews","Algorithm design and analysis;Approximation algorithms;Blogs;Clustering algorithms;Computer science;Electronic mail;Measurement","Web sites;electronic commerce;information retrieval;pattern clustering;purchasing;retail data processing;reviews;text analysis;unsupervised learning","Amazon review data;K-means clustering;digital information proliferation;e-commerce Websites;improved user experience;peak-searching clustering;product purchasing;product review clustering algorithms;similar Amazon review grouping;unstructured text data;unsupervised machine learning techniques","","","","12","","","","4-6 Feb. 2016","","IEEE","IEEE Conferences"
"Biclustering Learning of Trading Rules","Q. Huang; T. Wang; D. Tao; X. Li","School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China","IEEE Transactions on Cybernetics","20170520","2015","45","10","2287","2298","Technical analysis with numerous indicators and patterns has been regarded as important evidence for making trading decisions in financial markets. However, it is extremely difficult for investors to find useful trading rules based on numerous technical indicators. This paper innovatively proposes the use of biclustering mining to discover effective technical trading patterns that contain a combination of indicators from historical financial data series. This is the first attempt to use biclustering algorithm on trading data. The mined patterns are regarded as trading rules and can be classified as three trading actions (i.e., the buy, the sell, and no-action signals) with respect to the maximum support. A modified K nearest neighborhood (K-NN) method is applied to classification of trading days in the testing period. The proposed method [called biclustering algorithm and the K nearest neighbor (BIC-K-NN)] was implemented on four historical datasets and the average performance was compared with the conventional buy-and-hold strategy and three previously reported intelligent trading systems. Experimental results demonstrate that the proposed trading system outperforms its counterparts and will be useful for investment in various financial markets.","2168-2267;21682267","","10.1109/TCYB.2014.2370063","Australian Research Council Linkage Project; Key Research Program of the Chinese Academy of Sciences; National Basic Research Program of China (973 Program); Projects of Innovative Science and Technology, Department of Education, Guangdong Province; 10.13039/501100001809 - National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6975065","Biclustering;machine learning;technical analysis;trading rules","Artificial neural networks;Clustering algorithms;Data mining;Evolution (biology);Indexes;Market research;Stock markets","data mining;financial data processing;investment;learning (artificial intelligence);pattern classification;pattern clustering;stock markets","BIC-K-NN;K-NN method;biclustering learning;biclustering mining;financial markets;historical financial data series;investment;modified K nearest neighborhood method;pattern mining;technical analysis;technical indicators;technical trading patterns;trading day classification;trading decision making;trading rules","","6","","41","","","20141204","Oct. 2015","","IEEE","IEEE Journals & Magazines"
"Sketched Subspace Clustering","P. A. Traganitis; G. B. Giannakis","Department of Electrical and Computer Engineering and the Digital Technology Center, University of Minnesota, Minneapolis, MN, USA","IEEE Transactions on Signal Processing","20180214","2018","66","7","1663","1675","The immense amount of daily generated and communicated data presents unique challenges in their processing. Clustering, the grouping of data without the presence of ground-truth labels, is an important tool for drawing inferences from data. Subspace clustering (SC) is a relatively recent method that is able to successfully classify nonlinearly separable data in a multitude of settings. In spite of their high clustering accuracy, SC methods incur prohibitively high computational complexity when processing large volumes of high-dimensional data. Inspired by random sketching approaches for dimensionality reduction, the present paper introduces a randomized scheme for SC, termed Sketch-SC, tailored for large volumes of high-dimensional data. Sketch-SC accelerates the computationally heavy parts of state-of-the-art SC approaches by compressing the data matrix across both dimensions using random projections, thus enabling fast and accurate large-scale SC. Performance analysis as well as extensive numerical tests on real data corroborate the potential of Sketch-SC and its competitive performance relative to state-of-the-art scalable SC approaches.","1053-587X;1053587X","","10.1109/TSP.2017.2781649","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170271","Subspace clustering;big data;random projections;sketching","Clustering algorithms;Computational complexity;Machine learning algorithms;Matrix decomposition;Optimization;Signal processing algorithms","computational complexity;data handling;pattern classification;pattern clustering","Sketch-SC;computational complexity;data matrix;dimensionality reduction;random sketching approaches;sketched subspace clustering","","","","","","","20171208","April1, 1 2018","","IEEE","IEEE Journals & Magazines"
"Assessing short-term social media marketing outreach of a healthcare organization using machine learning","N. Melethadathil; B. Nair; S. Diwakar; S. Pazhanivelu","Amrita School of Biotechnology Amrita Vishwa Vidyapeetham (Amrita University) Clappana P.O., Kollam, Kerala 690525, India","2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","20171204","2017","","","387","392","Social networking portals serve as an ideal platform for a person or an organization, to accomplish self-presentation and self-enhancement goals there by to understand their social relevance and hence, there have been many studies attempting to identify the relationship between different aspects of social media articles. Machine learning methods play a critical role in social media data analytics. This study evaluates the efficiency of predictive analytics with social media data of a private healthcare institution using classification and clustering algorithms. In this study, we also investigated the influence of system specific feature set and user generated features of social media articles to identify relevant data mining algorithms that are suitable for identifying knowledge-related patterns in dataset. Among the classification methods it is found that Bayesian algorithm performs better compared to other classification techniques. K-Means and Filtered cluster among the clustering techniques have better predictive analytics efficiency compared to other algorithms. Identifying the predictive analytics efficacy of these algorithms helps healthcare institution to build a model with most appropriate characteristics of social media articles.","","Electronic:978-1-5090-6367-3; POD:978-1-5090-6368-0; USB:978-1-5090-6366-6","10.1109/ICACCI.2017.8125871","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8125871","","Algorithm design and analysis;Classification algorithms;Clustering algorithms;Data mining;Decision trees;Prediction algorithms;Social network services","Bayes methods;data analysis;data mining;health care;learning (artificial intelligence);marketing data processing;pattern clustering;portals;social networking (online)","Bayesian algorithm;classification;clustering algorithms;data mining algorithms;healthcare organization;machine learning methods;predictive analytics efficiency;self-enhancement goals;self-presentation goals;short-term social media marketing outreach;social media articles;social media data analytics;social networking portals;social relevance","","","","","","","","13-16 Sept. 2017","","IEEE","IEEE Conferences"
"Diagnosis model of low voltage generation based on large data","C. Xueting; L. Huanming; Z. Huiping; S. Shuyong; W. Tan; S. Yuxin; S. Zeyuan","State Grid Shanxi Electric Power Research Institute, Taiyuan, China","2017 IEEE Transportation Electrification Conference and Expo, Asia-Pacific (ITEC Asia-Pacific)","20171026","2017","","","1","6","Voltage is one of the measurement standard of power quality. The issue of low voltage in distribution network seriously affect the social economic development and people's life. Consequently, it would optimize low voltage investment program, clear direction of low voltage investment and provide decision support for the management of low voltage. Canopy- Kmeans is one of the widely used classical partition clustering algorithm. Support vector machine is a kind of sorting algorithms that could take full advantage of limited learning sample to acquire generalization. Both Canopy-Kmeans and Support Vector Machine have better behavior and precision rate. However, the runtime and RAM would be the bottleneck in the case of large amount of data. The application of data mining algorithm based on big data is particularly important. This paper take advantage of distributed Kmeans and SVM algorithm to realize diagnosis model of cause of low voltage based on big data platform of Hadoop. Experiments have shown that this method has a higher accuracy and reliability compared to traditional Kmeans and SVM algorithm.","","Electronic:978-1-5386-2894-2; POD:978-1-5386-2895-9; USB:978-1-5386-2893-5","10.1109/ITEC-AP.2017.8081006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8081006","Canopy-Kmeans;insert;large data;low voltage;styling","Algorithm design and analysis;Classification algorithms;Clustering algorithms;Low voltage;Machine learning algorithms;Power quality;Support vector machines","Big Data;data mining;fault diagnosis;learning (artificial intelligence);pattern clustering;power engineering computing;support vector machines;voltage measurement","Canopy-Kmeans;SVM;Support Vector Machine;data mining;diagnosis model;distribution network;low voltage generation;low voltage investment;low voltage management;partition clustering;power quality","","","","","","","","7-10 Aug. 2017","","IEEE","IEEE Conferences"
"Deep learning approach for large scale land cover mapping based on remote sensing data fusion","N. Kussul; A. Shelestov; M. Lavreniuk; I. Butko; S. Skakun","Space Research Institute NASU-SSAU, Kyiv, Ukraine","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","20161103","2016","","","198","201","In the paper we propose the methodology for solving the large scale classification and area estimation problems in the remote sensing domain on the basis of deep learning paradigm. It is based on a hierarchical model that includes self-organizing maps (SOM) for data preprocessing and segmentation (clustering), ensemble of multi-layer perceptrons (MLP) for data classification and heterogeneous data fusion and geospatial analysis for post-processing. The proposed methodology is applied for generation of high resolution land cover and land use maps for the territory of Ukraine from 1990 to 2010 and 2015.","","","10.1109/IGARSS.2016.7729043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729043","Deep learning;Landsat;big data;geospatial analysis;neural network;remote sensing data","Agriculture;Big data;Earth;Machine learning;Monitoring;Remote sensing;Satellites","geophysical image processing;image classification;image fusion;image segmentation;land cover;learning (artificial intelligence);terrain mapping","AD 1990 to 2010;AD 2015;Ukraine;area estimation;data classification;data preprocessing;data segmentation;deep learning approach;geospatial analysis;heterogeneous data fusion;high-resolution land cover map;high-resolution land use map;large-scale classification;large-scale land cover mapping;multilayer perceptrons;remote sensing data fusion;self-organizing maps","","","","","","","","10-15 July 2016","","IEEE","IEEE Conferences"
"Dione: Profiling spark applications exploiting graph similarity","N. Zacheilas; S. Maroulis; V. Kalogeraki","Department of Informatics, Athens University of Economics and Business, Athens, Greece","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","389","394","In recent years distributed processing frameworks such as Apache Spark have been utilized for running big data applications. Predicting the application's execution time has been an important goal since it can help the end user to determine the necessary processing resources to be reserved. While there have been some previous works that examine the problem of profiling Spark applications, they mainly focus on specific application types (e.g., Machine learning applications) and rely on the existence of a large number of previous execution runs. In this work we aim at overcoming these limitations by minimizing the number of past execution runs needed for the profiling phase. Furthermore, we identify patterns of continuous identical dataset transformations between different applications to cope with the limited historical data availability. We propose an on-line profiling framework, called Dione, that estimates the running times of new applications, even if no historical data is available. Finally, in our detailed experimental evaluation, using practical workloads on our local cluster, we illustrate that our approach accurately predicts the execution times of Spark applications and requires 30% less training time and monetary cost compared to the current state-of-the-art techniques.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8257950","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8257950","Big Data;Graph similarity;Profiling;Spark","Buildings;Cloud computing;Computational modeling;Data models;Measurement;Predictive models;Sparks","Big Data;cluster computing;data analysis;distributed processing;graph theory;program diagnostics","Apache Spark;Dione;big data applications;execution times;graph similarity;historical data availability;on-line profiling framework;profiling Spark applications;profiling phase;running times;specific application types","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Data Mining-Based Analysis of HPC Center Operations","J. Klinkenberg; C. Terboven; S. Lankes; M. S. Müller","","2017 IEEE International Conference on Cluster Computing (CLUSTER)","20170925","2017","","","766","773","Size and complexity of contemporary High Performance Computing (HPC) systems increases permanently. While the reliability of a single component and compute node is high, the huge amount of components comprising these systems results in the fact that defects happen regularly. This drives the need to manage failure situations. Common issues are component failures or node soft lock-ups that typically lead to crashes of the user jobs that are scheduled on the affected node, and may cause undesired downtime. One approach to mitigate the impact of such problems is to predict node failures with a sufficient lead time in order to take proactive measures. However, accurate prediction is a challenging task.The literature describes several approaches that focus on gathering and analyzing system event logs in order to create prediction models. In this paper, we present a different approach by using descriptive statistics and supervised machine learning to create a prediction model from monitoring data. Our approach is based on the assumption, that features of a certain time frame before a critical event (i. e., a failure or soft lock-up) can serve as an indicator. Consequently, our model is trained with monitoring data from critical and healthy time frames. The evaluation with standard monitoring data collected from the HPC systems at RWTH Aachen University shows that our classifier is able to locate potentially failing nodes with a 10-fold cross precision of 98% and recall of 91 %.","","Electronic:978-1-5386-2326-8; POD:978-1-5386-2327-5","10.1109/CLUSTER.2017.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8049014","big data;failure prediction;large scale;machine learning;monitoring","Fans;Feature extraction;Hardware;Monitoring;Predictive models;Temperature sensors;Time series analysis","computer centres;data analysis;data mining;failure analysis;learning (artificial intelligence);parallel processing;statistics;system monitoring","HPC center operations;HPC systems;component failures;data mining;descriptive statistics;healthy time frames;high performance computing center operations;node failures;node soft lock-ups;standard data monitoring;supervised machine learning","","","","","","","","5-8 Sept. 2017","","IEEE","IEEE Conferences"
"Text categorization using Rocchio algorithm and random forest algorithm","S. T. Selvi; P. Karthikeyan; A. Vincent; V. Abinaya; G. Neeraja; R. Deepika","Department of Computer Technology, Madras Institute of Technology Campus, Anna University, Chennai, India","2016 Eighth International Conference on Advanced Computing (ICoAC)","20170619","2017","","","7","12","Millions of file uploads and downloads happen every minute resulting in big data creation and manual text categorization is not possible. Hence, there is a need for automatic categorization of documents that makes storage and retrieval more efficient. This research paper proposes a hybrid text categorization model that combines both Rocchio algorithm and Random Forest algorithm to perform Multi-label text categorization. Stop word remover and word stemmer has been used to overcome the limitations in Rocchio Algorithm. Random Forest model takes minimal categories as input to reduce its error rate. Experiments were done on standard text categorization datasets. Our proposed model is found to be more efficient in categorizing the documents when compared with other text categorization models such as fuzzy relevance clustering, ML-KNN (Multi-label KNN) and Naïve-Bayes Algorithms.","","Electronic:978-1-5090-5888-4; POD:978-1-5090-5889-1","10.1109/ICoAC.2017.7951736","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7951736","decision trees;information retrieval;text categorization;vector space model","Algorithm design and analysis;Classification algorithms;Clustering algorithms;Decision trees;Feature extraction;Testing;Text categorization","Big Data;random functions;relevance feedback;text analysis","Big Data creation;Rocchio algorithm;automatic document categorization;file downloads;file uploads;hybrid text categorization model;information retrieval;multilabel text categorization;random forest algorithm;word remover;word stemmer","","","","","","","","19-21 Jan. 2017","","IEEE","IEEE Conferences"
"Privacy-Preserving Machine Learning Algorithms for Big Data Systems","K. Xu; H. Yue; L. Guo; Y. Guo; Y. Fang","Dept. of Electr. & Comput. Eng., Univ. of Florida, Gainesville, FL, USA","2015 IEEE 35th International Conference on Distributed Computing Systems","20150723","2015","","","318","327","Machine learning has played an increasing important role in big data systems due to its capability of efficiently discovering valuable knowledge and hidden information. Often times big data such as healthcare systems or financial systems may involve with multiple organizations who may have different privacy policy, and may not explicitly share their data publicly while joint data processing may be a must. Thus, how to share big data among distributed data processing entities while mitigating privacy concerns becomes a challenging problem. Traditional methods rely on cryptographic tools and/or randomization to preserve privacy. Unfortunately, this alone may be inadequate for the emerging big data systems because they are mainly designed for traditional small-scale data sets. In this paper, we propose a novel framework to achieve privacy-preserving machine learning where the training data are distributed and each shared data portion is of large volume. Specifically, we utilize the data locality property of Apache Hadoop architecture and only a limited number of cryptographic operations at the Reduce() procedures to achieve privacy-preservation. We show that the proposed scheme is secure in the semi-honest model and use extensive simulations to demonstrate its scalability and correctness.","1063-6927;10636927","Electronic:978-1-4673-7214-5; POD:978-1-4673-7215-2","10.1109/ICDCS.2015.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164918","","Big data;Data mining;Kernel;Protocols;Support vector machines;Training;Training data","Big Data;data privacy;learning (artificial intelligence);parallel processing","Apache Hadoop architecture;Big Data systems;cryptographic operations;data locality property;privacy-preserving machine learning","","12","","31","","","","June 29 2015-July 2 2015","","IEEE","IEEE Conferences"
"A survey of clustering techniques for big data analysis","S. Arora; I. Chana","Department of Computer Science and Engineering, Thapar University, Patiala, India","2014 5th International Conference - Confluence The Next Generation Information Technology Summit (Confluence)","20141110","2014","","","59","65","With the beginning of new era data has grown rapidly not only in size but also in variety. There is a difficulty in analyzing such big data. Data mining is the technique in which useful information and hidden relationship among data is extracted. The traditional data mining approaches could not be directly implanted on big data as it faces difficulties to analyze big data. Clustering is one of the major techniques used for data mining in which mining is performed by finding out clusters having similar group of data. In this paper we have discussed some of the current big data mining clustering techniques. Comprehensive analysis of these techniques is carried out and appropriate clustering algorithm is provided.","","CD-ROM:978-1-4799-4237-4; Electronic:978-1-4799-4236-7; POD:978-1-4799-4235-0","10.1109/CONFLUENCE.2014.6949256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6949256","Big data;Clustering Techniques;Data Mining","Algorithm design and analysis;Big data;Classification algorithms;Clustering algorithms;Data mining;Machine learning algorithms;Partitioning algorithms","Big Data;data mining;pattern clustering","Big Data analysis;clustering techniques;data mining","","5","","20","","","","25-26 Sept. 2014","","IEEE","IEEE Conferences"
"Fuzzy Based Clustering Algorithms to Handle Big Data with Implementation on Apache Spark","N. Bharill; A. Tiwari; A. Malviya","Dept. of Comput. Sci. & Eng., Indian Inst. of Technol., Indore, Indore, India","2016 IEEE Second International Conference on Big Data Computing Service and Applications (BigDataService)","20160523","2016","","","95","104","With the advancement in technology, a huge amount of data containing useful information, called Big Data, is generated on a daily basis. For processing such tremendous volume of data, there is a need of Big Data frameworks such as Hadoop MapReduce, Apache Spark etc. Among these, Apache Spark performs up to 100 times faster than conventional frameworks like Hadoop Mapreduce. For the effective analysis and interpretation of this data, scalable Machine Learning methods are required to overcome the space and time bottlenecks. Partitional clustering algorithms are widely adopted by researchers for clustering large datasets due to their low computational requirements. Thus, we focus on the design of partitional clustering algorithm and its implementation on Apache Spark. In this paper, we propose a partitional based clustering algorithm called Scalable Random Sampling with Iterative Optimization Fuzzy c-Means algorithm (SRSIO-FCM) which is implemented on Apache Spark to handle the challenges associated with Big Data Clustering. Experimentation is performed on several big datasets to show the effectiveness of SRSIO-FCM in comparison with a proposed scalable version of the Literal Fuzzy c-Means (LFCM) called SLFCM implemented on Apache Spark. The comparative results are reported in terms of value of F-measure, ARI, Objective function, Run-time and Scalability. The reported results show the great potential of SRSIO-FCM for Big Data clustering.","","Electronic:978-1-5090-2251-9; POD:978-1-5090-2252-6","10.1109/BigDataService.2016.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7474361","Apache Spark;Big Data;Fuzzy Clustering;Iterative Algorithms;Partitional clustering","","Big Data;fuzzy set theory;iterative methods;learning (artificial intelligence);optimisation;parallel programming;pattern clustering;random processes;sampling methods","ARI;Apache Spark;Big Data clustering;Big Data handling;F-measure;Hadoop MapReduce;SLFCM;SRSIO-FCM;data analysis;data interpretation;data processing;fuzzy based clustering algorithms;literal fuzzy c-means;objective function;partitional clustering algorithm;run-time analysis;scalability analysis;scalable machine learning method;scalable random sampling-with-iterative optimization fuzzy c-means algorithm","","2","","31","","","","March 29 2016-April 1 2016","","IEEE","IEEE Conferences"
"Data chaos: An entropy based MapReduce framework for scalable learning","J. Chen; H. Chen; X. Chen; G. Zheng; Z. Wu","","2013 IEEE International Conference on Big Data","20131223","2013","","","71","78","Chaos of data is the total unpredictability of all the data elements, and can by quantified by Shannon entropy. In this paper, we firstly propose an entropy based theoretic framework for machine learning, which states that chaos in sample data will decrease and rule will advance as learning progresses. However, it is usually time consuming to apply the theoretic framework because groups of rule need to be trained iteratively and data chaos will be recalculated during each iteration. To implement the theoretic framework for scalable learning, we propose a MapReduce based distributed computational framework. In a case study of classification, the framework parallelly trains multiple classifiers and calculats chaos of the sample set during each iteration, and then resamples a small sample subset with the highest entropy for training of the next iteration, reducing chaos in sample data as quickly as possible. With typical classification benchmarks, our experiment presents entropy in sample data, and proves that the theoretic framework is rational and can help improve the accuracy of machine learning. Meanwhile, the computational framework shows high performance including high efficiency and scalability for large scale learning on hadoop cluster.","","Electronic:978-1-4799-1293-3; POD:978-1-4799-1294-0","10.1109/BigData.2013.6691736","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691736","Chaos;Entropy;Machine Learning;MapReduce","Accuracy;Benchmark testing;Chaos;Entropy;Prediction algorithms;Training;Uncertainty","entropy;learning (artificial intelligence);parallel processing;pattern classification","Hadoop cluster;MapReduce based distributed computational framework;Shannon entropy;classification benchmark;computational framework;data chaos;data element total unpredictability;entropy based MapReduce framework;entropy based theoretic framework;large scale learning;machine learning;multiple classifier parallel training;scalable learning","","1","","24","","","","6-9 Oct. 2013","","IEEE","IEEE Conferences"
"[Title page i]","","","2016 IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking (SocialCom), Sustainable Computing and Communications (SustainCom) (BDCloud-SocialCom-SustainCom)","20161031","2016","","","i","i","The following topics are dealt with: cloud computing; Big Data processing; spatio-temporal data; machine learning; data mining; crowdsourcing; smart city; data clustering; security; privacy; sensor network; community detection; influential social network; advertisement; recommendation; environment sustainability; smart energy; energy-aware computing; energy-aware networking; Big Data science and bioinformatics.","","","10.1109/BDCloud-SocialCom-SustainCom.2016.1","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7723655","","","Big Data;advertising;bioinformatics;cloud computing;computer networks;data mining;data privacy;learning (artificial intelligence);pattern clustering;power aware computing;recommender systems;security of data;sensors;smart cities;social networking (online);sustainable development","Big Data processing;Big Data science;advertisement;bioinformatics;cloud computing;community detection;crowdsourcing;data clustering;data mining;energy-aware computing;energy-aware networking;environment sustainability;influential social network;machine learning;privacy;recommendation;security;sensor network;smart city;smart energy;spatio-temporal data","","","","","","","","8-10 Oct. 2016","","IEEE","IEEE Conferences"
"Large scale thematic mapping by supervised machine learning on ‘big data’ distributed cluster computing frameworks","J. Lozano; N. Aginako; M. Quartulli; I. Olaizola; E. Zulueta; P. Iriondo","Vicomtech-IK4, Digital TV and Multimedia Services, Mikeletegi 57, 20009 Donostia, Spain","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","20151112","2015","","","1504","1507","The Petabyte-scale data volumes in Earth Observation (EO) archives are not efficiently manageable with serial processes running on large isolated servers. Distributed storage and processing based on `big data' cloud computing frameworks needs to be considered as a part of the solution.","2153-6996;21536996","Electronic:978-1-4799-7929-5; POD:978-1-4799-7930-1; USB:978-1-4799-7928-8","10.1109/IGARSS.2015.7326065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326065","","Big data;Clustering algorithms;Feature extraction;Machine learning algorithms;Prototypes;Servers;Sparks","Big Data;cloud computing;geophysics computing;learning (artificial intelligence);pattern clustering","EO;Earth observation archives;big data cloud computing frameworks;big data distributed cluster computing frameworks;large isolated servers;large scale thematic mapping;petabyte-scale data volumes;supervised machine learning","","","","5","","","","26-31 July 2015","","IEEE","IEEE Conferences"
"Table of Contents","","","2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)","20180118","2017","","","5","24","The following topics are dealt with: deep learning; pattern clustering; machine learning; computer vision; natural language processing; neural networks; ensemble learning; speech processing; signal processing; data security; smart power grids; computer-aided instruction; medical computing; and Big Data.","","Electronic:978-1-5386-1418-1; POD:978-1-5386-1419-8","10.1109/ICMLA.2017.00004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8260601","","","computer aided instruction;learning (artificial intelligence);medical computing;natural language processing;neural nets;power engineering computing;security of data;signal processing;smart power grids","Big Data;computer vision;computer-aided instruction;data security;deep learning;ensemble learning;machine learning;medical computing;natural language processing;neural networks;pattern clustering;signal processing;smart power grids;speech processing","","","","","","","","18-21 Dec. 2017","","IEEE","IEEE Conferences"
"Deep Learning: The Frontier for Distributed Attack Detection in Fog-to-Things Computing","A. Abeshu; N. Chilamkurti","La Trobe Univ., Melbourne, VIC, Australia","IEEE Communications Magazine","20180213","2018","56","2","169","175","The increase in the number and diversity of smart objects has raised substantial cybersecurity challenges due to the recent exponential rise in the occurrence and sophistication of attacks. Although cloud computing has transformed the world of business in a dramatic way, its centralization hammers the application of distributed services such as security mechanisms for IoT applications. The new and emerging IoT applications require novel cybersecurity controls, models, and decisions distributed at the edge of the network. Despite the success of the existing cryptographic solutions in the traditional Internet, factors such as system development flaws, increased attack surfaces, and hacking skills have proven the inevitability of detection mechanisms. The traditional approaches such as classical machine-learning-based attack detection mechanisms have been successful in the last decades, but it has already been proven that they have low accuracy and less scalability for cyber-attack detection in massively distributed nodes such as IoT. The proliferation of deep learning and hardware technology advancement could pave a way to detecting the current level of sophistication of cyber-attacks in edge networks. The application of deep networks has already been successful in big data areas, and this indicates that fog-to-things computing can be the ultimate beneficiary of the approach for attack detection because a massive amount of data produced by IoT devices enable deep models to learn better than shallow algorithms. In this article, we propose a novel distributed deep learning scheme of cyber-attack detection in fog-to-things computing. Our experiments show that deep models are superior to shallow models in detection accuracy, false alarm rate, and scalability.","0163-6804;01636804","","10.1109/MCOM.2018.1700332","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8291134","","Cloud computing;Computational modeling;Computer architecture;Edge computing;Machine learning;Malware;Security","Internet;Internet of Things;cloud computing;computer crime;computer network security;data mining;learning (artificial intelligence)","IoT applications;IoT devices;attack detection mechanisms;centralization hammers;classical machine-learning;cloud computing;cryptographic solutions;cyber-attack detection;cyber-attacks;cybersecurity controls;deep learning scheme;deep models;deep networks;detection accuracy;distributed attack detection;distributed services;diversity;edge networks;exponential rise;fog-to-things computing;massively distributed nodes;new emerging IoT applications;security mechanisms;smart objects;substantial cybersecurity challenges;system development flaws;traditional Internet","","","","","","","","Feb. 2018","","IEEE","IEEE Journals & Magazines"
"Highly scalable data processing framework for pervasive computing applications","J. Riihijärvi; P. Mähönen","Institute for Networked Systems, RWTH Aachen University, Kackertstrasse 9, D-52072, Germany","2013 IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOM Workshops)","20130708","2013","","","306","308","One of the key problems in pervasive computing is enabling the collective processing of sensor data obtained from mobile devices such as smartphones. In this demonstration we present a highly scalable storage and processing framework for pervasive computing applications, enabling various estimation problems to be solved from massive data sets, consisting of measurements from millions of nodes or more. The key to achieving such scalability is the use of linear or sublinear time processing algorithms emerging from statistical and machine learning communities. We focus specifically on spatial and spatio-temporal estimation problems in the demonstration, such as prediction of sensor readings, user densities, or wireless network usage in regions for which direct measurements are not available.","","Electronic:978-1-4673-5077-8; POD:978-1-4673-5075-4; USB:978-1-4673-5076-1","10.1109/PerComW.2013.6529501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6529501","Pervasive computing;fixed rank kriging;massive data sets;sublinear methods","Computer architecture;Data processing;Estimation;Graphical user interfaces;Pervasive computing;Smart phones","learning (artificial intelligence);mobile radio;smart phones;statistical analysis;storage management;ubiquitous computing","collective processing;data processing framework;machine learning;massive data set;mobile device;pervasive computing;sensor data;sensor reading prediction;smartphone;spatial estimation problem;spatio-temporal estimation problem;statistical learning;storage;sublinear time processing algorithm;user density;wireless network usage","","0","","13","","","","18-22 March 2013","","IEEE","IEEE Conferences"
"An Asynchronous Distributed ADMM Algorithm and Efficient Communication Model","L. Fang; Y. Lei","","2016 IEEE 14th Intl Conf on Dependable, Autonomic and Secure Computing, 14th Intl Conf on Pervasive Intelligence and Computing, 2nd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)","20161013","2016","","","136","140","In this paper, we present a hierarchical butterfly communication model, which is applied to an asynchronous distributed ADMM algorithm. The goal is to minimize the communication overhead of the distributed ADMM algorithm in the fully connected network. We give a theoretical analysis of the convergence of the algorithm with hierarchical butterfly communication model. Experiments show that hierarchical butterfly communication model does not have a great impact on the convergence of the algorithm with the increase of computing nodes and thus effectively improve the performance and scalability of the algorithm.","","","10.1109/DASC-PICom-DataCom-CyberSciTec.2016.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7588835","ADMM;asynchronous;butterfly;hierarchical communication model","Algorithm design and analysis;Analytical models;Computational modeling;Computers;Convergence;Data models;Machine learning algorithms","","","","","","","","","","8-12 Aug. 2016","","IEEE","IEEE Conferences"
"DBSCAN on Resilient Distributed Datasets","I. Cordova; T. S. Moh","Department of Computer Science, San Jos&#x00E9; State University, California, U.S.A.","2015 International Conference on High Performance Computing & Simulation (HPCS)","20150903","2015","","","531","540","DBSCAN is a well-known density-based data clustering algorithm that is widely used due to its ability to find arbitrarily shaped clusters in noisy data. However, DBSCAN is hard to scale which limits its utility when working with large data sets. Resilient Distributed Datasets (RDDs), on the other hand, are a fast data-processing abstraction created explicitly for in-memory computation of large data sets. This paper presents a new algorithm based on DBSCAN using the Resilient Distributed Datasets approach: RDD-DBSCAN. RDD-DBSCAN overcomes the scalability limitations of the traditional DBSCAN algorithm by operating in a fully distributed fashion. The paper also evaluates an implementation of RDD-DBSCAN using Apache Spark, the official RDD implementation.","","CD-ROM:978-1-4673-7811-6; Electronic:978-1-4673-7813-0; POD:978-1-4673-7814-7","10.1109/HPCSim.2015.7237086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7237086","Apache Spark;DBSCAN;MapReduce;Resilient Distributed Datasets;data clustering;data partition;parallel systems","Clustering algorithms;Distributed databases;Machine learning algorithms;Noise;Partitioning algorithms;Prediction algorithms;Sparks","data handling;distributed processing;pattern clustering","Apache Spark;RDD-DBSCAN algorithm;arbitrarily shaped clusters;data-processing abstraction;density-based data clustering algorithm;in-memory computation;official RDD implementation;resilient distributed datasets approach","","3","","19","","","","20-24 July 2015","","IEEE","IEEE Conferences"
"A deep cascade neuro-fuzzy system for high-dimensional online fuzzy clustering","Z. Hu; Y. V. Bodyanskiy; O. K. Tyshchenko","Central China Normal University, 152 Louyu Road, 430079, Wuhan, China","2016 IEEE First International Conference on Data Stream Mining & Processing (DSMP)","20161006","2016","","","318","322","A deep cascade system (based on neuro-fuzzy nodes) and its online learning procedure are proposed in this paper. A number of layers can grow unlimitedly during a self-learning procedure. The system is based on nodes of a special type. A goal function of a special type is used for probabilistic high-dimensional fuzzy clustering. To assess a clustering quality of data processing, a neuron's architecture of a special type is introduced.","","","10.1109/DSMP.2016.7583567","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7583567","Computational Intelligence;Data Stream Processing;Deep Learning;Machine Learning;Neuro-Fuzzy System;Probabilistic Fuzzy Clustering","Clustering algorithms;Fuzzy systems;Machine learning;Neural networks;Neurons;Probabilistic logic;Quality assessment","cascade systems;data handling;fuzzy neural nets;fuzzy systems;neural net architecture;pattern clustering;probability;unsupervised learning","data processing clustering quality;deep cascade neuro-fuzzy system;neuron architecture;online learning procedure;probabilistic high-dimensional online fuzzy clustering;self-learning procedure","","2","","","","","","23-27 Aug. 2016","","IEEE","IEEE Conferences"
"Topic Modeling and Visualization for Big Data in Social Sciences","N. Sukhija; M. Tatineni; N. Brown; M. V. Moer; P. Rodriguez; S. Callicott","Distrib. Analytics & Security Inst., Mississippi State Univ., Starkville, MS, USA","2016 Intl IEEE Conferences on Ubiquitous Intelligence & Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)","20170116","2016","","","1198","1205","Topic modeling is a widely used approach for analyzing large text collections. In particular, Latent Dirichlet Allocation (LDA) is one of the most popular topic modeling approaches to aggregate vocabulary from a document corpus to form latent ""topics"". However, learning meaningful topic models with massive document collections which contain millions of documents, billions of tokens is challenging, given the complexity of the data involved, the difficulty in distributing the computation across multiple computing nodes. In recent years some data processing frameworks, such as Spark, Mallet, others have been developed to address the issues associated with analyzing large volumes of unlabeled text pertaining to various domains in a scalable, efficient manner. In this paper, we will present a preliminary case study demonstrating the scholarship achieved in the study of political consumerism via XSEDE resources. The experimental study will showcase the use of digitized social sciences data, text analytics toolkits to generate topic models, visualize topics for empowering intersectional research engaging the relationship between consumption, race, class, gender in the area of sociology. Consequently, this comparative big data textual analysis involving use of JSTOR data, LDA modeling toolkit's, visualization techniques, computational components is of paramount importance, especially for researchers from academic domain dealing with social science applications involving big data.","","Electronic:978-1-5090-2771-2; POD:978-1-5090-2772-9","10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0183","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816979","Big Data;LDA;Machine learning;Mallet;Scalability;Social Science;Spark;Text Analytics;Topic Modeling;Visualization","Analytical models;Big data;Computational modeling;Data models;Resource management;Sparks;Vocabulary","Big Data;data aggregation;data analysis;data visualisation;learning (artificial intelligence);social sciences computing;text analysis;vocabulary","JSTOR data;LDA modeling toolkit;Mallet;Spark;XSEDE resources;comparative big data textual analysis;data complexity;data processing;digitized social science data;document corpus;large text collection analysis;latent Dirichlet allocation;latent topics;massive document collection;political consumerism;social science application;sociology;text analytics toolkit;topic model generation;topic model learning;topic visualization;unlabeled text;vocabulary aggregation","","","","","","","","18-21 July 2016","","IEEE","IEEE Conferences"
"Big data and the regulation of financial markets","S. O'Halloran; S. Maskey; G. McAllister; D. K. Park; K. Chen","Columbia University","2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)","20160211","2015","","","1118","1124","The development of computational data science techniques in natural language processing (NLP) and machine learning (ML) algorithms to analyze large and complex textual information opens new avenues to study intricate processes, such as government regulation of financial markets, at a scale unimaginable even a few years ago. This paper develops scalable NLP and ML algorithms (classification, clustering and ranking methods) that automatically classify laws into various codes/labels, rank feature sets based on use case, and induce best structured representation of sentences for various types of computational analysis. The results provide standardized coding labels of policies to assist regulators to better understand how key policy features impact financial markets.","","Electronic:978-1-4503-3854-7; POD:978-1-5090-2094-2","10.1145/2808797.2808841","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7403687","big data;financial regulation;machine learning;natural language processing;political economics","Encoding;Government;Indexes;Natural language processing;Social network services;Uncertainty","Big Data;financial data processing;learning (artificial intelligence);natural language processing;pattern classification;pattern clustering;stock markets","Big Data;ML algorithms;NLP;classification method;clustering method;complex textual information;computational analysis;computational data science techniques;feature sets ranking;financial market regulation;machine learning;natural language processing;ranking method","","","","23","","","","25-28 Aug. 2015","","IEEE","IEEE Conferences"
"Smart risk management with financial big data","J. Lin; S. Jia; J. Deng","Graduate school at Shenzhen Tsinghua University, The University Town, Shenzhen, 518055, P.R. China","2017 IEEE/SICE International Symposium on System Integration (SII)","20180205","2017","","","60","65","Smart risk management is a new research area to combine artificial intelligence with traditional quantitative risk management model to leverage financial big data in secondary market more effectively. Usually, the construction of traditional quantitative risk management model is not fully automatic from financial data. Industry classification, parameter estimation of factor returns covariance matrix and eliminate random noise in factor correlation coefficient matrix are usually decided by modeler's experience. The industry classification in this paper was made based on a machine learning methodology called quantitative clustering method from stock analyst forecast big data. Half-life parameter for factor returns covariance matrix is dynamically estimated from trading data intelligently combined with random matrix theory (RMT). Finally, this paper gives several numerical test results to verify the effectiveness of smart risk management methods in Chinese financial markets.","","Electronic:978-1-5386-2263-6; POD:978-1-5386-2264-3; USB:978-1-5386-2262-9","10.1109/SII.2017.8279189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8279189","","Big Data;Covariance matrices;Data models;Indexes;Industries;Portfolios;Risk management","Big Data;covariance matrices;financial data processing;learning (artificial intelligence);risk management;stock markets","Chinese financial markets;artificial intelligence;factor correlation coefficient matrix;factor return covariance matrix parameter estimation;financial Big Data;industry classification;quantitative clustering method;smart risk management methods;traditional quantitative risk management model","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Recovery conditions and sampling strategies for network Lasso","A. Mara; A. Jung","Department of Computer Science, Aalto University, Finland","2017 51st Asilomar Conference on Signals, Systems, and Computers","20180416","2017","","","405","409","The network Lasso is a recently proposed convex optimization method for machine learning from massive network structured datasets, i.e., big data over networks. It is a variant of the well-known least absolute shrinkage and selection operator (Lasso), which is underlying many methods in learning and signal processing involving sparse models. Highly scalable implementations of the network Lasso can be obtained by state-of-the-art proximal methods, e.g., the alternating direction method of multipliers (ADMM). By generalizing the concept of the compatibility condition put forward by van de Geer and Buhlmann as a powerful tool for the analysis of plain Lasso, we derive a sufficient condition, i.e., the network compatibility condition, on the underlying network topology such that network Lasso accurately learns a clustered underlying graph signal. This network compatibility condition relates the location of sampled nodes with the clustering structure of the network. In particular, the NCC informs the choice of which nodes to sample, or in machine learning terms, which data points provide most information if labeled.","","CD:978-1-5386-0666-7; Electronic:978-1-5386-1823-3; POD:978-1-5386-1824-0","10.1109/ACSSC.2017.8335369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8335369","big data;complex networks;compressed sensing;convex optimzation;semi-supervised learning","Big Data;Convex functions;Data models;Network topology;Noise measurement;TV;Topology","convex programming;data analysis;graph theory;learning (artificial intelligence);network theory (graphs);sampling methods;signal processing","ADMM;alternating direction method of multipliers;convex optimization method;graph signal;machine learning;massive network structured datasets;network Lasso;network compatibility condition;network topology;plain Lasso analysis;recovery conditions;sampling strategies","","","","","","","","Oct. 29 2017-Nov. 1 2017","","IEEE","IEEE Conferences"
"Characterizing Deep Learning over Big Data (DLoBD) Stacks on RDMA-Capable Networks","X. Lu; H. Shi; M. H. Javed; R. Biswas; D. K. Panda","","2017 IEEE 25th Annual Symposium on High-Performance Interconnects (HOTI)","20171019","2017","","","87","94","Deep Learning over Big Data (DLoBD) is becoming one of the most important research paradigms to mine value from the massive amount of gathered data. Many emerging deep learning frameworks start running over Big Data stacks, such as Hadoop and Spark. With the convergence of HPC, Big Data, and Deep Learning, these DLoBD stacks are taking advantage of RDMA and multi-/many-core based CPUs/GPUs. Even though a lot of activities are happening in the field, there is a lack of systematic studies on analyzing the impact of RDMA-capable networks and CPU/GPU on DLoBD stacks. To fill this gap, we propose a systematical characterization methodology and conduct extensive performance evaluations on three representative DLoBD stacks (i.e., CaffeOnSpark, TensorFlowOnSpark, and BigDL) to expose the interesting trends regarding performance, scalability, accuracy, and resource utilization. Our observations show that RDMA-based design for DLoBD stacks can achieve up to 2.7x speedup compared to the IPoIB based scheme. The RDMA scheme can also scale better and utilize resources more efficiently than the IPoIB scheme over InfiniBand clusters. For most cases, GPU-based deep learning can outperform CPU-based designs, but not always. We see that for LeNet on MNIST, CPU + MKL can achieve better performance than GPU and GPU + cuDNN on 16 nodes. Through our evaluation, we see that there are large rooms to improve the designs of current generation DLoBD stacks further.","","Electronic:978-1-5386-1013-8; POD:978-1-5386-1014-5","10.1109/HOTI.2017.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8071061","Big Data;DLoBD;Deep Learning;InfiniBand;RDMA","Big Data;Graphics processing units;Libraries;Machine learning;Parallel processing;Sparks;Training","Big Data;file organisation;learning (artificial intelligence)","BigDL;CaffeOnSpark;DLoBD stacks;RDMA-based design;RDMA-capable networks;TensorFlowOnSpark;deep learning over Big Data stacks;remote direct memory access;systematical characterization methodology","","","","","","","","28-30 Aug. 2017","","IEEE","IEEE Conferences"
"Petuum: A New Platform for Distributed Machine Learning on Big Data","E. P. Xing; Q. Ho; W. Dai; J. K. Kim; J. Wei; S. Lee; X. Zheng; P. Xie; A. Kumar; Y. Yu","School of Computer Science, Carnegie Mellon University","IEEE Transactions on Big Data","20170520","2015","1","2","49","67","What is a systematic way to efficiently apply a wide spectrum of advanced ML programs to industrial scale problems, using Big Models (up to 100 s of billions of parameters) on Big Data (up to terabytes or petabytes)? Modern parallelization strategies employ fine-grained operations and scheduling beyond the classic bulk-synchronous processing paradigm popularized by MapReduce, or even specialized graph-based execution that relies on graph representations of ML programs. The variety of approaches tends to pull systems and algorithms design in different directions, and it remains difficult to find a universal platform applicable to a wide range of ML programs at scale. We propose a general-purpose framework, Petuum, that systematically addresses data- and model-parallel challenges in large-scale ML, by observing that many ML programs are fundamentally optimization-centric and admit error-tolerant, iterative-convergent algorithmic solutions. This presents unique opportunities for an integrative system design, such as bounded-error network synchronization and dynamic scheduling based on ML program structure. We demonstrate the efficacy of these system designs versus well-known implementations of modern ML algorithms, showing that Petuum allows ML programs to run in much less time and at considerably larger model sizes, even on modestly-sized compute clusters.","","","10.1109/TBDATA.2015.2472014","10.13039/100000001 - US National Science Foundation (NSF); 10.13039/100000185 - US Defense Advanced Research Projects Agency (DARPA); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7239545","Big Data;Big Model;Data-Parallelism;Distributed Systems;Machine Learning;Machine learning;Model-Parallelism;Theory;big data;big model;data-parallelism;distributed systems;model-parallelism;theory","Big data;Computational modeling;Convergence;Data models;Mathematical model;Servers;Synchronization","Big Data;learning (artificial intelligence);parallel programming;scheduling","Big Data;Big Models;MapReduce;Petuum platform;bounded-error network synchronization;bulk-synchronous processing paradigm;data-parallel method;distributed machine learning;dynamic scheduling;error-tolerant-iterative-convergent algorithmic solutions;fine-grained operations;general-purpose framework;graph representations;graph-based execution;integrative system design;large-scale ML;model-parallel method;optimization-centric programs;parallelization strategies;pull systems;universal platform","","20","","38","","","20150903","June 1 2015","","IEEE","IEEE Journals & Magazines"
"Enriched Over_Sampling Techniques for Improving Classification of Imbalanced Big Data","S. S. Patil; S. P. Sonavane","Rajaramnagar Islampur, Comput. Sci. & Eng. R.I.T., Islampur, India","2017 IEEE Third International Conference on Big Data Computing Service and Applications (BigDataService)","20170612","2017","","","1","10","Big Data generated in exabytes per year has become a watchword of today's research. They are exceptionally afar from the capability of commonly used software tools and also beyond the handling possibility of the single machine architecture. Facing this challenge has activated a requisite to reexamine the data management options. The new avenues of NoSQL Big Data compared to the traditional forms has insisted on adapting experimental beds, helping to discover large unknown values from enormous data sets. Also, outmoded management systems and statistical packages express trouble handling Big Data. In numerous real applications, handling of imbalanced data sets is the fact of precedence. The classification of data sets having imbalanced class distribution has produced a notable drawback in performance obtained by the most standard classifier learning algorithms. Assuming balanced class distribution and equal misclassification costs lead to poor results. In a real-world domain, the classification methods of multi-class imbalance problem need more attention compared to the two-class problem. A methodology is presented for binary/multi-class imbalanced data sets with improved over_sampling (O.S.) techniques to enhance classification. The methods are broadly classified into two categories: non-clustered and cluster based advanced approach compared to prior work on O.S. techniques. The balanced data are subsequently analyzed for classification using various classifiers. Proposed techniques are performed using mapreduce environment on Apache Hadoop, using various data sets from UCI/KEEL repository. Fmeasures and ROC area are used to measure the performance of this classification.","","Electronic:978-1-5090-6318-5; POD:978-1-5090-6319-2","10.1109/BigDataService.2017.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7944914","Big Data;data level approach;imbalanced data sets;over_sampling techniques","Area measurement;Big Data;Cancer;Classification algorithms;Data mining;Machine learning algorithms;Standards","Big Data;learning (artificial intelligence);pattern classification;pattern clustering","Apache Hadoop;F measures;MapReduce environment;NoSQL Big Data;ROC area;UCI/KEEL repository;balanced class distribution;binary/multiclass imbalanced data sets;classifiers;cluster based advanced approach;data management options;enriched oversampling techniques;equal misclassification costs;imbalanced big data classification;imbalanced class distribution;imbalanced data set handling;multiclass imbalance problem;nonclustered based advanced approach;standard classifier learning algorithms;two-class problem","","","","","","","","6-9 April 2017","","IEEE","IEEE Conferences"
"4.6 A1.93TOPS/W scalable deep learning/inference processor with tetra-parallel MIMD architecture for big-data applications","S. Park; K. Bong; D. Shin; J. Lee; S. Choi; H. J. Yoo","KAIST, Daejeon, Korea","2015 IEEE International Solid-State Circuits Conference - (ISSCC) Digest of Technical Papers","20150319","2015","","","1","3","Recently, deep learning (DL) has become a popular approach for big-data analysis in image retrieval with high accuracy [1]. As Fig. 4.6.1 shows, various applications, such as text, 2D image and motion recognition use DL due to its best-in-class recognition accuracy. There are 2 types of DL: supervised DL with labeled data and unsupervised DL with unlabeled data. With unsupervised DL, most of learning time is spent in massively iterative weight updates for a restricted Boltzmann machine [2]. For a -100MB training dataset, >100 TOP computational capability and ~40GB/s IO and SRAM data bandwidth is required. So, a 3.4GHz CPU needs >10 hours learning time with a -100K input-vector dataset and takes ~1 second for recognition, which is far from real-time processing. Thus, DL is typically done using cloud servers or high-performance GPU environments with learning-on-server capability. However, the wide use of smart portable devices, such as smartphones and tablets, results in many applications which need big-data processing with machine learning, such as tagging private photos in personal devices. A high-performance and energy-efficient DL/DI (deep inference) processor is required to realize user-centric pattern recognition in portable devices.","0193-6530;01936530","Electronic:978-1-4799-6224-2; POD:978-1-4799-6225-9","10.1109/ISSCC.2015.7062935","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7062935","","Bandwidth;Multicore processing;Parallel processing;Pipelines;Program processors;Scalability","Big Data;Boltzmann machines;SRAM chips;graphics processing units;image retrieval;inference mechanisms;learning (artificial intelligence);parallel architectures","Boltzmann machine;CPU;GPU environment;IO;SRAM;big data applications;frequency 3.4 GHz;image retrieval;inference processor;input vector dataset;iterative weight update;labeled data;learning-on-server capability;machine learning;personal devices;private photos tagging;scalable deep learning processor;smart portable devices;supervised DL;tetra-parallel MIMD architecture;unlabeled data;unsupervised DL;user centric pattern recognition","","7","","7","","","","22-26 Feb. 2015","","IEEE","IEEE Conferences"
"Deep learning-based MSMS spectra reduction in support of running multiple protein search engines on cloud","M. Maabreh; B. Qolomany; I. Alsmadi; A. Gupta","Department of Computer Science, Western Michigan University, Kalamazoo, MI, USA","2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20171218","2017","","","1909","1914","The diversity of the available protein search engines with respect to the utilized matching algorithms, the low overlap ratios among their results and the disparity of their coverage encourage the community of proteomics to utilize ensemble solutions of different search engines. The advancing in cloud computing technology and the availability of distributed processing clusters can also provide support to this task. However, data transferring and results' combining, in this case, could be the major bottleneck. The flood of billions of observed mass spectra, hundreds of Gigabytes or potentially Terabytes of data, could easily cause the congestions, increase the risk of failure, poor performance, add more computations' cost, and waste available resources. Therefore, in this study, we propose a deep learning model in order to mitigate the traffic over cloud network and, thus reduce the cost of cloud computing. The model, which depends on the top 50 intensities and their m/z values of each spectrum, removes any spectrum which is predicted not to pass the majority voting of the participated search engines. Our results using three search engines namely: pFind, Comet and X!Tandem, and four different datasets are promising and promote the investment in deep learning to solve such type of Big data problems.","","Electronic:978-1-5090-3050-7; POD:978-1-5090-3051-4","10.1109/BIBM.2017.8217951","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8217951","Cloud Computing;Data Reduction;Deep Learning;Network Traffic;Protein Search Engine;Searching Space Reduction","Cloud computing;Databases;Machine learning;Peptides;Proteins;Proteomics;Search engines","Big Data;bioinformatics;cloud computing;data analysis;distributed processing;learning (artificial intelligence);mass spectra;pattern clustering;pattern matching;proteins;proteomics;search engines","Big data problems;MSMS spectra reduction;cloud computing;cloud network;data transferring;deep learning model;distributed processing clusters;ensemble solutions;mass spectra;matching algorithms;multiple protein search engines","","","","","","","","13-16 Nov. 2017","","IEEE","IEEE Conferences"
"Credit modelling using hybrid machine learning technique","S. Dahiya; S. S. Handa; N. P. Singh","Deptt. of Computer Science & Engg., Manav Rachna International University (MRIU) Faridabad, India, And Scientist, IASRI, New Delhi","2015 International Conference on Soft Computing Techniques and Implementations (ICSCTI)","20160613","2015","","","103","106","Credit evaluation models are the important tools used by banks for the evaluation of loan customers as good or bad. These models are developed as a part of data mining projects using mainly the Classification and Clustering tasks. Their accuracy plays a very significant role as they are the backbones behind the important decisions taken by banks. The accuracy can be improved by using many factors, some of these are the use of good machine learning techniques, balanced input data, and using hybrid techniques in model development. The machine learning and statistical techniques can be combined in various ways for creating the effective hybrid models. In this paper the input data has been balanced to avoid biased model training towards the larger class. Machine learning techniques which have been proved successful in many experiments on financial data are used for this study. The machine learning techniques used are: Naïve Bayes, MLP, RBF, Logistic Regression and C4.5. First single models have been developed using these machine learning techniques and the one with highest accuracy has been found. Then this model was hybridized with others for improving the classification accuracy. The accuracy of all these models was tested on a separate test set that has not been shown to the model while training. A bench marked credit dataset has been utilized for conducting the experiments. The results of the single and hybrid models shows that the MLP outperformed all other individual models while the hybrid model developed by combining the MLP with MLP gave the best results.","","CD-ROM:978-1-4673-6790-5; Electronic:978-1-4673-6792-9; POD:978-1-4673-6793-6","10.1109/ICSCTI.2015.7489612","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489612","Balanced data;Credit Evaluation;Hybrid Models;Machine Learning","Data mining;Data models;Decision trees;Logistics;Predictive models;Radial basis function networks;Training","Bayes methods;bank data processing;credit transactions;data mining;decision trees;learning (artificial intelligence);multilayer perceptrons;pattern classification;pattern clustering;radial basis function networks;regression analysis","C4.5 decision tree algorithm;MLP;RBF;banks;classification tasks;clustering tasks;credit evaluation modelling;data mining projects;financial data;hybrid machine learning technique;loan customer evaluation;logistic regression;multilayer perceptron;naïve Bayes;radial basis function;statistical techniques","","","","12","","","","8-10 Oct. 2015","","IEEE","IEEE Conferences"
"Sensor design and model-based tactile feature recognition","V. Müller; T. L. Lam; N. Elkmann","Fraunhofer Institute for Factory Operation and Automation IFF, Fraunhofer-Gesellschaft zur F&#x00F6;rderung der angewandten Forschung e.V., Magdeburg, Germany","2017 IEEE SENSORS","20171225","2017","","","1","3","This paper<sup>1</sup> presents the design of a flexible tactile sensor and a model-based approach for the pose estimation and surface reconstruction of objects in a gripper. We show that the proposed sensor composite can be easily attached to almost all object shapes, while still achieving a high spatial sensor resolution and a high force sensitivity. Since machine learning algorithms require a large data base and do not offer the scalability of training data, the approach that we prefer here uses model-based feature classification. In order to improve the accuracy of our approach, we investigated fundamental sensor properties and applied sustainable correction methods to the data processing. Finally, the sensor's operability and the evaluation results have been verified in a pick-and-place application for two different grippers.","","Electronic:978-1-5090-1012-7; POD:978-1-5090-1013-4","10.1109/ICSENS.2017.8234169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8234169","dexterous object manipulation;pose estimation;tactile feature recognition;tactile gripper;tactile sensor","Grippers;Piezoresistance;Sensor arrays;Shape;Spatial resolution;Tactile sensors","grippers;image reconstruction;learning (artificial intelligence);pattern classification;pose estimation;tactile sensors","data processing;feature classification;flexible tactile sensor;fundamental sensor properties;gripper;high force sensitivity;high spatial sensor resolution;machine learning algorithms;object shapes;object surface reconstruction;pose estimation;sensor design;sustainable correction;tactile feature recognition;training data","","","","","","","","Oct. 29 2017-Nov. 1 2017","","IEEE","IEEE Conferences"
"High-Dimensional Data Stream Classification via Sparse Online Learning","D. Wang; P. Wu; P. Zhao; Y. Wu; C. Miao; S. C. H. Hoi","Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore","2014 IEEE International Conference on Data Mining","20150129","2014","","","1007","1012","The amount of data in our society has been exploding in the era of big data today. In this paper, we address several open challenges of big data stream classification, including high volume, high velocity, high dimensionality, and high sparsity. Many existing studies in data mining literature solve data stream classification tasks in a batch learning setting, which suffers from poor efficiency and scalability when dealing with big data. To overcome the limitations, this paper investigates an online learning framework for big data stream classification tasks. Unlike some existing online data stream classification techniques that are often based on first-order online learning, we propose a framework of Sparse Online Classification (SOC) for data stream classification, which includes some state-of-the-art first-order sparse online learning algorithms as special cases and allows us to derive a new effective second-order online learning algorithm for data stream classification. We conduct an extensive set of experiments, in which encouraging results validate the efficacy of the proposed algorithms in comparison to a family of state-of-the-art techniques on a variety of data stream classification tasks.","1550-4786;15504786","Electronic:978-1-4799-4302-9; POD:978-1-4799-4301-2","10.1109/ICDM.2014.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7023438","data stream classification;online learning;sparse","Algorithm design and analysis;Big data;Electronic mail;Error analysis;Machine learning algorithms;Prediction algorithms;Training","Big Data;data analysis;learning (artificial intelligence);pattern classification","Big Data stream classification;SOC;data stream classification tasks;first-order sparse online learning algorithms;high-dimensional data stream classification;online data stream classification techniques;second-order online learning algorithm;sparse online classification","","8","","19","","","","14-17 Dec. 2014","","IEEE","IEEE Conferences"
"Sampling-based consensus fuzzy clustering on Big Data","M. A. Zoghlami; M. S. Hidri; R. B. Ayed","Universit&#x00E9; de Tunis El Manar, Ecole Nationale d'Ing&#x00E9;nieurs de Tunis, BP. 37, Le Belv&#x00E8;d&#x00E8;re 1002, Tunisia","2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)","20161110","2016","","","1501","1508","Many companies spend vast amounts of resources to collect, transform and store the massive amounts of data that flows through their business processes. When it comes to doing analysis and machine learning such as clustering on this data, time and compute speed gate determine how much data can be analyzed. Moreover, most Big Data clustering algorithms do not look at a complete, large dataset. Instead, they look at a subsample and work on approximations. However, work on samples can spread useful data that can be sources of value. In this paper, we use sampling combined with consensus strategy to dissemble the whole Big Data into small subsets, then basic partitions are locally generated from them using parallel processing. For the sampling part, we propose a partial data clustering (PDC) according to different nodes to classify the current sub-samples of partial data access (PDA) merged together with optimal prototypes generated from the last PDC and condensed into weighted points. For the consensus part, we apply a split-and-merge fuzzy clustering to equivalently transfer the consensus clustering problem into an optimization clustering one. Extensive experiments on several datasets demonstrate the ability to handle massive data and the consensus computing make the proposed classifier promising candidate for Big Data clustering.","","Electronic:978-1-5090-0626-7; POD:978-1-5090-0627-4; USB:978-1-5090-0625-0","10.1109/FUZZ-IEEE.2016.7737868","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7737868","Big Data;Consensus;Fuzzy clustering;Parallel processing;Sampling","Algorithm design and analysis;Big data;Clustering algorithms;Handheld computers;Partitioning algorithms;Program processors;Prototypes","Big Data;fuzzy set theory;learning (artificial intelligence);pattern clustering","Big Data clustering algorithms;PDC;business processes;consensus clustering problem;consensus computing;machine learning;optimization clustering;parallel processing;partial data access;partial data clustering;sampling-based consensus fuzzy clustering;split-and-merge fuzzy clustering","","","","","","","","24-29 July 2016","","IEEE","IEEE Conferences"
"Purchase likelihood prediction for targeted organic food marketing campaigns in China","B. Giannini; S. Chen; P. Paramonov; Y. Y. Wu","Tongji University, School of Economics and Management, Shanghai 200092, P. R. China","Proceedings of PICMET '14 Conference: Portland International Center for Management of Engineering and Technology; Infrastructure and Service Integration","20141013","2014","","","1759","1769","The demand for organic food products in China is growing in response to both increased spending power and food safety concerns. However, identifying likely buyers of organic products proves challenging due to their relatively small fraction in the overall population. Our study explores applications of machine learning algorithms for effective management of organic food marketing campaigns in China. Based on the data we collected through an online choice-experiment type questionnaire of Chinese consumers, a purchase likelihood estimation framework has been developed that utilizes customer profile traits such as age group, family status, education level, and geographic location. In addition, we apply clustering techniques to perform data-driven organic market segmentation and identify consumer profiles ready to pay more for high quality, certified organic products. The resulting market segments are compared to various types of organic consumers discussed in the literature. Our algorithms provide a useful framework for online retailers who are seeking lean strategies of market entry in China with their health food brands.","2159-5100;21595100","Electronic:978-1-890843-29-8; POD:978-1-4799-5769-9; USB:978-1-890843-30-4","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6921061","","Certification;Cities and towns;Dairy products;Economic indicators;Education;Machine learning algorithms;Prediction algorithms","food processing industry;food safety;learning (artificial intelligence);market research;marketing data processing;pattern clustering","China;clustering techniques;consumer profile identification;data-driven organic market segmentation;food safety concern;health food brands;machine learning algorithms;online choice-experiment type questionnaire;organic food marketing campaign;organic products;purchase likelihood estimation framework;purchase likelihood prediction","","0","","29","","","","27-31 July 2014","","IEEE","IEEE Conferences"
"Applying Data Analytics towards Optimized Issue Management: An Industrial Case Study","M. R. Karim; S. M. D. A. Alam; S. J. Kabeer; G. Ruhe; B. Baluta; S. Mahmud","SEDS Lab., Univ. of Calgary Calgary, Calgary, AB, Canada","2016 IEEE/ACM 4th International Workshop on Conducting Empirical Studies in Industry (CESI)","20170109","2016","","","7","13","This document describes our experience of applying data analytics at Plexina, a leading IT company working in the healthcare domain. The main goal of the project was to identify factors currently affecting issue management and to make analytics based suggestions for optimizing the process. Various statistical and machine learning techniques were applied on a data set extracted from six releases of Plexina, containing more than 666 issues. Statistical techniques successfully identified the various factors that leads to estimation inaccuracy related to issues as well as identified the hidden relationships existing among various variables. The employed predictive analytic models was also successful to some extent, in predicting effort estimation related inaccuracy associated with the issues. The insights provided by the entire data analytics study can be of great help to product managers or the developers to make more informed decisions. In addition, the guidelines presented in this paper based on the lessons learnt can be applied to other data analytics and academia-industry collaboration project.","","Electronic:978-1-4503-4154-7; POD:978-1-5090-2197-0","10.1109/CESI.2016.012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7809361","Case study;Data analytics;Industry-academia collaboration.;Issue management","Clustering algorithms;Collaboration;Companies;Data analysis;Estimation;Industries;Software","data analysis;health care;learning (artificial intelligence);statistical analysis","IT company;Plexina;academia-industry collaboration project;data analytics;data set extraction;machine learning;optimized issue management;predictive analytic models;statistical techniques","","","","","","","","17-17 May 2016","","IEEE","IEEE Conferences"
"Classification of students by using an incremental ensemble of classifiers","R. Ade; P. R. Deshmukh","Dept. of Computer Engineering, Sipna College of Engineering and Technology, Sant Gadge Baba Amravati University, Maharashtra, India","Proceedings of 3rd International Conference on Reliability, Infocom Technologies and Optimization","20150122","2014","","","1","5","The amount of students data in the education system databases is growing day by day, so the knowledge taken out from these data need to be updated continuously. The training set of the supervised learning algorithms contains student's score in the test. Incremental learning ability is further significant for machine learning methodologies as student's data and the information is increasing. Against to the classical batch learning algorithm, incremental learning algorithm tries to forget unrelated information while training new instances. Now a days, combination of a classifiers is a novel concept for overall progress in the classification result. Therefore, an incremental ensemble of two classifiers namely Naïve Bayes, K-Star using majority voting scheme is proposed. The large scale comparison of a proposed ensemble technique by using different voting scheme with the state-of the art algorithm on the student's data set has been done. The experimental results shown high accuracy for the proposed ensemble for the student's classification. High accuracy was also achieved for the majority voting scheme as compared to other voting scheme.","","Electronic:978-1-4799-6896-1; POD:978-1-4799-6897-8","10.1109/ICRITO.2014.7014666","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014666","education system;ensemble;incremental learning;voting scheme","Accuracy;Classification algorithms;Machine learning algorithms;Neural networks;Prediction algorithms;Support vector machines;Training","Bayes methods;educational administrative data processing;learning (artificial intelligence);pattern classification;pattern clustering","K-Star algorithm;clustering;education system databases;incremental classifier ensemble;incremental learning algorithm;majority voting scheme;naive Bayes;student classification","","0","","34","","","","8-10 Oct. 2014","","IEEE","IEEE Conferences"
"Modified Fuzzy K-mean clustering using MapReduce in Hadoop and cloud","D. Garg; P. Gohil; K. Trivedi","Department of Computer Science and Engineering, Babaria Institute of Technology, Varnama, Vadodara, Gujarat, India","2015 IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT)","20150827","2015","","","1","5","Apache Hadoop is an open source software framework which structures Big data (both structured and unstructured). It is nowadays one of the biggest motivator in market as data storage is inexpensive in it. The storage method of Hadoop uses a distributed file system which lets the user store large amount of data by simply adding more nodes to a Hadoop cluster. Clustering a large amount of data is a point of concern. MapReduce, a programming model used by Hadoop allows a parallelization technique by decomposing a larger problem involving large dataset to smaller portion of data and then executing it. A scalable machine learning library named as Mahout is an approach to clustering which runs on Hadoop. In this paper, the Hadoop multi-node cluster is formed using Amazon EC2. This paper focuses on Fuzzy k-mean clustering algorithm which is modified by centroid generation method using MapReduce in Hadoop. Experimental results depict a decrease in the number of iterations thereby leading to a decrease in the execution time when modification of Fuzzy K-mean clustering algorithm is done using Canopy generation in MapReduce in Hadoop.","","Electronic:978-1-4799-6085-9; POD:978-1-4799-6086-6","10.1109/ICECCT.2015.7226046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7226046","Cloud;Fuzzy K-mean clustering;HDFS;Hadoop;Mahout;MapReduce","Classification algorithms;Clustering algorithms;Random access memory","Big Data;cloud computing;fuzzy set theory;learning (artificial intelligence);parallel programming;pattern clustering;public domain software","Amazon EC2;Apache Hadoop;Canopy generation;Hadoop multinode cluster;Mahout;MapReduce;big data;centroid generation method;cloud computing;data clustering;data storage;distributed file system;execution time;modified fuzzy K-mean clustering;open source software framework;parallelization technique;programming model;scalable machine learning library","","1","","21","","","","5-7 March 2015","","IEEE","IEEE Conferences"
"Big-data analysis of multi-source logs for anomaly detection on network-based system","Z. Jia; C. Shen; X. Yi; Y. Chen; T. Yu; X. Guan","Xi'an Jiaotong University, Xi'an, Shaanxi, 710049, China","2017 13th IEEE Conference on Automation Science and Engineering (CASE)","20180115","2017","","","1136","1141","Log data are important audit basis to record routine events occurring on computer or network system, which are also critical data source for detecting system anomalies. By analyzing the data from multi-source logs, it is helpful to detect abnormal system behaviors and discover intruder attacks in real time. In this paper, a Spark-based log data security platform is designed and built to analyze the large-scale log data and detect abnormal network behaviors. By integrating data mining, machine learning, and statistical analysis technologies, our proposed framework can quickly analyze large-scale multi-source log data and accurately discriminate the abnormal behaviors. Furthermore, the association analysis is applied to detect abnormal behaviors or potential threats in the system. Under a real-world network environment, extensive experiments are conducted to evaluate the system performance, which can achieve a fast and accurate detection for abnormal network behaviors, and significantly improve the accuracies under various types of network attack scenarios.","","Electronic:978-1-5090-6781-7; POD:978-1-5090-6782-4; USB:978-1-5090-6780-0","10.1109/COASE.2017.8256257","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8256257","","Anomaly detection;Clustering algorithms;Intrusion detection;Linux;Monitoring","Big Data;computer network security;data analysis;data mining;learning (artificial intelligence);statistical analysis","Spark-based log data security platform;abnormal behavior detection;abnormal network behaviors;abnormal system behaviors;anomaly detection;association analysis;big-data analysis;critical data source;data mining;data security platform;large-scale multisource log data;machine learning;multisource logs;network attack scenarios;network-based system;real-world network environment;statistical analysis technologies;system anomalies","","","","","","","","20-23 Aug. 2017","","IEEE","IEEE Conferences"
"Affinity Propagation Clustering for Intelligent Portfolio Diversification and Investment Risk Reduction","C. C. Chang; Z. T. Lin; W. W. Koc; C. Chou; S. H. Huang","Dept. of Inf. Manage. & Finance, Nat. Chiao Tung Univ. HsinChu, Hsinchu, Taiwan","2016 7th International Conference on Cloud Computing and Big Data (CCBD)","20170717","2016","","","145","150","In this paper, an intelligent portfolio selection method based on Affinity Propagation clustering algorithm is proposed to solve the stable investment problem. The goal of this work is to minimize the volatility of the selected portfolio from the component stocks of S&P 500 index. Each independent stock can be viewed as a node in graph, and the similarity measurements of stock price variations between companies are calculated as the edge weights. Affinity Propagation clustering algorithm solve the graph theory problem by repeatedly update responsibility and availability message passing matrices. This research tried to find most representative and discriminant features to model the stock similarity. The testing features are divided into two major categories, including time-series covariance, and technical indicators. The historical price and trading volume data is used to simulate the portfolio selection and volatility measurement. After grouping these investment targets into a small set of clusters, the selection process will choose fixed number of stocks from different clusters to form the portfolio. The experimental results show that the proposed system can effectively generate more stable portfolio by Affinity Propagation clustering algorithm with proper similarity features than average cases with similar settings.","","Electronic:978-1-5090-3555-7; POD:978-1-5090-3556-4","10.1109/CCBD.2016.037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7979895","affinity propagation;artificial intelligence;clustering;diversified investment;machine learning;portfolio selection","Artificial intelligence;Clustering algorithms;Indexes;Investment;Neural networks;Portfolios;Standards","covariance analysis;financial data processing;graph theory;investment;pattern clustering;stock markets;time series","S&P 500 index;affinity propagation clustering;availability message passing matrices;component stocks;discriminant features;edge weights;graph theory;historical price;intelligent portfolio diversification;investment risk reduction;portfolio selection;representative features;responsibility message passing matrices;similarity measurements;stock price variations;stock similarity;technical indicators;testing features;time-series covariance;trading volume data;volatility measurement","","","","","","","","16-18 Nov. 2016","","IEEE","IEEE Conferences"
"Sampling Adaptively Using the Massart Inequality for Scalable Learning","J. Chen; J. Xu","Sch. of Electr. Eng. & Comput. Sci., Louisiana State Univ., Baton Rouge, LA, USA","2013 12th International Conference on Machine Learning and Applications","20140410","2013","2","","362","367","With the advent of the ""big data"" era, the data mining community is facing an increasingly critical problem of developing scalable algorithms capable of mining knowledge from massive amount of data. This paper develops a sampling-based method to address the issue of scalability. We show how to utilize the new, adaptive sampling method in [4] to develop a scalable learning algorithm by boosting, an ensemble learning method. We present experimental results using bench-mark data sets from the UC-Irvine ML data repository that confirm the much improved efficiency and thus scalability, and competitive prediction accuracy of the new adaptive boosting method, in comparison with existing approaches.","","CD-ROM:978-1-4799-4154-4; Electronic:978-0-7695-5144-9; POD:978-1-4799-4155-1","10.1109/ICMLA.2013.149","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6786136","Adaptive Sampling;Boosting;Ensemble Learning;Sample Size;Scalable Learning","Accuracy;Boosting;Computer science;Data mining;Prediction algorithms;Sampling methods;Scalability","data mining;learning (artificial intelligence);sampling methods","Massart inequality;UC-Irvine ML data repository;adaptive sampling method;benchmark data sets;boosting;competitive prediction accuracy;data mining;ensemble learning method;scalable learning algorithm","","0","","16","","","","4-7 Dec. 2013","","IEEE","IEEE Conferences"
"A Machine Learning Approach for Tracking and Predicting Student Performance in Degree Programs","J. Xu; K. H. Moon; M. van der Schaar","Department of Electrical and Computer Engineering, University of Miami, Coral Gables, FL, USA","IEEE Journal of Selected Topics in Signal Processing","20170814","2017","11","5","742","753","Accurately predicting students' future performance based on their ongoing academic records is crucial for effectively carrying out necessary pedagogical interventions to ensure students' on-time and satisfactory graduation. Although there is a rich literature on predicting student performance when solving problems or studying for courses using data-driven approaches, predicting student performance in completing degrees (e.g., college programs) is much less studied and faces new challenges: (1) Students differ tremendously in terms of backgrounds and selected courses; (2) courses are not equally informative for making accurate predictions; and (3) students' evolving progress needs to be incorporated into the prediction. In this paper, we develop a novel machine learning method for predicting student performance in degree programs that is able to address these key challenges. The proposed method has two major features. First, a bilayered structure comprising multiple base predictors and a cascade of ensemble predictors is developed for making predictions based on students' evolving performance states. Second, a data-driven approach based on latent factor models and probabilistic matrix factorization is proposed to discover course relevance, which is important for constructing efficient base predictors. Through extensive simulations on an undergraduate student dataset collected over three years at University of California, Los Angeles, we show that the proposed method achieves superior performance to benchmark approaches.","1932-4553;19324553","","10.1109/JSTSP.2017.2692560","10.13039/100000001 - National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7894238","Data-driven course clustering;personalized education;student performance prediction","Complexity theory;Education;Machine learning algorithms;Prediction algorithms;Probabilistic logic;Recommender systems;Signal processing algorithms","educational administrative data processing;educational courses;learning (artificial intelligence);matrix decomposition;probability","academic records;data-driven;degree programs;latent factor models;machine learning;probabilistic matrix factorization;student performance prediction","","","","","","","20170407","Aug. 2017","","IEEE","IEEE Journals & Magazines"
"A first estimation of the proportion of cybercriminal entities in the bitcoin ecosystem using supervised machine learning","H. Sun Yin; R. Vatrapu","Centre for Business Data Analytics, Copenhagen Business School, Denmark","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","3690","3699","Bitcoin, a peer-to-peer payment system and digital currency, is often involved in illicit activities such as scamming, ransomware attacks, illegal goods trading, and thievery. At the time of writing, the Bitcoin ecosystem has not yet been mapped and as such there is no estimate of the share of illicit activities. This paper provides the first estimation of the portion of cyber-criminal entities in the Bitcoin ecosystem. Our dataset consists of 854 observations categorised into 12 classes (out of which 5 are cybercrime-related) and a total of 100,000 uncategorised observations. The dataset was obtained from the data provider who applied three types of clustering of Bitcoin transactions to categorise entities: co-spend, intelligence-based, and behaviour-based. Thirteen supervised learning classifiers were then tested, of which four prevailed with a cross-validation accuracy of 77.38%, 76.47%, 78.46%, 80.76% respectively. From the top four classifiers, Bagging and Gradient Boosting classifiers were selected based on their weighted average and per class precision on the cybercrime-related categories. Both models were used to classify 100,000 uncategorised entities, showing that the share of cybercrime-related is 29.81% according to Bagging, and 10.95% according to Gradient Boosting with number of entities as the metric. With regard to the number of addresses and current coins held by this type of entities, the results are: 5.79% and 10.02% according to Bagging; and 3.16% and 1.45% according to Gradient Boosting.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258365","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258365","Bitcoin;Blockchain;Cryptocurrency;Cybercrime;Ecosystem;Machine Learning;Ransomware;Supervised Learning","Bitcoin;Ecosystems;Malware;Peer-to-peer computing;Public key","computer crime;financial data processing;gradient methods;learning (artificial intelligence);pattern classification;security of data","Bagging classifiers;Bitcoin transactions;Gradient Boosting classifiers;bitcoin ecosystem;cyber-criminal entities;cybercrime-related categories;cybercriminal entities;illegal goods trading;illicit activities;peer-to-peer payment system;proportion estimation;supervised learning classifiers;supervised machine learning;uncategorised observations","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"YinMem: A distributed parallel indexed in-memory computation system for large scale data analytics","Y. Huang; Y. Yesha; M. Halem; Y. Yesha; S. Zhou","Computer Science and Electrical Engineering, University of Maryland, Baltimore County, Baltimore, MD, 21220","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","214","222","Machine learning and graph analytics typically process data in an iterative way, reading the same data multiple times and sharing intermediate results across the worker nodes in cluster. Hadoop MapReduce and Spark are two popular open source cluster compute frameworks for large scale data analytics. Apache Spark is currently the state-of-the-art in-memory computation model extending MapReduce by transforming data into RDDs stored in memory. One limitation of Spark, however, lies in the fact that data transformation and distribution is implicitly managed by HDFS. Data locality is not guaranteed for iterative machine learning algorithms which read the same data multiple times. For example, data needed for operations to one worker node might reside in RDDs stored in other worker nodes. The resulting data shuffling becomes a bottleneck when iteratively reading such RDDs. We propose YinMem, a parallel distributed indexed in-memory computation system, bridging the gap between Hadoop ecosystem and HPC by replacing MapReduce with MPI while obtaining the advantage of the distributed data storage. YinMem achieves fair load balancing prior to computation for large sparse matrix by scheduling and distributing indexed data from NoSQL database to the RAM of working nodes. YinMem explores Alluxio as the in-memory storage system and enables efficient data sharing of intermediate results. Preliminary results show that YinMem has achieved 3× speedup to Spark, for computing eigenvalue and eigenvectors of a 16-million scale sparse matrix.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840607","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840607","","","data analysis;iterative methods;learning (artificial intelligence);parallel programming;sparse matrices","HPC;Hadoop MapReduce;MPI;NoSQL database;Spark;YinMem;distributed data storage;distributed parallel indexed in-memory computation system;graph analytics;iterative machine learning algorithms;large scale data analytics;large sparse matrix;machine learning;memory computation model","","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conferences"
"A comparative study of various clustering techniques on big data sets using Apache Mahout","V. R. Eluri; M. Ramesh; A. S. M. Al-Jabri; M. Jane","Dept. of Information Technology, Shinas College of Technology, Shinas, Oman","2016 3rd MEC International Conference on Big Data and Smart City (ICBDSC)","20160428","2016","","","1","4","Clustering algorithms have materialized as an unconventional tool to precisely examine the immense volume of data produced by present applications. In specific, their main objective is to classify data into clusters such that objects are grouped in the same cluster when they are similar rendering to particular metrics and dissimilar to objects of other groups. From the machine learning perspective clustering can be viewed as unsupervised learning of concepts. Hadoop is a distributed file system and an open-source implementation of MapReduce dealing with big data. Apache Mahout clustering algorithms are implemented on top of Hadoop using MapReduce paradigm. In this paper three clustering algorithms are described: K-means, Fuzzy K-Means (FKM) and Canopy clustering implemented by using Apache Mahout as well as providing a comparison. In addition, we underlined the clustering algorithms that are the preeminent performing for big data.","","Electronic:978-1-4673-9584-7; POD:978-1-4673-9585-4","10.1109/ICBDSC.2016.7460397","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7460397","Apache Mahout;Canopy clustering;Clustering Algorithm;Fuzzy K-Means;K-means;big data","Big data;Clustering algorithms;Electronic mail;Image segmentation;Information technology;Machine learning algorithms;Smart cities","Big Data;fuzzy set theory;learning (artificial intelligence);network operating systems;pattern clustering","Apache Mahout clustering algorithms;Canopy clustering;FKM;Hadoop;MapReduce dealing;MapReduce paradigm;big data sets;clustering techniques;distributed file system;fuzzy k-means;machine learning perspective clustering;open source;rendering;unsupervised learning","","3","","10","","","","15-16 March 2016","","IEEE","IEEE Conferences"
"Machine learning based real-time activity detection system design","K. K. Eren; K. Küçük","Bilgisayar M&#x00FC;hendisli&#x011F;i B&#x00F6;l&#x00FC;m&#x00FC;, Kocaeli &#x00DC;niversitesi, Kocaeli, T&#x00FC;rkiye","2017 International Conference on Computer Science and Engineering (UBMK)","20171102","2017","","","462","467","Identification of human activities is a popular pattern recognition problem. In order to solve this problem, solutions based on machine learning are popularly used. Solutions based on the principle of collecting and processing classified data from one person are often used for non-real-time solutions. In this study, a system design is presented in which real time processing of the received acceleration data is performed using a mobile device, and a hardware three-axis accelerometer and the daily movement of the person is detected through different classification methods. Besides, pre-processing is carried out in the training clusters, enabling the system to respond in real time. The open source WISDM (Wireless Sensor Data Mining) dataset is used for classification in system design. The WISDM data set has a continuous-time data set and a discrete-time version of the data set. In this study, the continuous time data was handled again and some modifications were made to the data set and the discretization process was performed. In this respect, the classification performance for the J48 classification algorithm increased from 85.05% to 89.80%, and the performance in the data set for MLP (Multilayer Perceptron) increased from 84.94% to 93.08%. Furthermore, in the system obtained by using the obtained dataset, real-time usage result is taken as 70% performance. Reasons for the success difference between real time system and data set are discussed and solution proposal is presented.","","Electronic:978-1-5386-0930-9; POD:978-1-5386-0931-6","10.1109/UBMK.2017.8093437","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8093437","Activity Recognition;classification;data mining;machine learning;real time system;smartphone","Art;Data mining;Multilayer perceptrons;Real-time systems;System analysis and design;Wireless communication;Wireless sensor networks","accelerometers;data mining;feature extraction;image classification;image motion analysis;learning (artificial intelligence);multilayer perceptrons;object detection;public domain software","J48 classification algorithm;MLP;classification methods;classification performance;classified data collection;classified data processing;continuous-time data set;discrete-time data set;discretization process;hardware three-axis accelerometer;human activities identification;machine learning based real-time activity detection system design;mobile device;multilayer perceptron;open source WISDM data set;pattern recognition problem;person daily movement detection;received acceleration data;wireless sensor data mining dataset","","","","","","","","5-8 Oct. 2017","","IEEE","IEEE Conferences"
"Mobile application based sustainable irrigation water usage decision support system: An intelligent sensor CLOUD approach","C. Li; R. Dutta; C. Kloppers; C. D'Este; A. Morshed; A. Almeida; A. Das; J. Aryal","Commonwealth Sci. & Ind. Res. Organ. (CSIRO), Hobart, TAS, Australia","2013 IEEE SENSORS","20131219","2013","","","1","4","In this paper a novel data integration approach based on three environmental Sensors - Model Networks (including the Bureau of Meteorology-SILO database, Australian Cosmic Ray Sensor Network database (CosmOz), and Australian Water Availability Project (AWAP) database) has been proposed to estimate ground water balance and average water availability. An unsupervised machine learning based clustering technique (Dynamic Linear Discriminant Analysis (D-LDA)) has been applied for extracting knowledge from the large integrated database. The Commonwealth Scientific and Industrial Research Organisation (CSIRO) Sensor CLOUD computing infrastructure has been used extensively to process big data integration and the machine learning based decision support system. An analytical outcome from the Sensor CLOUD is presented as dynamic web based knowledge recommendation service using JSON file format. An intelligent ANDROID based mobile application has been developed, capable of automatically communicating with the Sensor CLOUD to get the most recent daily irrigation, water requirement for a chosen location and display the status in a user friendly traffic light system. This recommendation could be used directly by the farmers to make the final decision whether to buy extra water for irrigation or not on a particular day.","1930-0395;19300395","Electronic:978-1-4673-4642-9; POD:978-1-4673-4641-2; USB:978-1-4673-4640-5","10.1109/ICSENS.2013.6688523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688523","","Australia;Clouds;Decision support systems;Irrigation;Mobile communication;Sensors;Soil","cloud computing;decision support systems;environmental science computing;irrigation;learning (artificial intelligence);mobile computing;pattern clustering;recommender systems","CSIRO;D-LDA;JSON file format;average water availability;commonwealth scientific and industrial research organisation sensor CLOUD computing infrastructure;data integration approach;dynamic Web based knowledge recommendation service;dynamic linear discriminant analysis;environmental sensors;ground water balance;intelligent ANDROID based mobile application;intelligent sensor CLOUD approach;mobile application based sustainable irrigation water usage decision support system;model networks;traffic light system;unsupervised machine learning based clustering technique","","2","","4","","","","3-6 Nov. 2013","","IEEE","IEEE Conferences"
"A waste city management system for smart cities applications","D. D. Vu; G. Kaddoum","Electrical Engineering Department, &#x00C9;cole de Technologie Sup&#x00E9;rieure, Montr&#x00E9;al, Canada","2017 Advances in Wireless and Optical Communications (RTUWO)","20171221","2017","","","225","229","This paper presents a new method of smart waste city management which makes the environment of the city clean with a low cost. In this approach, the sensor model detects, measures, and transmits waste volume data over the Internet. The collected data including trash bin's geolocation and the serial number is processed by using regression, classification and graph theory. Thenceforth a new method is proposed to dynamically and efficiently manage the waste collection by predicting waste status, classifying trash bin location, and monitoring the amount of waste. Then, this latter recommends the optimization of the route to manage the garbage truck efficiently. Finally, the simulation results are presented and estimated.","","Electronic:978-1-5386-0585-1; POD:978-1-5386-0586-8","10.1109/RTUWO.2017.8228538","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8228538","Big data;IoT;machine learning;optimization;waste management","Clustering algorithms;Data models;Monitoring;Prediction algorithms;Smart cities;Waste management","Internet;environmental science computing;graph theory;optimisation;pattern classification;regression analysis;smart cities;town and country planning;waste management","Internet;classification;collected data including trash bin;garbage truck management;graph theory;regression;route optimization;sensor model;serial number processing;smart cities applications;smart waste city management;trash bin location classification;waste city management system;waste collection management;waste status prediction;waste volume data","","","","","","","","2-3 Nov. 2017","","IEEE","IEEE Conferences"
"Cognitive exploration of regions through analyzing geo-tagged social media data","Y. Wang; G. Baciu; C. Li","Deptartment of Computing, The Hong Kong Polytechnic University, Hong Kong","2017 IEEE 16th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC)","20171116","2017","","","59","64","Social media has now become a pervasive global communication channel. Many applications and platforms have become available for users to post messages, follow friends and share experiences. Due to the high frequency with which users update their states, a large amount of data is being generated around the world every second. By analyzing this data, valuable patterns can be extracted such as the distribution of users, their common interests, activities, locations visited, etc. In this paper, we focus on the cognitive exploration of photo sharing data. Traditionally, each photo sharing record comes with information about the location where the photo was taken, a timestamp, and potentially some description about the photo. Therefore, we can often deduce the features of photo-spots. Spots with similar features constitute a region of cognitive interest. The primary goal of this paper is to identify these regions and allow users to explore into regions of interest by cognitive understanding of their features and patterns of feature propagation in time. To achieve this goal, we propose an approach that makes use of semantic analysis in big data sets, data clustering, and cognitive visualization design issues. Our contributions are two-fold. First, we put forward a novel social-media data classification method. This is based on cognitive semantic analysis. Second, we suggest a new method to explore social activity maps by discovering regions of cognitive interest. In this paper, we introduce the design of an interactive visualization interface which projects photo sharing data to cognitive social activity map components. Experiments are performed on the Flickr dataset.","","CD:978-1-5386-0770-1; Electronic:978-1-5386-0771-8; POD:978-1-5386-0772-5","10.1109/ICCI-CC.2017.8109730","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8109730","cognitive visualization;data mining;machine learning;photo tag;region discovery;semantic analysis;social media","Data mining;Data visualization;Semantics;Shape;Social network services;Transportation;Visualization","Big Data;geographic information systems;pattern classification;social networking (online)","Big Data sets;cognitive exploration;cognitive semantic analysis;cognitive visualization design issues;data clustering;feature propagation;geo-tagged social media data;photo sharing data;photo-spots;social activity maps;social-media data classification method;timestamp","","","","","","","","26-28 July 2017","","IEEE","IEEE Conferences"
"Fuzzy Clustering Algorithms — Review of the Applications","J. Li; H. W. Lewis","Dept. of Syst. Sci. & Ind. Eng., State Univ. of New York at Binghamton, Binghamton, NY, USA","2016 IEEE International Conference on Smart Cloud (SmartCloud)","20161226","2016","","","282","288","Fuzzy clustering is an alternative method to conventional or hard clustering algorithms, which makes partitions of data containing similar subjects. The tendency of adopting machine learning, big data science, cloud computation in various industries depends on unsupervised learning on data structures to tell the story about consumers' behavior, fraud detection, and market segmentation. Fuzzy clustering contrasts with hard clustering by its nonlinear nature and discipline of flexibility in grouping massive data. It provides more accurate and close-to-nature solutions for partitions and herein implies more possibility of solutions for decision-making. In the specific matter of computation, fuzzy clustering has its roots in fuzzy logic and indicates the likelihood or degrees of one data point belonging to more than one group. This paper focuses on the study of models of fuzzy clustering in various cases. Uniquely designed algorithms enhance the accuracy of outcomes and are worth studying to assist future work. In some case scenarios, modeling processes are data-driven and place emphasis on the distances between points and new centers of clusters. In some other cases, which aim at market segmentation or evaluation of patients by healthcare records, membership degree is a key element in the algorithm. This paper surveys a wide-range of research that has well-designed mathematic models for fuzzy clustering, some of which include genetic algorithms and neural networks. The last section introduces open sources of Python and displays sample results from hands-on practice with these packages.","","Electronic:978-1-5090-5263-9; POD:978-1-5090-5264-6","10.1109/SmartCloud.2016.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7796188","fuzzy c-mean clustering;genetic algorithm;neural network;pattern recognition;validity index","Clustering algorithms;Data structures;Euclidean distance;Genetic algorithms;Histograms;Indexes;Mathematical model","data handling;data structures;decision making;fuzzy logic;fuzzy set theory;genetic algorithms;neural nets;pattern clustering;public domain software;software packages;unsupervised learning","Python;data partition;data structure;decision making;fuzzy clustering algorithm;fuzzy logic;genetic algorithm;neural network;open source;software package;unsupervised learning","","","","","","","","18-20 Nov. 2016","","IEEE","IEEE Conferences"
"Enabling versatile analysis of large scale traffic video data with deep learning and HiveQL","L. Huang; W. Xu; S. Liu; V. Pandey; N. R. Juri","Texas Advanced Computing Center, The University of Texas at Austin, 10100 Burnet Rd. Austin, TX, 78758","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","1153","1162","While monocular roadside cameras have been widely deployed and used to monitor traffic conditions across the United States, the analysis of those video data are commonly implemented either manually or through commercial applications tailor-made for specific tasks. The goal of this project is to develop an efficient system that can meet dynamic content based video analysis needs and scale to large scale traffic camera video data. The proposed system utilizes deep learning methods to recognize objects in the video data. That information can then be processed and analyzed through an analysis layer implemented using Spark and Hive. The analysis layer supports HiveQL, which enables end users to conduct sophisticated analysis with customized queries. In this paper, we present the implementation of this prototype application in details. The application can utilize both GPU and multiple CPUs to accelerate its computation. We evaluated its performance and scalability with different hardware and parameter settings, including Intel Knights Landing, Intel Skylake, Nvidia K40 GPU, and Nvidia P100 GPU, for object recognition. To demonstrate its versatile, we show two practical use case examples: counting moving vehicles and identifying scenes including pedestrians and vehicles. We show the accuracy of the system by comparing vehicular counts produced by the analysis with manually annotated results. The comparison shows our methods can achieve over eighty percent accuracy comparing to manual results.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258041","Bid Data;Deep Learning;Traffic Flow Estimation and Identification;Transportation safety;Video Analysis","Algorithm design and analysis;Cameras;Electronic mail;Machine learning;Safety;Sparks","cameras;graphics processing units;learning (artificial intelligence);object detection;object recognition;query processing;road traffic;traffic engineering computing;video signal processing","HiveQL;Nvidia K40 GPU;Nvidia P100 GPU;United States;analysis layer;commercial applications tailor;deep learning methods;dynamic content;monocular roadside cameras;prototype application;scale traffic camera video data;scale traffic video data;sophisticated analysis;specific tasks;traffic conditions;versatile analysis;video analysis","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Deep learning for analysing synchrotron data streams","B. Wang; Z. Guan; S. Yao; H. Qin; M. H. Nguyen; K. Yager; D. Yu","Department of Computer Science, Department of Biochemistry and Structure Biology, Stony Brook University, Stony Brook, U.S.A.","2016 New York Scientific Data Summit (NYSDS)","20161121","2016","","","1","5","The National Synchrotron Light Source II (NSLS-II) at Brookhaven National Laboratory (BNL) is now providing some of the world's brightest x-ray beams. A suite of imaging and diffraction methods, exploiting megapixel detectors with kilohertz frame-rates at NSLS-II beamlines, generate a variety of image streams in unprecedented velocities and volumes. A complete understanding of a complex material system often requires a cluster of x-ray characterization tools that can reveal its elemental, structural, chemical and physical properties at different length-scales and time-scales. The flourish and continuing refinement of x-ray probes enable that the same sample may be studied with different perspectives and granularities, and at different time and locations; these powerful tools generate a correspondingly daunting big data challenge, with multiple image streams that outpaces any manual efforts and traditional data analysis practice. In this paper, we applied deep learning methods, in particular, deep convolutional neural network (CNN) to automatically recognize image features from image streams from NSLS-II, and integrated our deep-learning methods into the Google Tensorflow to cluster and label both real and synthetic 2-D scattering image patterns. These methods would empower scientists by providing timely insights, allowing them to steer experiments efficiently during their precious x-ray beamtime allocation. Experiment shows that the CNN-based image labeling attains a 10% improvement over traditional K-mean and Support Vector Machine.","","Electronic:978-1-4673-9051-4; POD:978-1-4673-9052-1","10.1109/NYSDS.2016.7747813","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7747813","CNN;Deep learning;X Ray Image Classification","Feature extraction;Machine learning;Neural networks;Scattering;Streaming media;X-ray imaging;X-ray scattering","X-ray imaging;image recognition;neural nets;pattern clustering;physics computing;synchrotrons","CNN-based image labeling;Google Tensorflow;K-means method;NSLS-II;National Synchrotron Light Source II;convolutional neural network;deep learning methods;image clustering;image feature recognition;image streams;support vector machine;synchrotron data stream analysis;synthetic 2-D scattering image patterns;x-ray beamtime allocation","","","","","","","","14-17 Aug. 2016","","IEEE","IEEE Conferences"
"Analytical review on human activity recognition in video","R. Bhardwaj; P. K. Singh","Amity School of Engineering & Technology, Amity University Uttar Pradesh, Noida, India","2016 6th International Conference - Cloud System and Big Data Engineering (Confluence)","20160709","2016","","","531","536","The main motive of this review paper is to recognise the human activities in video using different posses and various types of activities done by human in video. To achieve this activity recognition author's used a different technique such as object segmentation, feature extraction and representation, Hidden markov model, bag of word approach. And some basic concepts of machine learning and algorithms such as supervised learning, clustering, Linear Discriminant analysis, Finite state automata, K-Nearest Neighbour have been used. The domain area for this analysis is surveillances, entertainment and healthcare environment. And the authors have collected the data for their analysis from various sources such as Youtube, movies, real human activities videos are collected from Railway stations, banks, hospitals, circus area specially which are under the camera notification.","","CD-ROM:978-1-4673-8202-1; Electronic:978-1-4673-8203-8; POD:978-1-4673-8204-5","10.1109/CONFLUENCE.2016.7508177","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7508177","Bag of words approach;Hidden Markov Model;Human activity recognition;Image Processing;K-Nearest neighbour;Linear Discriminant Analysis;Object Segmentation;Support Vector Machine","Cameras;Databases;Hidden Markov models;Motion pictures;Streaming media;Support vector machines","learning (artificial intelligence);object recognition;video signal processing","analytical review;entertainment;healthcare environment;human activity recognition;machine learning;surveillances;video","","","","","","","","14-15 Jan. 2016","","IEEE","IEEE Conferences"
"Class Discovery via Bimodal Feature Selection in Unsupervised Settings","J. Curtis; M. Kon","Dept. of Math. & Stat., Boston Univ., Boston, MA, USA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","699","702","In machine learning there are numerous supervised techniques that extend naturally to analogous unsupervised methods, such as clustering. In this paper, we consider so-called rare-weak models, in which the number of important features is small (or rare) and the signal strength of each important feature is minimal (or weak). When classical clustering is applied crudely in ""big data"" scenarios, significant problems can arise, including long computational run times and significant clustering errors. One solution is to use feature selection (FS) to reduce dataset dimensionality before clustering. We introduce two novel unsupervised feature selection methods, one parametric and one nonparametric, based on what we call bimodal feature selection. These methods produce ranked lists of features based on their univariate multi-modality. Unlike previously developed univariate FS methods, which have typically been restricted to 2-cluster scenarios, our method has been adapted and tested to discriminate binary and higher level clusterings. The method is particularly advantageous in rare-weak settings, since reducing data dimensionality allows classical clustering methods to be applied computationally faster and with greater accuracy.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424401","clustering;feature selection;unsupervised data","Clustering algorithms;Clustering methods;Electronic mail;Estimation;Kernel;Standards","Big Data;feature selection;learning (artificial intelligence);pattern clustering","2-cluster scenario;analogous unsupervised method;big data scenario;bimodal feature selection;binary level clustering;class discovery;classical clustering method;clustering error;data dimensionality;dataset dimensionality;higher level clustering;machine learning;rare-weak model;signal strength;supervised technique;univariate multimodality;unsupervised feature selection method;unsupervised setting","","","","17","","","","9-11 Dec. 2015","","IEEE","IEEE Conferences"
"Machine Learning-Based Configuration Parameter Tuning on Hadoop System","C. O. Chen; Y. Q. Zhuo; C. C. Yeh; C. M. Lin; S. W. Liao","Comput. Intell. Technol. Center, Ind. Technol. Res. Inst., Hsinchu, Taiwan","2015 IEEE International Congress on Big Data","20150820","2015","","","386","392","Apache Hadoop system is a software framework with the capability to process large-scale datasets across a cluster of distributed machines using MapReduce programming model. However, there are two main challenges for system administrators to manage the Hadoop system, (1) system administrators are difficult to tune the parameters appropriately since the behaviors and characteristics of large-scale distributed systems are too complicated, (2) there are dozens of configuration parameters affecting the system performance which makes the configuration parameters tuning task becomes troublesome. In this paper, we focus on optimizing the Hadoop MapReduce job performance by tuning configuration parameters, and then we propose an analytical method to help system administrators choose approximately optimal configuration parameters depending on the characteristics of each application. Our approach has two key phases: prediction and optimization phase. The prediction phase is to estimate the performance of a MapReduce job, whereas the optimization phase is to search the approximately optimal configuration parameters strategically by invoking the predictor repeatedly. In our evaluation results, our work can help system administrators to improve the performance about 2X to 8X better than traditional methods.","2379-7703;23797703","Electronic:978-1-4673-7278-7; POD:978-1-4673-7279-4","10.1109/BigDataCongress.2015.64","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7207248","Distributed System;Machine Learning;Optimization Problem","Accuracy;Approximation methods;Optimization;Predictive models;Regression tree analysis;System performance;Tuning","learning (artificial intelligence);parallel processing","Apache Hadoop system;Hadoop MapReduce job performance;MapReduce programming model;approximately optimal configuration parameter;configuration parameters tuning task;distributed machine;large-scale dataset;large-scale distributed system;machine learning-based configuration parameter tuning;software framework;system administrator;system performance","","1","","36","","","","June 27 2015-July 2 2015","","IEEE","IEEE Conferences"
"Accelerating Machine Learning Kernel in Hadoop Using FPGAs","K. Neshatpour; M. Malik; H. Homayoun","","2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing","20150709","2015","","","1151","1154","Big data applications share inherent characteristics that are fundamentally different from traditional desktop CPU, parallel and web service applications. They rely on deep machine learning and data mining applications. A recent trend for big data analytics is to provide heterogeneous architectures to allow support for hardware specialization to construct the right processing engine for analytics applications. However, these specialized heterogeneous architectures require extensive exploration of design aspects to find the optimal architecture in terms of performance and cost. % Considering the time dedicated to create such specialized architectures, a model that estimates the potential speedup achievable through offloading various parts of the algorithm to specialized hardware would be necessary. This paper analyzes how offloading computational intensive kernels of machine learning algorithms to a heterogeneous CPU+FPGA platform enhances the performance. We use the latest Xilinx Signboards for implementation and result analysis. Furthermore, we perform a comprehensive analysis of communication and computation overheads such as data I/O movements, and calling several standard libraries that can not be offloaded to the accelerator to understand how the speedup of each application will contribute to its overall execution in an end-to-end Hadoop MapReduce environment.","","Electronic:978-1-4799-8006-2; POD:978-1-4799-8007-9","10.1109/CCGrid.2015.165","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7152609","Acceleration;Big Data;FPGA","Acceleration;Big data;Computer architecture;Field programmable gate arrays;Hardware;Kernel;Machine learning algorithms","Big Data;data analysis;data mining;field programmable gate arrays;learning (artificial intelligence);parallel processing","Big data applications;FPGA;Hadoop;Web service applications;Xilinx Zynq boards;data I/O movements;data mining applications;deep machine learning application;desktop CPU;end-to-end Hadoop MapReduce environment;heterogeneous CPU+FPGA platform;heterogeneous architectures;machine learning algorithms;machine-learning kernels;optimal architecture;processing engine","","7","","10","","","","4-7 May 2015","","IEEE","IEEE Conferences"
"Low-cost high-performance computing via consumer GPUs","P. Somaru; I. Obeid; J. Picone","The Neural Engineering Data Consortium, Temple University, USA","2016 IEEE Signal Processing in Medicine and Biology Symposium (SPMB)","20170209","2016","","","1","2","Big data and machine learning are rapidly developing fields with evolving and increasingly diverse hardware requirements. The goal of this project was to demonstrate that an enterprise-ready, Warewulf-based HPC compute cluster could support heterogeneous hardware via the integration of a GPU compute server. The benefits of this were two-fold. First, the integration of the GPU compute server into the cluster would validate its ability to support heterogeneous hardware, demonstrating that our approach to HPC cluster management is an effective long-term strategy. Second, a GPU compute server would significantly reduce the runtime of experiments, increasing the rate at which research advancements are made.","","Electronic:978-1-5090-6713-8; POD:978-1-5090-6714-5","10.1109/SPMB.2016.7846867","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846867","","Big data;Graphics processing units;Hardware;Random access memory;Runtime;Servers;Writing","Big Data;graphics processing units;learning (artificial intelligence);parallel processing","GPU;Warewulf-based HPC;big data;low-cost high-performance computing;machine learning","","","","","","","","3-3 Dec. 2016","","IEEE","IEEE Conferences"
"A Flood Forecasting Model Based on Deep Learning Algorithm via Integrating Stacked Autoencoders with BP Neural Network","F. Liu; F. Xu; S. Yang","Coll. of Comput. & Inf., Hohai Univ., Nanjing, China","2017 IEEE Third International Conference on Multimedia Big Data (BigMM)","20170703","2017","","","58","61","Artificial neural network (ANN) has been widely applied in flood forecasting and got good results. However, it can still not go beyond one or two hidden layers for the problematic non-convex optimization. This paper proposes a deep learning approach by integrating stacked autoencoders (SAE) and back propagation neural networks (BPNN) for the prediction of stream flow, which simultaneously takes advantages of the powerful feature representation capability of SAE and superior predicting capacity of BPNN. To further improve the non-linearity simulation capability, we first classify all the data into several categories by the K-means clustering. Then, multiple SAE-BP modules are adopted to simulate their corresponding categories of data. The proposed approach is respectively compared with the support-vector-machine (SVM) model, the BP neural network model, the RBF neural network model and extreme learning machine (ELM) model. The experimental results show that the SAE-BP integrated algorithm performs much better than other benchmarks.","","Electronic:978-1-5090-6549-3; POD:978-1-5090-6550-9","10.1109/BigMM.2017.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966716","artificial neural network;deep learning;flood forecasting;stacked autoencoders","Data models;Forecasting;Machine learning;Neural networks;Predictive models;Support vector machines;Training","backpropagation;concave programming;feature extraction;floods;forecasting theory;geophysics computing;pattern classification;pattern clustering","BP neural network;BPNN;K-means clustering;SAE-BP integrated algorithm;backpropagation neural networks;data classification;deep learning algorithm;feature representation;flood forecasting model;nonconvex optimization;nonlinearity simulation;stacked autoencoders;stream flow prediction","","","","","","","","19-21 April 2017","","IEEE","IEEE Conferences"
"Scaling GMM Expectation Maximization algorithm using bulk synchronous Parallel approach","A. A. Ratnaparkhi; E. Pilli; R. C. Joshi","Department of Computer Science and Engineering, Graphic Era University, Dehradun, India","2015 International Conference on Green Computing and Internet of Things (ICGCIoT)","20160114","2015","","","558","562","We have provided a parallel implementation of Gaussian Mixture Model (GMM) Expectation Maximization algorithm using Apache Hama Bulk synchronous Parallel approach. Apache Hama is suitable for iterative, compute intensive tasks. EM is iterative algorithm which converges to local minimum after many iterations. We have provided approach for distributing workload for Expectation and Maximization tasks on cluster nodes in case of big data. The approach is compared with Hadoop MaprRduce and Apache Spark implementations, using different datasets.","","Electronic:978-1-4673-7910-6; POD:978-1-4673-7911-3; USB:978-1-4673-7909-0","10.1109/ICGCIoT.2015.7380527","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7380527","big data algorithms;bulk synchronous parallel;clustering;expectation maximization;parallel EM","Clustering algorithms;Computational modeling;Machine learning algorithms;Peer-to-peer computing;Probability;Sparks;Synchronization","Gaussian processes;data handling;expectation-maximisation algorithm;iterative methods;mixture models;parallel processing","Apache Spark implementations;Apache hama bulk synchronous parallel approach;EM;GMM expectation maximization algorithm;Gaussian mixture model;Hadoop MapReduce;compute intensive tasks;iterative algorithm","","1","","22","","","","8-10 Oct. 2015","","IEEE","IEEE Conferences"
"A Comprehensive Big-Data-Based Monitoring System for Yield Enhancement in Semiconductor Manufacturing","K. Nakata; R. Orihara; Y. Mizuoka; K. Takagi","Knowledge Media Laboratory, Corporate Research and Development Center, Toshiba Corporation, Kawasaki, Japan","IEEE Transactions on Semiconductor Manufacturing","20171023","2017","30","4","339","344","In this paper, we focus on yield analysis task where engineers identify the cause of failure from wafer failure map patterns and manufacturing histories. We organize yield analysis task into the following three stages, namely, failure map pattern monitoring, failure cause identification, and failure recurrence monitoring, and incorporate machine learning and data mining technologies into each stage to support engineers' work. The important point is that big data analysis enables comprehensive and long-term monitoring automation. We make use of fast and scalable methods of clustering and pattern mining and realize daily comprehensive monitoring with massive manufacturing data. We also apply deep learning, which has been an innovative core technology of machine learning in recent years, to classification of wafer failure map patterns, and explore its performance in detail. Finally, these machine learning and data mining techniques are integrated into an automated monitoring system with interfaces familiar to engineers to attain large yield enhancement.","0894-6507;08946507","","10.1109/TSM.2017.2753251","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8039264","Data mining;deep learning;machine learning;pattern recognition;semiconductor defects","Data mining;Machine learning;Monitoring;Neural networks;Pattern recognition;Semiconductor device manufacture;Semiconductor device modeling","Big Data;data mining;learning (artificial intelligence);production engineering computing;semiconductor device manufacture;semiconductor industry","automated monitoring system;big data analysis;clustering pattern mining;comprehensive big-data;daily comprehensive monitoring;data mining techniques;data mining technologies;failure cause identification;failure map pattern monitoring;failure recurrence monitoring;incorporate machine learning;long-term monitoring automation;manufacturing histories;massive manufacturing data;semiconductor manufacturing;wafer failure map patterns","","1","","","","","20170918","Nov. 2017","","IEEE","IEEE Journals & Magazines"
"Energy efficient VLSI circuits for machine learning on-chip","Hao Yu","Nanyang Technological University, China","2017 International Symposium on VLSI Design, Automation and Test (VLSI-DAT)","20170608","2017","","","1","1","The machine-learning based data analytics to support a cloud intelligence (such as Google's αGo) has already gone beyond the scalability of the present computing technology and architecture. The current deep learning based method is not efficient and requires huge consumption of data and power, which has a long latency running on data servers. With the emergence of autonomous vehicles, unmanned aerial vehicles and robotics, there is a huge demand to only analyze a necessary sensed data with small latency and low power at edge devices. In this talk, we will discuss efficient machine-learning algorithms such as fast least-squares method, binary and tensory convolutional neural network method, with according prototyping accelerator developed in FPGA and CMOS-ASIC chips, which has potential to outperform traditional GPU devices. The mapping on future RRAM device will be also briefly addressed.","","Electronic:978-1-5090-3969-2; POD:978-1-5090-3970-8","10.1109/VLSI-DAT.2017.7939671","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7939671","","","","","","","","","","","","24-27 April 2017","","IEEE","IEEE Conferences"
"Big data gathering and mining pipelines for CRM using open-source","K. Li; V. Deolalikar; N. Pradhan","Search and Data Mining, Groupon Palo Alto, CA 94306","2015 IEEE International Conference on Big Data (Big Data)","20151228","2015","","","2936","2938","Customer Relationship Management (CRM) is currently the fastest growing sector of enterprise software, estimated to increase to $36.5B worldwide by 2017. CRM technologies increasingly use data mining primitives across multiple applications. At the same time, the growth of big data has led to the evolution of an open source big data software stack (primarily powered by Apache software) that rivals traditional enterprise database (RDBMS) stacks. New technologies such as Kafka, Storm, HBase have significantly enriched this open source stack, alongside more established technologies such as Hadoop MapReduce and Mahout. Today, enterprises have a choice to make regarding which stack they will choose to power their big data applications. However, there are no published studies in literature on enterprise big data pipelines built using open source components supporting CRM. Specific questions that enterprises have include: how is the data processed and analyzed in such pipelines? What are the building blocks of such pipelines? How long does each step of this processing take? In this work, we answer these questions for a large scale (serving over a 100M customers) industrial CRM pipeline that incorporates data mining, and serves several applications. Our pipeline has, broadly, two parts. The first is a data gathering part that uses Kafka, Storm, and HBase. The second is a data mining part that uses Mahout and Hadoop MapReduce. We also provide timings for common tasks in the second part such as data preprocessing for machine learning, clustering, reservoir sampling, and frequent itemset extraction.","","Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9","10.1109/BigData.2015.7364128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7364128","","Benchmark testing;Big data;Customer relationship management;Data mining;Pipelines;Software;Storms","Big Data;customer relationship management;data mining;public domain software","Apache software;Big Data applications;Big Data gathering;CRM technologies;HBase;Hadoop MapReduce;Kafka;Mahout;RDBMS;Storm;clustering;customer relationship management;data mining;data preprocessing;enterprise database;enterprise software;frequent itemset extraction;machine learning;mining pipelines;open source Big Data software stack;open source components;reservoir sampling","","1","","15","","","","Oct. 29 2015-Nov. 1 2015","","IEEE","IEEE Conferences"
"A Domain-Driven, Generative Data Model for Big Pet Store","R. J. Nowling; J. Vyas","Red Hat Inc., Raleigh, NC, USA","2014 IEEE Fourth International Conference on Big Data and Cloud Computing","20150209","2014","","","49","55","Generating large amounts of semantically-rich data for testing big data workflows is paramount for scalable performance benchmarking and quality assurance in modern machine-learning and analytics workloads. The most obvious use case for such a generative algorithm is in conjunction with a big data application blueprint, which can be used by developers (to test their emerging big data solutions) as well as end users (as a starting point for validating infrastructure installations, building novel applications, and learning analytics methods). We present a new domain-driven, generative data model for Big Pet Store, a big data application blueprint for the Hadoop ecosystem included in the Apache Big Top distribution. We describe the model and demonstrate its ability to generate semantically-rich data at variable scale ranging from a single machine to a large cluster. We validate the model by using the generated data to answer questions about customer locations and purchasing habits for a fictional targeted advertising campaign, a common business use case.","","Electronic:978-1-4799-6719-3; POD:978-1-4799-6720-9","10.1109/BDCloud.2014.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7034765","benchmarking;big data;data generation;probabilistic models;synthetic data sets;testing","Benchmark testing;Big data;Data models;Generators;Hidden Markov models;Probability density function","Big Data;data models;learning (artificial intelligence);program testing;public domain software;quality assurance;software quality;workflow management software","Apache big top distribution;Big Data application blueprint;Big data workflow testing;BigPetStore;Hadoop ecosystem;analytics workloads;domain-driven generative data model;generative algorithm;machine-learning;quality assurance;scalable performance benchmarking;semantically-rich data","","1","","26","","","","3-5 Dec. 2014","","IEEE","IEEE Conferences"
"CortexSuite: A synthetic brain benchmark suite","S. Thomas; C. Gohkale; E. Tanuwidjaja; T. Chong; D. Lau; S. Garcia; M. Bedford Taylor","University of San Diego, University of California, San Diego","2014 IEEE International Symposium on Workload Characterization (IISWC)","20141215","2014","","","76","79","These days, many traditional end-user applications are said to “run fast enough” on existing machines, so the search continues for novel applications that can leverage the new capabilities of our evolving hardware. Foremost of these potential applications are those that are clustered around information processing capabilities that humans have today but are lacking in computers. The fact that brains can perform these computations serves as an existence proof that these applications are realizable. At the same time, we often discover that the human nervous system, with its 80 billion neurons, on some metrics, is more powerful and energy-efficient than today's machines. Both of these aspects make this class of applications a desirable target for an architectural benchmark suite, because there is evidence that these applications are both useful and computationally challenging. This paper details CortexSuite, a Synthetic Brain Benchmark Suite, which seeks to capture this workload. We classify and identify benchmarks within CortexSuite by analogy to the human neural processing function. We use the major lobes of the cerebral cortex as a model for the organization and classification of data processing algorithms. To be clear, our goal is not to emulate the brain at the level of the neuron, but rather to collect together synthetic, man-made algorithms that have similar function and have met with success in the real world. We consulted six world-class machine learning and computer vision researchers, who collectively hold 83,091 citations across their distinct subareas, asking them to identify newly emerging computationally-intensive algorithms or applications that are going to have a large impact over the next ten years. This is coupled with datasets that reflect the philosophy of practical use algorithms and are coded in “clean C” so as to make them accessible, analyzable, and usable for parallel and approximate compiler and architecture r- searchers alike.","","Electronic:978-1-4799-6454-3; POD:978-1-4799-6455-0; USB:978-1-4799-6453-6","10.1109/IISWC.2014.6983043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983043","","Algorithm design and analysis;Benchmark testing;Classification algorithms;Computer vision;Data processing;Image resolution;Machine learning algorithms","brain;computer vision;learning (artificial intelligence);natural language processing;neurophysiology","C language;CortexSuite;cerebral cortex;computer vision;data processing algorithms;human nervous system;human neural processing function;information processing capabilities;machine learning;man-made algorithms;neuron;synthetic brain benchmark suite","","6","","9","","","","26-28 Oct. 2014","","IEEE","IEEE Conferences"
"Learning-Based Memory Allocation Optimization for Delay-Sensitive Big Data Processing","L. Tsai; H. Franke; C. S. Li; W. Liao","Graduate Institute of Electrical Engineering, National Taiwan University, Taipei, Taiwan","IEEE Transactions on Parallel and Distributed Systems","20180508","2018","29","6","1332","1341","Optimal resource provisioning is essential for scalable big data analytics. However, it has been difficult to accurately forecast the resource requirements before the actual deployment of these applications as their resource requirements are heavily application and data dependent. This paper identifies the existence of <italic>effective </italic> memory resource requirements for most of the big data analytic applications running inside JVMs in distributed Spark environments. Provisioning memory less than the <italic>effective</italic> memory requirement may result in rapid deterioration of the application execution in terms of its total execution time. A machine learning-based prediction model is proposed in this paper to forecast the <italic>effective</italic> memory requirement of an application given its service level agreement. This model captures the memory consumption behavior of big data applications and the dynamics of memory utilization in a distributed cluster environment. With an accurate prediction of the <italic>effective</italic> memory requirement, it is shown that up to 60 percent savings of the memory resource is feasible if an execution time penalty of 10 percent is acceptable. The accuracy of the model is evaluated on a physical Spark cluster with 128 cores and 1TB of total memory. The experiment results show that the proposed solution can predict the minimum required memory size for given acceptable delays with high accuracy, even if the behavior of target applications is unknown during the training of the model.","1045-9219;10459219","","10.1109/TPDS.2018.2800011","Ministry of Science and Technology, Taiwan, R.O.C.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8276656","Big data;garbage collection;memory over-commitment;modeling;performance-cost tradeoff;profiling;spark","Big Data;Java;Memory management;Predictive models;Resource management;Sparks;Task analysis","","","","","","","","","20180131","June 1 2018","","IEEE","IEEE Journals & Magazines"
"Network security and anomaly detection with Big-DAMA, a big data analytics framework","P. Casas; F. Soro; J. Vanerio; G. Settanni; A. D'Alconzo","AIT Austrian Institute of Technology, Austria","2017 IEEE 6th International Conference on Cloud Networking (CloudNet)","20171019","2017","","","1","7","The complexity of the Internet and the volume of network traffic have dramatically increased in the last few years, making it more challenging to design scalable Network Traffic Monitoring and Analysis (NTMA) systems. Critical NTMA applications such as the detection of network attacks and anomalies require fast mechanisms for on-line analysis of thousands of events per second, as well as efficient techniques for off-line analysis of massive historical data. The high-dimensionality of network data provided by current network monitoring systems opens the door to the massive application of machine learning approaches to improve the detection and classification of network attacks and anomalies, but this higher dimensionality comes with an extra data processing overhead. In this paper we present Big-DAMA, a big data analytics framework (BDAF) for NTMA applications. Big-DAMA is a flexible BDAF, capable to analyze and store big amounts of both structured and unstructured heterogeneous data sources, with both stream and batch processing capabilities. Big-DAMA uses off-the-shelf big data storage and processing engines to offer both stream data processing and batch processing capabilities, decomposing separate engines for stream, batch and query, following a Data Stream Warehouse (DSW) paradigm. Big-DAMA implements several algorithms for anomaly detection and network security using supervised and unsupervised machine learning (ML) models, using off-the-shelf ML libraries. We apply Big-DAMA to the detection of different types of network attacks and anomalies, benchmarking multiple supervised ML models. Evaluations are conducted on top of real network measurements collected at the WIDE backbone network, using the well-known MAWILab dataset for attacks labeling. Big-DAMA can speed up computations by a factor of 10 when compared to a standard Apache Spark cluster, and can be easily deployed in cloud environments, using hardware virtualization technology.","","Electronic:978-1-5090-4026-1; POD:978-1-5090-4027-8; USB:978-1-5090-4025-4","10.1109/CloudNet.2017.8071525","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8071525","Big-Data;High-Dimensional Data;MAWILab;Machine Learning;Network Attacks;Network Traffic Monitoring and Analysis","Batch production systems;Big Data;Feature extraction;Monitoring;Ports (Computers);Sparks","Big Data;cloud computing;computer network security;data analysis;data warehouses;learning (artificial intelligence);telecommunication traffic","Big-DAMA;Data Stream Warehouse paradigm;WIDE backbone network;anomaly detection;batch processing capabilities;big data analytics framework;big data storage;current network monitoring systems;extra data processing overhead;massive historical data;network attacks;network data;network measurements;network security;processing engines;scalable Network Traffic Monitoring;stream data processing;structured data sources;supervised machine learning;unstructured heterogeneous data sources;unsupervised machine learning","","","","","","","","25-27 Sept. 2017","","IEEE","IEEE Conferences"
"Text mining analysis of wind turbine accidents: An ontology-based framework","G. Ertek; X. Chi; A. N. Zhang; S. Asian","College of Business, Abu Dhabi University, Abu Dhabi, U.A.E.","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","3233","3241","As the global energy demand is increasing, the share of renewable energy and specifically wind energy in the supply is growing. While vast literature exists on the design and operation of wind turbines, there exists a gap in the literature with regards to the investigation and analysis of wind turbine accidents. This paper describes the application of text mining and machine learning techniques for discovering actionable insights and knowledge from news articles on wind turbine accidents. The applied analysis methods are text processing, clustering, and multidimensional scaling (MDS). These methods have been combined under a single analysis framework, and new insights have been discovered for the domain. The results of our research can be used by wind turbine manufacturers, engineering companies, insurance companies, and government institutions to address problem areas and enhance systems and processes throughout the wind energy value chain.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258305","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258305","accident analysis;ontology;text mining;wind energy;wind turbine accidents","Accidents;Ontologies;Text mining;Text processing;Wind energy;Wind turbines","data mining;learning (artificial intelligence);ontologies (artificial intelligence);pattern clustering;power engineering computing;text analysis;wind power plants","analysis methods;applied analysis methods;engineering companies;global energy demand;government institutions;insurance companies;ontology-based framework;renewable energy;single analysis framework;text mining analysis;wind energy value chain;wind turbine accidents;wind turbine manufacturers","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Big data and machine learning driven handover management and forecasting","L. L. Vy; L. P. Tung; B. S. P. Lin","College of Computer Science, National Chiao Tung University, Hsinchu, Taiwan","2017 IEEE Conference on Standards for Communications and Networking (CSCN)","20171030","2017","","","214","219","Handover (HO), as a key aspect of mobility management, plays an important role in improving network quality and mobility performance in mobile networks. Especially, in 5G networks, heterogeneous networks (HetNets) deployment of macro cells and small cells, and the deployment of ultra-dense networks (UDNs) make HO management become more challenging. Besides, the understanding of HO behavior in a cell is quite limited in existing studies, thus the forecasting HO for an individual cell is complicated, even impossible. This challenge led the authors to propose a practical process for managing and forecasting HO for a huge number of cells, based on machinelearning (ML) algorithms and big data. Moreover, based on HO forecasting, the authors also propose an approach to detect any abnormal HO in cells. The performance of the proposed approaches was evaluated by applying it to a real dataset that collected HO KPI of more than 6000 cells of a real network during the years, 2016 and 2017. The results show that the study was successful in identifying, separating HO behavior, forecasting the future number of HO attempts, and detecting abnormal HO behaviors of cells.","","Electronic:978-1-5386-3070-9; POD:978-1-5386-3071-6","10.1109/CSCN.2017.8088624","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8088624","5G;Machine Learning;SON;big data;drive test;handover;key performance indicators (KPIs)","5G mobile communication;Big Data;Clustering algorithms;Forecasting;Handover;Hidden Markov models","Big Data;learning (artificial intelligence);mobility management (mobile radio)","big data;handover management;machine learning;macro cells;mobile networks;mobility management;network quality;ultra-dense networks","","","","","","","","18-20 Sept. 2017","","IEEE","IEEE Conferences"
"Student performance analysis using clustering algorithm","I. Singh; A. S. Sabitha; A. Bansal","ASET, CSE, Amity University Uttar Pradesh, Noida, India","2016 6th International Conference - Cloud System and Big Data Engineering (Confluence)","20160709","2016","","","294","299","University and technical organizations are facing high competition and their challenge is in analyzing their performance. The major challenges are in admission, student placement and in the curriculum. The two most important process during which data's are collected and analyzed are admission and placement. The ranking of the university depends on academic performance and placement of the student. Apart from academic performance there are various other factors which help in understanding the overall performance of the student. In this research work, the data mining technique is used to understand the performance of student and group the students under various categories as a student need to consistently improve to compete in today's world.","","CD-ROM:978-1-4673-8202-1; Electronic:978-1-4673-8203-8; POD:978-1-4673-8204-5","10.1109/CONFLUENCE.2016.7508131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7508131","K-means algorithm;cluster analysis;data mining;overall performance;silhouette measure;student performance","Algorithm design and analysis;Big data;Clustering algorithms;Data mining;Decision trees;Machine learning algorithms;Performance analysis","data mining;educational administrative data processing;pattern clustering","clustering algorithm;data mining technique;student performance analysis","","","","","","","","14-15 Jan. 2016","","IEEE","IEEE Conferences"
"Automatic Bitcoin Address Clustering","D. Ermilov; M. Panov; Y. Yanovich","Bitfury, Amsterdam, Netherlands","2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)","20180118","2017","","","461","466","Bitcoin is digital assets infrastructure powering the first worldwide decentralized cryptocurrency of the same name. All history of Bitcoins owning and transferring (addresses and transactions) is available as a public ledger called blockchain. But real-world owners of addresses are not known in general. That's why Bitcoin is called pseudo-anonymous. However, some addresses can be grouped by their ownership using behavior patterns and publicly available information from off-chain sources. Blockchain-based common behavior pattern analysis (common spending and one-time change heuristics) is widely used for Bitcoin clustering as votes for addresses association, while offchain information (tags) is mostly used to verify results. In this paper, we propose to use off-chain information as votes for address separation and to consider it together with blockchain information during the clustering model construction step. Both blockchain and off-chain information are not reliable, and our approach aims to filter out errors in input data. The results of the study show the feasibility of a proposed approached for Bitcoin address clustering. It can be useful for the users to avoid insecure Bitcoin usage patterns and for the investigators to conduct a more advanced de-anonymizing analysis.","","Electronic:978-1-5386-1418-1; POD:978-1-5386-1419-8","10.1109/ICMLA.2017.0-118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8260674","Bitcoin,-blockchain,-clustering,-privacy,-anonymity","Authentication;Bitcoin;Clustering algorithms;Companies;History;Mixers","cryptography;electronic money;financial data processing;pattern clustering","Bitcoin address clustering;Bitcoin clustering;automatic Bitcoin address;blockchain information;common behavior pattern analysis;de-anonymizing analysis;digital assets infrastructure;insecure Bitcoin usage patterns;off-chain information;public ledger called blockchain;worldwide decentralized cryptocurrency","","","","","","","","18-21 Dec. 2017","","IEEE","IEEE Conferences"
"A Deep-Intelligence Framework for Online Video Processing","W. Zhang; L. Xu; Z. Li; Q. Lu; Y. Liu","China University of Petroleum","IEEE Software","20160226","2016","33","2","44","51","Video data has become the largest source of big data. Owing to video data's complexities, velocity, and volume, public security and other surveillance applications require efficient, intelligent runtime video processing. To address these challenges, a proposed framework combines two cloud-computing technologies: Storm stream processing and Hadoop batch processing. It uses deep learning to realize deep intelligence that can help reveal knowledge hidden in video data. An implementation of this framework combines five architecture styles: service-oriented architecture, publish-subscribe, the Shared Data pattern, MapReduce, and a layered architecture. Evaluations of performance, scalability, and fault tolerance showed the framework's effectiveness. This article is part of a special issue on Software Engineering for Big Data Systems.","0740-7459;07407459","","10.1109/MS.2016.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7412619","Hadoop;MapReduce;Storm;big data;cloud computing;deep learning;fault tolerance;software development;software engineering;video processing","Big data;Cloud computing;Computer architecture;Deep learning;Fault tolerance;Machine learning;Real-time systems;Scalability;Software development;Software engineering;Streaming media","Big Data;cloud computing;data handling;learning (artificial intelligence);service-oriented architecture;software fault tolerance;video signal processing","Hadoop batch processing;MapReduce;big data systems;cloud-computing technologies;deep learning;deep-intelligence framework;fault tolerance;layered architecture;online video processing;publish-subscribe style;service-oriented architecture;shared data pattern;software engineering;storm stream processing","","5","","6","","","20160218","Mar.-Apr. 2016","","IEEE","IEEE Journals & Magazines"
"Big R: Large-Scale Analytics on Hadoop Using R","O. D. L. Yejas; W. Zhuang; A. Pannu","IBM Silicon Valley Lab., San Jose, CA, USA","2014 IEEE International Congress on Big Data","20140925","2014","","","570","577","As the volume of available data continues to rapidly grow from a variety of sources, scalable and performant analytics solutions have become an essential tool to enhance business productivity and revenue. Existing data analysis environments, such as R, are constrained by the size of the main memory and cannot scale in many applications. This paper introduces Big R, a new platform which enables accessing, manipulating, analyzing, and visualizing data residing on a Hadoop cluster from the R user interface. Big R is inspired by R semantics and overloads a number of R primitives to support big data. Hence, users will be able to quickly prototype big data analytics routines without the need of learning a new programming paradigm. The current Big R implementation works on two main fronts: (1) data exploration, which enables R as a query language for Hadoop and (2) partitioned execution, allowing the execution of any R function on smaller pieces of a large dataset across the nodes in the cluster.","2379-7703;23797703","Electronic:978-1-4799-5057-7; POD:978-1-4799-5058-4","10.1109/BigData.Congress.2014.88","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906830","Big data;Machine Learning;MapReduce","Big data;Data mining;Data visualization;Database languages;Delays;Semantics;Vectors","data analysis;data visualisation;pattern clustering;public domain software;query languages;user interfaces","Big R;Hadoop cluster;R primitives;R semantics;R user interface;data analysis;data exploration;data manipulation;data visualization;large-scale analytics;partitioned execution;query language","","0","","11","","","","June 27 2014-July 2 2014","","IEEE","IEEE Conferences"
"Automotive big data: Applications, workloads and infrastructures","A. Luckow; K. Kennedy; F. Manhardt; E. Djerekarov; B. Vorster; A. Apon","Innovation Lab, BMW Group IT Research Center, Information Management Americas, Greenville, South Carolina, USA","2015 IEEE International Conference on Big Data (Big Data)","20151228","2015","","","1201","1210","Data is increasingly affecting the automotive industry, from vehicle development, to manufacturing and service processes, to online services centered around the connected vehicle. Connected, mobile and Internet of Things devices and machines generate immense amounts of sensor data. The ability to process and analyze this data to extract insights and knowledge that enable intelligent services, new ways to understand business problems, improvements of processes and decisions, is a critical capability. Hadoop is a scalable platform for compute and storage and emerged as de-facto standard for Big Data processing at Internet companies and in the scientific community. However, there is a lack of understanding of how and for what use cases these new Hadoop capabilities can be efficiently used to augment automotive applications and systems. This paper surveys use cases and applications for deploying Hadoop in the automotive industry. Over the years a rich ecosystem emerged around Hadoop comprising tools for parallel, in-memory and stream processing (most notable MapReduce and Spark), SQL and NOSQL engines (Hive, HBase), and machine learning (Mahout, MLlib). It is critical to develop an understanding of automotive applications and their characteristics and requirements for data discovery, integration, exploration and analytics. We then map these requirements to a confined technical architecture consisting of core Hadoop services and libraries for data ingest, processing and analytics. The objective of this paper is to address questions, such as: What applications and datasets are suitable for Hadoop? How can a diverse set of frameworks and tools be managed on multi-tenant Hadoop cluster? How do these tools integrate with existing relational data management systems? How can enterprise security requirements be addressed? What are the performance characteristics of these tools for real-world automotive applications? To address the last question, we utilize a standard benchmark- (TPCx-HS), and two application benchmarks (SQL and machine learning) that operate on a dataset of multiple Terabytes and billions of rows.","","Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9","10.1109/BigData.2015.7363874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363874","","Automotive applications;Big data;Business;Industries;Lakes;Vehicles","Big Data;Internet of Things;SQL;automobile industry;data handling;learning (artificial intelligence);parallel processing","Hadoop;Internet companies;Internet of things devices;NOSQL engines;SQL engines;TPCx-HS;automotive big data;automotive industry;big data processing;data discovery;intelligent services;machine learning;multitenant Hadoop cluster;online services;real-world automotive applications;scientific community;vehicle development","","8","","49","","","","Oct. 29 2015-Nov. 1 2015","","IEEE","IEEE Conferences"
"Big Data Analytics Integrating a Parallel Columnar DBMS and the R Language","Y. Zhang; C. Ordonez; W. Cabrera","Univ. of Houston, Houston, TX, USA","2016 16th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid)","20160721","2016","","","627","630","Most research has proposed scalable and parallel analytic algorithms that work outside a DBMS. On the other hand, R has become a very popular system to perform machine learning analysis, but it is limited by main memory and single-threaded processing. Recently, novel columnar DBMSs have shown to provide orders of magnitude improvement in SQL query processing speed, preserving the parallel speedup of row-based parallel DBMSs. With that motivation in mind, we present COLUMNAR, a system integrating a parallel columnar DBMS and R, that can directly compute models on large data sets stored as relational tables. Our algorithms are based on a combination of SQL queries, user-defined functions (UDFs) and R calls, where SQL queries and UDFs compute data set summaries that are sent to R to compute models in RAM. Since our hybrid algorithms exploit the DBMS for the most demanding computations involving the data set, they show linear scalability and are highly parallel. Our algorithms generally require one pass on the data set or a few passes otherwise (i.e. fewer passes than traditional methods). Our system can analyze data sets faster than R even when they fit in RAM and it also eliminates memory limitations in R when data sets exceed RAM size. On the other hand, it is an order of magnitude faster than Spark (a prominent Hadoop system) and a traditional row-based DBMS.","","Electronic:978-1-5090-2453-7; POD:978-1-5090-2454-4","10.1109/CCGrid.2016.94","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515749","","Computational modeling;Data models;Layout;Load modeling;Mathematical model;Numerical models;Random access memory","Big Data;SQL;data analysis;learning (artificial intelligence);parallel algorithms;query processing;random-access storage","Big Data analytics;Hadoop system;R calls;SQL query processing speed;UDF;hybrid algorithms;linear scalability;machine learning analysis;main memory processing;memory limitations;parallel analytic algorithms;parallel columnar DBMS language;parallel columnar R language;relational tables;row-based parallel DBMS;scalable analytic algorithms;single-threaded processing;user-defined functions","","1","","","","","","16-19 May 2016","","IEEE","IEEE Conferences"
"A Density based clustering with Artificial Immunity inspired preprocessing","S. K. Paul; P. Bhaumik","Information Technology, Tata Consultancy Services, Kolkata, India","2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","20141201","2014","","","2648","2654","In this paper we propose an algorithm which can identify varied shaped clusters from wide variety of input dataset with high degree of accuracy in presence of noise. The initial data processing module adopts a novel approach of Artificial Immune system to reduce data redundancy while preserving the original data patterns. The clustering module pursues a density based approach to identify clusters from the compressed dataset produced by the preprocessing module. We introduced several new concepts like selective Antigenic binding, Local Reachability Factor, Global Reachability Factor to effectively recognize clusters with varied shape, varied density and low intercluster separation with acceptable computational cost. We performed experimental evaluation of our algorithm with wide variety of real and synthetic dataset and obtained higher cluster success rate for all dataset when compared to DBSCAN.","","Electronic:978-1-4799-3080-7; POD:978-1-4799-3081-4; USB:978-1-4799-3079-1","10.1109/ICACCI.2014.6968258","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6968258","Artificial Immune Systems;Density based clustering algorithms;Detecting varied shaped clusters;Machine Learning;Pattern Recognition","Complexity theory;Sorting","artificial immune systems;pattern clustering;reachability analysis","DBSCAN;artificial immune system;artificial immunity inspired preprocessing;clustering module;data redundancy redundancy;density based clustering;global reachability factor;initial data processing module;local reachability factor;selective antigenic binding","","0","","19","","","","24-27 Sept. 2014","","IEEE","IEEE Conferences"
"Facebook and public health: A study to understand facebook post performance with organizations' strategy","N. Straton; R. Vatrapu; R. R. Mukkamala","Centre for Business Data Analytics, Copenhagen Business School, Denmark","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","3123","3132","This paper reports on a survey about the perceptions and practices of social media managers and experts in the area of public health. We have collected Facebook data from 153 public health care organizations and conducted a survey on them. 12% of organizations responded to the questionnaire. The survey results were combined with the findings from our previous work of applying clustering and supervised learning algorithms on big social data from the official Facebook walls of these organizations. In earlier research, we showed that the most successful strategy that leads to higher post engagement is visual content. In this paper, we investigated if organisations pursue this strategy or some other strategy that was successful and has not been uncovered by the machine learning algorithms. Performance of each organisation on Facebook is based on the number of posts (volume share) and the number of actions (value share). Calculation of performance with number of actions in the numerator and number of posts in the denominator reduces possible bias in the conclusions due to the varied size of organizations on social media. Moreover, our survey attempts to better understand the behaviour of organizations and to explain why almost half of the public health care content posted on Facebook is in the form of a short text message, where as the information can be communicated through seven other post types. Similar patterns and characteristics for different engagement clusters, also high and low performing companies suggests that a mixed-methods research approach consisting of machine learning techniques combined with expert knowledge using qualitative methods can offer important insights.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258288","Big Social Data;Facebook data;Social Media Performance;Social Media Strategy","Facebook;Organizations;Public healthcare;Twitter;Visualization","health care;learning (artificial intelligence);social networking (online);social sciences computing","Facebook data;big social data;engagement clusters;facebook post performance;high performing companies;low performing companies;machine learning algorithms;official Facebook walls;post engagement;public health care content;public health care organizations;short text message;social media managers","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Using personal preference in calculating rating scores for recommendations","C. H. Lee; Y. Y. Su; P. M. Chu; S. J. Lee","Department of Digital Content Application and Management, Wenzao Ursuline University of Languages, Kaohsiung 807, Taiwan","2016 IEEE International Conference on Information and Automation (ICIA)","20170202","2016","","","1149","1153","Online shopping is a common shopping style for human being nowadays. Rating mechanisms usually exist in most of the shopping sites. Therefore, predicting which products a customer is going to buy next from the rating information becomes possible, making recommender systems important for online shopping. The success of an online shopping site can be dominated by the quality of the recommender system involved. One factor for the quality is whether the user preference can be taken into account in the recommender system. A method that examines if a user's rating truly reflects a product's quality was proposed by Allahbakhsh and Lgnjatovic. By giving different weights to users' ratings according to the examined results, products are recommended to interested users automatically. However, the method does not consider that preferences are different among different individual users. In this paper, we propose a method which is also based on the examined results, but takes personal preference into account as well. As a result, different recommendations are preferably provided to different users. Experimental results are shown to demonstrate the effectiveness of our proposed method.","","Electronic:978-1-5090-4102-2; POD:978-1-5090-4103-9; USB:978-1-5090-4101-5","10.1109/ICInfA.2016.7831992","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7831992","machine learning;online rating;recommender system","Clustering algorithms;Companies;Mathematical model;Measurement;Motion pictures;Recommender systems;Testing","Internet;Web sites;recommender systems;relevance feedback;retail data processing","online shopping sites;personal preference;product quality;rating scores;recommendations;recommender system quality;shopping style;user ratings","","","","","","","","1-3 Aug. 2016","","IEEE","IEEE Conferences"
"Kvasir: Scalable Provision of Semantically Relevant Web Content on Big Data Framework","L. Wang; S. Tasoulis; T. Roos; J. Kangasharju","University of Cambridge, Cambridge, United Kingdom","IEEE Transactions on Big Data","20170520","2016","2","3","219","233","The Internet is overloading its users with excessive information flows, so that effective content-based filtering becomes crucial in improving user experience and work efficiency. Latent semantic analysis has long been demonstrated as a promising information retrieval technique to search for relevant articles from large text corpora. We build Kvasir, a semantic recommendation system, on top of latent semantic analysis and other state-of-the-art technologies to seamlessly integrate an automated and proactive content provision service into web browsing. We utilize the processing power of Apache Spark to scale up Kvasir into a practical Internet service. In addition, we improve the classic randomized partition tree to support efficient indexing and searching of millions of documents. Herein we present the architectural design of Kvasir, the core algorithms, along with our solutions to the technical challenges in the actual system implementation.","","","10.1109/TBDATA.2016.2557348","The Finnish Centre of Excellence in Computational Inference Research (COIN); 10.13039/501100002341 - Academy of Finland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7462177","Apache Spark;Web application;information retrieval;machine learning;random projection;semantic search","Big data;Indexing;Information retrieval;Internet;Scalability;Semantics;Sparks","Big Data;content-based retrieval;indexing;recommender systems;semantic Web;trees (mathematics)","Apache Spark;Big Data framework;Internet service;Kvasir architectural design;Web browsing;automated content provision service;classic randomized partition tree;content-based filtering;document indexing;document searching;information flows;information retrieval technique;large text corpora;latent semantic analysis;proactive content provision service;semantic recommendation system;semantic relevant Web content;work efficiency","","2","","","","","20160428","Sept. 1 2016","","IEEE","IEEE Journals & Magazines"
"Parallel Processing Systems for Big Data: A Survey","Y. Zhang; T. Cao; S. Li; X. Tian; L. Yuan; H. Jia; A. V. Vasilakos","State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China","Proceedings of the IEEE","20161019","2016","104","11","2114","2136","The volume, variety, and velocity properties of big data and the valuable information it contains have motivated the investigation of many new parallel data processing systems in addition to the approaches using traditional database management systems (DBMSs). MapReduce pioneered this paradigm change and rapidly became the primary big data processing system for its simplicity, scalability, and fine-grain fault tolerance. However, compared with DBMSs, MapReduce also arouses controversy in processing efficiency, low-level abstraction, and rigid dataflow. Inspired by MapReduce, nowadays the big data systems are blooming. Some of them follow MapReduce's idea, but with more flexible models for general-purpose usage. Some absorb the advantages of DBMSs with higher abstraction. There are also specific systems for certain applications, such as machine learning and stream data processing. To explore new research opportunities and assist users in selecting suitable processing systems for specific applications, this survey paper will give a high-level overview of the existing parallel data processing systems categorized by the data input as batch processing, stream processing, graph processing, and machine learning processing and introduce representative projects in each category. As the pioneer, the original MapReduce system, as well as its active variants and extensions on dataflow, data access, parameter tuning, communication, and energy optimizations will be discussed at first. System benchmarks and open issues for big data processing will also be studied in this survey.","0018-9219;00189219","","10.1109/JPROC.2016.2591592","CAS Interdisciplinary Innovation Team of Efficient Space Weather Forecast Models; Key Technology Research and Development Programs of Guangdong Province; National High Technology Research and Development Program of China; National Key Research and Development Program of China; 10.13039/501100001809 - National Natural Science Foundation of China; 10.13039/501100002858 - China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7547948","Big data;MapReduce;SQL;machine learning;parallel processing;survey","Benchmark testing;Big data;Computer applications;Data models;Machine learning;Parallel processing;Programming;Structured Query Language","database management systems;fault tolerance;parallel processing","DBMS;MapReduce system;batch processing;database management system;energy optimization;fine-grain fault tolerance;graph processing;low-level abstraction;machine learning processing;parallel data processing system;rigid dataflow;stream data processing","","7","","","","","20160819","Nov. 2016","","IEEE","IEEE Journals & Magazines"
"Designing a high performance cluster for large-scale SQL-on-hadoop analytics","A. Dholakia; P. Venkatachar; K. Doshi; R. Durgavajhala; S. Tate; B. Schiefer; M. Sheard; R. S. Sagar","Lenovo, Morrisville, NC, USA","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","1701","1703","Executing and optimizing SQL analytics on Data Lakes and Enterprise Data Warehouses (EDW) are areas of significant and growing interest. Achieving high performance for SQL analytics on large-scale data repositories remains a key challenge for data practitioners. The SQL-on-Hadoop Analytics solution described in this paper is very well suited for implementing the infrastructure to support these modern analytics initiatives while meeting requirements such as higher performance, lower cost, more efficient data center footprint, lower power consumption, appropriate storage needs and increased reliability. By using a TPC-DS derived workload applied to 100 TB of data, the work demonstrates for the first time the feasibility of designing such an extremely high-performance cluster. Furthermore, it enables investigation of large-scale SQL-on-Hadoop systems as the Spark SQL framework matures and enables similar investigations into machine learning and related Spark capabilities.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258110","Apache Spark;High-Performance Cluster;High-Speed Network;NVMe storage;SQL on Hadoop","Benchmark testing;Big Data;Computer architecture;Hardware;Nonvolatile memory;Software;Sparks","SQL;computer centres;data handling;data mining;data warehouses;parallel processing;query processing;relational databases","SQL analytics;SQL-on-Hadoop Analytics solution;SQL-on-Hadoop systems;Spark SQL framework matures;achieving high performance;data practitioners;high performance cluster;high-performance cluster;key challenge;large-scale data repositories;lower power consumption;memory size 100.0 TByte;modern analytics initiatives;significant growing interest","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Plant identification with noisy web data","W. Y. Zhang; X. S. Hua","Monta Vista High School","2014 IEEE International Conference on Multimedia and Expo (ICME)","20140908","2014","","","1","6","One of the main problems in image based plant identification has been the lack of quality training image data. A few attempts for solving this problem through generating high quality plant images from crowd sourced Web image collections like Flickr are proposed in this paper. These methods try to automatically identify correct and informative training images from those Web images, which typically have very noisy metadata (for example, user tags in Flickr), to enhance existing manually labeled training set. Firstly, for each plant, a set of images is collected from searching Flickr by using the plant name as the query. Then, images are clustered into visually consistent clusters, and in each cluster hopefully a majority of the images are all relevant or irrelevant to the particular plant. From these clusters, a managed plant image data set from ImageCLEF is used as reference to automatically select the highest quality cluster for each plant. The image quality of the selected clusters is further improved by two algorithms: an iterative method and image similarity based ranking. We show that the larger training data set automatically selected by this method significantly increases the accuracy of image based plant identification. In addition, this approach is a generic solution to almost all image recognition problems as long as additional (noisy) training data can be obtained from the Internet automatically.","1945-7871;19457871","Electronic:978-1-4799-4761-4; POD:978-1-4799-4760-7","10.1109/ICME.2014.6890180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6890180","Image classification;crowd sourced big data;machine learning;plant identification","Accuracy;Clustering algorithms;Data models;Noise measurement;Support vector machines;Training;Training data","Internet;agricultural engineering;image retrieval;iterative methods;object detection;object recognition;pattern clustering;social networking (online)","Flickr;ImageCLEF;Internet;Web image collection;automatic informative training image identification;image based plant identification;image clustering;image quality;image recognition problem;image similarity based ranking;iterative method;labeled training set;noisy Web data;query processing;visually consistent cluster","","0","","12","","","","14-18 July 2014","","IEEE","IEEE Conferences"
"Large-scale learning with AdaGrad on Spark","A. T. Hadgu; A. Nigam; E. Diaz-Aviles","L3S Research Center, Hannover, Germany","2015 IEEE International Conference on Big Data (Big Data)","20151228","2015","","","2828","2830","Stochastic Gradient Descent (SGD) is a simple yet very efficient online learning algorithm for optimizing convex (and often non-convex) functions and one of the most popular stochastic optimization methods in machine learning today. One drawback of SGD is that it is sensitive to the learning rate hyper-parameter. The Adaptive Sub-gradient Descent, AdaGrad, dynamically incorporates knowledge of the geometry of the data observed in earlier iterations to calculate a different learning rate for every feature. In this work, we implement a distributed version of AdaGrad for large-scale machine learning tasks using Apache Spark. Apache Spark is a fast cluster computing engine that provides similar scalability and fault tolerance properties to MapReduce, but in contrast to Hadoop's two-stage disk-based MapReduce paradigm, Spark's multi-stage in-memory primitives allow user programs to load data into a cluster's memory and query it repeatedly, which makes it ideal for building scalable machine learning applications. We empirically evaluate our implementation on large-scale real-world problems in the machine learning canonical tasks of classification and regression. Comparing our implementation of AdaGrad with the SGD scheduler currently available in Spark's Machine Learning Library (MLlib), we experimentally show that AdaGrad saves time by avoiding manually setting a learning-rate hyperparameter, converges fast and can even achieve better generalization errors.","","Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9","10.1109/BigData.2015.7364091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7364091","Adaptive gradient;Distributed machine learning;Spark","Aggregates;History;Sparks;Standards;Stochastic processes;Support vector machines;Training","convex programming;data handling;fault tolerant computing;geometry;gradient methods;learning (artificial intelligence);parallel processing;stochastic programming;user interfaces","AdaGrad;Apache Spark;Hadoop;MLlib;SGD;cluster computing engine;convex functions;disk-based MapReduce paradigm;fault tolerance properties;geometry;large-scale learning;machine learning library;online learning algorithm;stochastic gradient descent;stochastic optimization;user programs","","2","","7","","","","Oct. 29 2015-Nov. 1 2015","","IEEE","IEEE Conferences"
"Predictive analysis of diabetic patient data using machine learning and Hadoop","G. D. Kalyankar; S. R. Poojara; N. V. Dharwadkar","Dept. of Computer Science and Engineering, Rajarambapu Institute of Technology, Sakhrale, Sangli Dist","2017 International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)","20171005","2017","","","619","624","Now days from health care industries large volume of data is generating. It is necessary to collect, store and process this data to discover knowledge from it and utilize it to take significant decisions. Diabetic Mellitus (DM) is from the Non Communicable Diseases (NCD), and lots of people are suffering from it. Now days, for developing countries such as India, DM has become a big health issue. The DM is one of the critical diseases which has long term complications associated with it and also follows with various health problems. With the help of technology, it is necessary to build a system that store and analyze the diabetic data and predict possible risks accordingly. Predictive analysis is a method that integrates various data mining techniques, machine learning algorithms and statistics that use current and past data sets to gain insight and predict future risks. In this work machine learning algorithm in Hadoop MapReduce environment are implemented for Pima Indian diabetes data set to find out missing values in it and to discover patterns from it. This work will be able to predict types of diabetes are widespread, related future risks and according to the risk level of patient the type of treatment can be provided.","","DVD:978-1-5090-3241-9; Electronic:978-1-5090-3243-3; POD:978-1-5090-3244-0; Paper:978-1-5090-3242-6","10.1109/I-SMAC.2017.8058253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8058253","Hadoop;Healthcare industry;Machine Learning;MapReduce;Predictive Analysis","Algorithm design and analysis;Classification algorithms;Clustering algorithms;Data mining;Diabetes;Unsupervised learning","data analysis;data handling;data mining;diseases;health care;learning (artificial intelligence);medical information systems;parallel processing;patient diagnosis;pattern classification","DM;Diabetic Mellitus;Hadoop MapReduce environment;NonCommunicable Diseases;Pima Indian diabetes data;data mining techniques;data processing;data sets;diabetic patient data;machine learning algorithms;predictive analysis;risk level;statistics","","","","","","","","10-11 Feb. 2017","","IEEE","IEEE Conferences"
"A randomized approach to large-scale subspace clustering","P. A. Traganitis; G. B. Giannakis","Dept. of ECE & Digital Technology Center, Univ. of Minnesota, USA","2016 50th Asilomar Conference on Signals, Systems and Computers","20170306","2016","","","1019","1023","Subspace clustering has become a popular tool for clustering high-dimensional non-linearly separable data. However, state-of-the-art subspace clustering algorithms do not scale well as the number of data increases. The present paper puts forth a novel randomized subspace clustering scheme for high-volume data based on random projections. Performance of the proposed method is assessed via numerical tests, and is compared with state-of-the-art subspace clustering and large-scale subspace clustering methods.","","DVD:978-1-5386-3952-8; Electronic:978-1-5386-3954-2; POD:978-1-5386-3955-9","10.1109/ACSSC.2016.7869522","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7869522","Subspace clustering;big data;random projections;sketching","Clustering algorithms;Complexity theory;Machine learning algorithms;Optimization;Principal component analysis;Signal processing algorithms;Sparse matrices","data handling;pattern clustering","high-dimensional nonlinearly separable data clustering;high-volume data;large-scale subspace clustering;random projection;randomized approach","","","","","","","","6-9 Nov. 2016","","IEEE","IEEE Conferences"
"Evaluating parallel logistic regression models","H. Peng; D. Liang; C. Choi","HTC Res. Center, Beijing, China","2013 IEEE International Conference on Big Data","20131223","2013","","","119","126","Logistic regression (LR) has been widely used in applications of machine learning, thanks to its linear model. However, when the size of training data is very large, even such a linear model can consume excessive memory and computation time. To tackle both resource and computation scalability in a big-data setting, we evaluate and compare different approaches in distributed platform, parallel algorithm, and sublinear approximation. Our empirical study provides design guidelines for choosing the most effective combination for the performance requirement of a given application.","","Electronic:978-1-4799-1293-3; POD:978-1-4799-1294-0","10.1109/BigData.2013.6691743","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691743","Big Data;Logistic Regression Model;Parallel Computing;Sublinear Method","Algorithm design and analysis;Approximation algorithms;Computational modeling;Logistics;Machine learning algorithms;Sparks;Vectors","Big Data;approximation theory;design;learning (artificial intelligence);parallel algorithms;regression analysis","Big Data;computation scalability;design guidelines;distributed platform;linear model;machine learning;parallel algorithm;parallel logistic regression models;performance requirement;sublinear approximation","","3","","24","","","","6-9 Oct. 2013","","IEEE","IEEE Conferences"
"A cascade deep neuro-fuzzy system for high-dimensional online possibilistic fuzzy clustering","Z. Hu; Y. V. Bodyanskiy; O. K. Tyshchenko","School of Educational Information Technology, Central China Normal University, Wuhan, China","2016 XIth International Scientific and Technical Conference Computer Sciences and Information Technologies (CSIT)","20161013","2016","","","119","122","A cascade deep learning system (based on neuro-fuzzy nodes) and its online learning procedure are proposed in this paper. The system is based on nodes of a special type. A goal function of a special type is used for possibilistic high-dimensional fuzzy clustering. To estimate a clustering quality of data processing, an optimal value of a cluster validity index is used.","","","10.1109/STC-CSIT.2016.7589884","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589884","Computational Intelligence;Data Stream Processing;Evolving System;Fuzzy Clustering;Machine Learning;Neuro-Fuzzy System","Adaptive systems;Clustering algorithms;Computer architecture;Fuzzy systems;Indexes;Machine learning;Neural networks","fuzzy neural nets;fuzzy set theory;learning (artificial intelligence);pattern clustering;possibility theory","cascade deep learning system;cascade deep neuro-fuzzy system;cluster validity index;clustering quality;data processing;goal function;high-dimensional online possibilistic fuzzy clustering;online learning procedure","","2","","","","","","6-10 Sept. 2016","","IEEE","IEEE Conferences"
"Big Universe, Big Data: Machine Learning and Image Analysis for Astronomy","J. Kremer; K. Stensbo-Smidt; F. Gieseke; K. S. Pedersen; C. Igel","University of Copenhagen","IEEE Intelligent Systems","20170327","2017","32","2","16","22","Astrophysics and cosmology are rich with data. The advent of wide-area digital cameras on large aperture telescopes has led to ever more ambitious surveys of the sky. Data volumes of entire surveys a decade ago can now be acquired in a single night, and real-time analysis is often desired. Thus, modern astronomy requires big data know-how, in particular, highly efficient machine learning and image analysis algorithms. But scalability isn't the only challenge: astronomy applications touch several current machine learning research questions, such as learning from biased data and dealing with label and measurement noise. The authors argue that this makes astronomy a great domain for computer science research, as it pushes the boundaries of data analysis. They focus here on exemplary results, discuss main challenges, and highlight some recent methodological advancements in machine learning and image analysis triggered by astronomical applications.","1541-1672;15411672","","10.1109/MIS.2017.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7887648","astronomy;big data;computer vision;intelligent systems;machine learning","Astronomy;Big data;Computer vision;Extrasolar planets;Extraterrestrial measurements;Image analysis;Machine learning;Telescopes","Big Data;astronomical image processing;learning (artificial intelligence)","Big Data;aperture telescopes;astronomy;astrophysics;computer science research;cosmology;data analysis;digital cameras;image analysis;image analysis algorithms;machine learning","","","","","","","","Mar.-Apr. 2017","","IEEE","IEEE Journals & Magazines"
"Table of contents","","","2017 13th International Wireless Communications and Mobile Computing Conference (IWCMC)","20170720","2017","","","1","47","The following topics are dealt with: theoretical system; multimedia applications; emerging practical systems; D2D; channel estimation; WSN security; modulation; signal processing; resource allocation; network security; vehicular security; IoT; vehicle models; high-density networks; MIMO; transmission techniques; NG signaling; heterogeneous mechanisms; system security; routing protocols; network coding; millimeter wave; OFDM; Internet of Things; clustering mechanisms; vehicle performance evaluation; cloud-fog networking; machine learning; localization; MAC protocols; scheduling protocols; smart cities; physical layer security; QoE;QoS; wireless access networks; m-health; 5G communication; clustering algorithms; distributed algorithms; e-health; WSN-based monitoring systems; Big Data security; next generation networking; next generation wireless communications; secure communication; and mobile computing.","","Electronic:978-1-5090-4372-9; POD:978-1-5090-4373-6","10.1109/IWCMC.2017.7986250","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7986250","","","5G mobile communication;Big Data;Internet of Things;MIMO communication;OFDM modulation;access protocols;channel estimation;cloud computing;learning (artificial intelligence);mobile computing;next generation networks;quality of experience;quality of service;routing protocols;signal processing;smart cities;telecommunication scheduling;telecommunication security;wireless sensor networks","5G communication;Big Data security;D2D;Internet of Things;IoT;MAC protocols;MIMO;NG signaling;OFDM;QoE;QoS;WSN security;WSN-based monitoring systems;channel estimation;cloud-fog networking;clustering algorithms;clustering mechanisms;distributed algorithms;e-health;emerging practical systems;heterogeneous mechanisms;high-density networks;localization;m-health;machine learning;millimeter wave;mobile computing;modulation;multimedia applications;network coding;network security;next generation networking;next generation wireless communications;physical layer security;resource allocation;routing protocols;scheduling protocols;secure communication;signal processing;smart cities;system security;transmission techniques;vehicle models;vehicle performance evaluation;vehicular security;wireless access networks","","","","","","","","26-30 June 2017","","IEEE","IEEE Conferences"
"Smart-MLlib: A High-Performance Machine-Learning Library","D. Siegal; J. Guo; G. Agrawal","Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA","2016 IEEE International Conference on Cluster Computing (CLUSTER)","20161208","2016","","","336","345","As the popularity of big data analytics has continued to grow, so has the need for accessible and scalable machine-learning implementations. In recent years, Apache Spark's machine-learning library, MLlib, has been used to fulfill this need. Though Spark outperforms Hadoop, it is not clear if it is the best performing underlying middleware to support machine learning implementations. Building on a C++ and MPI based middleware system Smart, we present a machine-learning library prototype (Smart-MLlib). Like MLlib, Smart MLlib allows machine learning implementations to be invoked from a Scala program, and with a very similar API. To test our library's performance, we built four machine-learning applications that are also provided in Spark's MLlib: k-means clustering, linear regression, Gaussian mixture models, and support vector machines. On average, we outperformed Spark's MLlib by over 800%. Our library also scaled better than Spark's MLlib for every application tested. Thus, the new machine-learning library enables higher performance than Spark's MLlib without sacrificing the easy-to-use API.","","Electronic:978-1-5090-3653-0; POD:978-1-5090-3654-7","10.1109/CLUSTER.2016.49","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776527","","Clustering algorithms;Distributed databases;Iterative methods;Libraries;Programming;Sparks","Big Data;Gaussian processes;data analysis;learning (artificial intelligence);middleware;mixture models;pattern clustering;support vector machines","API;Apache Spark;C++ based middleware system;Gaussian mixture models;Hadoop;MPI based middleware system;Scala program;Smart-MLlib;big data analytics;k-means clustering;linear regression;machine-learning library;support vector machines","","1","","","","","","12-16 Sept. 2016","","IEEE","IEEE Conferences"
"Extracting route patterns of vessels from AIS data by using topic model","I. Fujino; C. Claramunt; A. O. Boudraa","School of Information and Telecommunication Engineering, Tokai University, Tokyo, Japan","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","4744","4746","Automatic Identification System (AIS) provides realtime information of moving vessels in the sea of the whole world. The AIS data can be exploited for vessel tracking, collision avoidance, traffic management and maritime surveillance. Although the AIS data accumulated from vessels over the area of interest may become a very large amount, clustering of these AIS data leads to benefit of knowledge discovery of maritime traffic and detection of abnormal events. On the other side, in the realm of machine learning and natural language processing, topic model provides novel approaches for extracting the topics that are implicit in massive and unstructured collection of documents. The most common implementation of topic model is LDA algorithm. From these facts, we are seized to apply topic model to solve route extraction problem. In this paper, we will describe our approach for route extraction from AIS data by using vector quantization and topic model at first. Then we will show some experimental results of route patterns extraction with a practical AIS data set.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258528","Automatic Identification System (AIS);Route Pattern Extraction;topic model;vector quantization","Artificial intelligence;Data mining;Data models;Finite element analysis;Natural language processing;Probability distribution;Vector quantization","data mining;learning (artificial intelligence);marine engineering;natural language processing;pattern clustering;ships;traffic engineering computing;vector quantisation;vehicle routing","AIS data;Automatic Identification System;LDA algorithm;abnormal events detection;clustering;collision avoidance;knowledge discovery;machine learning;maritime surveillance;maritime traffic;natural language processing;route patterns extraction;topic model;traffic management;vector quantization;vessel tracking","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Nonnegative matrix factorization with adaptive neighbors","S. Huang; Z. Xu; F. Wang","Big Data Res. Center, School Comp. Sci. Eng., Univ. of Electr. Sci. and Tech. of China","2017 International Joint Conference on Neural Networks (IJCNN)","20170703","2017","","","486","493","Nonnegative Matrix factorization (NMF) and its graph regularized extensions have been playing an outstanding role in machine learning and data mining. Recent studies of graph regularized NMF have focused on the application for clustering algorithms. The clustering results of these methods highly depend on the data similarity learning since the data group is utilized based on the input data similarity matrix. Previous graph regularized NMF usually construct the data graph by considering the K-Nearest Neighbors (KNN) which may mislead the factorization since the nearest neighbors may belong to different clusters. That is, it is not a good similarity measurement to construct the data graph by considering the KNN. In this paper, we present NMF with Adaptive Neighbors (NMFAN) for clustering. NMFAN learns the data similarity matrix by assigning the adaptive and optimal neighbors for each data point by exploring the local connectivity of data. It is based on the assumption that the data points with a smaller distance should have a larger probability to be neighbors. Furthermore, in order to achieve the ideal neighbors assignment, we constrain the data similarity matrix such that the neighbors assignment becomes an adaptive process, thus an ideal neighbors assignment can be expected. In order to solve the optimization problem of our method, an efficient iterative updating algorithm is proposed and its convergence is also guaranteed theoretically. Experiments on benchmark data sets demonstrate the effectiveness of the proposed method.","","Electronic:978-1-5090-6182-2; POD:978-1-5090-6183-9","10.1109/IJCNN.2017.7965893","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965893","","","convergence;data mining;iterative methods;learning (artificial intelligence);matrix decomposition;optimisation;pattern clustering","KNN;NMFAN;adaptive neighbors;adaptive process;benchmark data sets;clustering algorithms;convergence;data graph;data group;data local connectivity;data mining;data points;data similarity learning;graph regularized NMF;ideal neighbors assignment;input data similarity matrix;iterative updating algorithm;k-nearest neighbors;machine learning;nonnegative matrix factorization;optimal neighbors;optimization problem","","","","","","","","14-19 May 2017","","IEEE","IEEE Conferences"
"Scalable graph exploration and visualization: Sensemaking challenges and opportunities","R. Pienta; J. Abello; M. Kahng; D. H. Chau","Georgia Institute of Technology","2015 International Conference on Big Data and Smart Computing (BIGCOMP)","20150402","2015","","","271","278","Making sense of large graph datasets is a fundamental and challenging process that advances science, education and technology. We survey research on graph exploration and visualization approaches aimed at addressing this challenge. Different from existing surveys, our investigation highlights approaches that have strong potential in handling large graphs, algorithmically, visually, or interactively; we also explicitly connect relevant works from multiple research fields - data mining, machine learning, human-computer ineraction, information visualization, information retrieval, and recommender systems - to underline their parallel and complementary contributions to graph sensemaking. We ground our discussion in sensemaking research; we propose a new graph sensemaking hierarchy that categorizes tools and techniques based on how they operate on the graph data (e.g., local vs global). We summarize and compare their strengths and weaknesses, and highlight open challenges. We conclude with future research directions for graph sensemaking.","2375-933X;2375933X","Electronic:978-1-4799-7303-3; POD:978-1-4799-7304-0; USB:978-1-4799-7302-6","10.1109/35021BIGCOMP.2015.7072812","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7072812","","Algorithm design and analysis;Data mining;Data visualization;Machine learning algorithms;Pattern matching;Scalability;Visualization","data visualisation;graph theory;mathematics computing","data mining;graph datasets;graph handling;graph sensemaking hierarchy;human-computer ineraction;information retrieval;information visualization;machine learning;recommender systems;scalable graph exploration approach;scalable graph visualization approach","","6","","86","","","","9-11 Feb. 2015","","IEEE","IEEE Conferences"
"Discovering thematic structure in political datasets","M. T. Le; J. Sweeney; M. F. Lawlor; S. W. Zucker","Dept. of Computer Science, Yale University New Haven, CT 06520-8285, USA","2013 IEEE International Conference on Intelligence and Security Informatics","20130815","2013","","","163","165","Big data for security informatics requires an analysis of both actors and measurement instruments. By analogy with social and political network analyses, primary questions involve who is doing what and in concert with whom. We seek to examine these questions using cluster analysis, non-linear dimensionality reduction, and machine learning techniques. We apply them to available political science and international relations databases, as these are natural proxies for security informatics databases. In particular we develop an embedding/clustering algorithm that reveals those political themes driving UN voting patterns as well as IGO (Inter-Governmental Organization) memberships. Our algorithm could readily be applied to international conflict, urban crime, and military engagement databases.","","Electronic:978-1-4673-6213-9; POD:978-1-4673-6214-6","10.1109/ISI.2013.6578810","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578810","","Databases;Harmonic analysis;Irrigation","government data processing;learning (artificial intelligence);pattern clustering;politics;security of data","IGO memberships;UN voting patterns;big-data analysis;cluster analysis;embedding algorithm;intergovernmental organization memberships;international conflict database;international relations database;machine learning techniques;military engagement database;natural proxies;nonlinear dimensionality reduction;political network analysis;political science database;security informatics database;social network analysis;thematic structure discovery;urban crime database","","0","","11","","","","4-7 June 2013","","IEEE","IEEE Conferences"
"Table of contents","","","2014 International Conference on Information Technology Systems and Innovation (ICITSI)","20150226","2014","","","1","5","The following topics are dealt with: film recommendation systems; data clustering; cyber crime; event sentence extraction; machine learning; data mining; ontology; medical computing; multiagent system; ZigBee loop network; P2P computing; face-based gender classification; palm vein identification; e-learning; IT business development; e-government; online gaming; ERP; IT value analysis; wireless sensor network; online banking; accounting transaction; and health care.","","Electronic:978-1-4799-6527-4; POD:978-1-4799-6528-1","10.1109/ICITSI.2014.7048227","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7048227","","","DP management;artificial intelligence;biometrics (access control);business data processing;computer aided instruction;computer games;computer networks;entertainment;financial management;government data processing;health care;medical computing;multi-agent systems;pattern clustering;recommender systems;security of data;wireless sensor networks","ERP;IT business development;IT value analysis;P2P computing;ZigBee loop network;accounting transaction;cyber crime;data clustering;data mining;e-government;e-learning;event sentence extraction;face-based gender classification;film recommendation systems;health care;machine learning;medical computing;multiagent system;online banking;online gaming;ontology;palm vein identification;wireless sensor network","","0","","","","","","24-27 Nov. 2014","","IEEE","IEEE Conferences"
"Weather data analysis and sensor fault detection using an extended IoT framework with semantics, big data, and machine learning","A. C. Onal; O. Berat Sezer; M. Ozbayoglu; E. Dogdu","Department of Computer Engineering, TOBB University of Economics and Technology, Ankara, Turkey 06560","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","2037","2046","In recent years, big data and Internet of Things (IoT) implementations started getting more attention. Researchers focused on developing big data analytics solutions using machine learning models. Machine learning is a rising trend in this field due to its ability to extract hidden features and patterns even in highly complex datasets. In this study, we used our Big Data IoT Framework in a weather data analysis use case. We implemented weather clustering and sensor anomaly detection using a publicly available dataset. We provided the implementation details of each framework layer (acquisition, ETL, data processing, learning and decision) for this particular use case. Our chosen learning model within the library is Scikit-Learn based k-means clustering. The data analysis results indicate that it is possible to extract meaningful information from a relatively complex dataset using our framework.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258150","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258150","Internet of things;anomaly detection;big data analytics;clustering;fault detection;framework;machine learning;weather data analysis","Big Data;Data analysis;Feature extraction;Machine learning algorithms;Meteorology;Resource description framework;Semantics","Big Data;Internet of Things;data analysis;fault diagnosis;feature extraction;geophysics computing;learning (artificial intelligence);meteorology;pattern clustering;sensors","Big Data IoT Framework;Internet of Things implementation;Scikit-Learn;big data analytics solutions;data processing;extended IoT framework;framework layer;hidden feature extraction;highly complex datasets;information extraction;k-means clustering;machine learning models;publicly available dataset;relatively complex dataset;sensor anomaly detection;sensor fault detection;weather clustering;weather data analysis use case","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Mitigating the impact of faults in unreliable memories for error-resilient applications","S. Ganapathy; G. Karakonstantis; A. Teman; A. Burg","Telecommunications Circuits Laboratory, &#x00C9;cole Polytechnique F&#x00E9;d&#x00E9;rale de Lausanne, Switzerland","2015 52nd ACM/EDAC/IEEE Design Automation Conference (DAC)","20150727","2015","","","1","6","Inherently error-resilient applications in areas such as signal processing, machine learning and data analytics provide opportunities for relaxing reliability requirements, and thereby reducing the overhead incurred by conventional error correction schemes. In this paper, we exploit the tolerable imprecision of such applications by designing an energy-efficient fault-mitigation scheme for unreliable data memories to meet target yield. The proposed approach uses a bit-shuffling mechanism to isolate faults into bit locations with lower significance. This skews the bit-error distribution towards the low order bits, substantially limiting the output error magnitude. By controlling the granularity of the shuffling, the proposed technique enables trading-off quality for power, area, and timing overhead. Compared to error-correction codes, this can reduce the overhead by as much as 83% in read power, 77% in read access time, and 89% in area, when applied to various data mining applications in 28nm process technology.","0738-100X;0738100X","Electronic:978-1-4799-8052-9; POD:978-1-4799-8053-6","10.1145/2744769.2744871","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167286","Approximate Computing;Bit-shuffling;Error Correction;Error-resilient Applications;Priority-ECC;Significance-driven computing;Unreliable Memory","Arrays;Benchmark testing;Circuit faults;Delays;Error correction codes;Frequency modulation","fault tolerant computing;storage management","bit locations;bit-shuffling mechanism;data analytics;data memories;data mining applications;energy-efficient fault-mitigation scheme;error correction schemes;error-resilient applications;fault impact mitigation;machine learning;reliability requirements;signal processing","","7","","22","","","","8-12 June 2015","","IEEE","IEEE Conferences"
"Performance evaluation of distributed computing environments with Hadoop and Spark frameworks","V. Taran; O. Alienin; S. Stirenko; Y. Gordienko; A. Rojbi","National Technical University of Ukraine &#x201C;Igor Sikorsky, Kyiv Polytechnic Institute&#x201D;, Kyiv, Ukraine","2017 IEEE International Young Scientists Forum on Applied Physics and Engineering (YSF)","20171204","2017","","","80","83","Recently, due to rapid development of information and communication technologies, the data are created and consumed in the avalanche way. Distributed computing create preconditions for analyzing and processing such Big Data by distributing the computations among a number of compute nodes. In this work, performance of distributed computing environments on the basis of Hadoop and Spark frameworks is estimated for real and virtual versions of clusters. As a test task, we chose the classic use case of word counting in texts of various sizes. It was found that the running times grow very fast with the dataset size and faster than a power function even. As to the real and virtual versions of cluster implementations, this tendency is the similar for both Hadoop and Spark frameworks. Moreover, speedup values decrease significantly with the growth of dataset size, especially for virtual version of cluster configuration. The problem of growing data generated by IoT and multimodal (visual, sound, tactile, neuro and brain-computing, muscle and eye tracking, etc.) interaction channels is presented. In the context of this problem, the current observations as to the running times and speedup on Hadoop and Spark frameworks in real and virtual cluster configurations can be very useful for the proper scaling-up and efficient job management, especially for machine learning and Deep Learning applications, where Big Data are widely present.","","Electronic:978-1-5386-2994-9; POD:978-1-5386-2995-6","10.1109/YSF.2017.8126655","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8126655","Big Data;Hadoop;Spark;clusters;data image processing and recognition;distributed computing;information systems;machine learning;multimodal interactions;speedup","Big Data;Distributed databases;Servers;Sparks;Standards","Big Data;cloud computing;learning (artificial intelligence);parallel processing","Hadoop framework;IoT;Spark framework;compute nodes;dataset size;distributed computing environments;multimodal channel;performance evaluation;virtual cluster configurations","","","","","","","","17-20 Oct. 2017","","IEEE","IEEE Conferences"
"Unsupervised deep learning for subspace clustering","A. Sekmen; A. B. Koku; M. Parlaktuna; A. Abdul-Malek; N. Vanamala","Department of Computer Science, Tennessee State University","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","2089","2094","This paper presents a novel technique for the segmentation of data W = [w<sub>1</sub> · · · w<sub>n</sub>] ⊂ R<sup>D</sup> drawn from a union u = ∪<sup>M</sup><sub>i=1</sub> of subspaces {S<sub>i</sub>}<sup>M</sup><sub>i=1</sub>. First, an existing subspace segmentation algorithm is used to perform an initial data clustering {C<sub>i</sub>}<sup>M</sup><sub>i=1</sub>, where C<sub>i</sub> = {w<sub>i1</sub> · · ·w<sub>ik</sub>} ⊂ W is the set of data from the i<sup>th</sup> cluster. Then, a local subspace LS<sub>i</sub> is matched for each C<sub>i</sub> and the distance d<sub>ij</sub> between LS<sub>i</sub> and each point w<sub>ij</sub> ϵ C<sub>i</sub> is computed. A data-driven threshold η is computed and the data points (in C<sub>i</sub>) whose distances to LS<sub>i</sub> are larger than η are eliminated since they are considered as outliers or erroneously clustered data points in C<sub>i</sub>. The remaining data points C<sub>i</sub> ⊂ C<sub>i</sub> are considered to be coming from the same subspace with high confidence. Then, {C<sub>i</sub>}<sup>M</sup><sub>i=1</sub> are used in unsupervised way to train a convolution neural network to obtain a deep learning model, which is in turn used to re-cluster W. The system has been successfully implemented using the MNIST dataset and it improved the segmentation accuracy of a particular algorithm (EnSC-ORGEN) from 93.79% to 96.52%.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258156","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258156","Subspace segmentation;convolution neural networks;data clustering;deep learning;union of subspaces","Clustering algorithms;Clustering methods;Data models;Machine learning;Neural networks;Silicon;Training","image classification;image segmentation;learning (artificial intelligence);neural nets;pattern clustering;unsupervised learning","EnSC-ORGEN algorithm;MNIST dataset;data clustering;data points;data segmentation;data-driven threshold;deep learning model;erroneously clustered data points;local subspace;segmentation accuracy;subspace clustering;subspace segmentation algorithm;unsupervised deep learning","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"An improved recommendation model using linear regression and clustering for a private university in Thailand","K. Kongsakun","Department of Digital Media Design, Didyasarin International College, Hatyai University, Songkhla, 90110, Thailand","2013 International Conference on Machine Learning and Cybernetics","20140908","2013","04","","1625","1630","In order to enhance the number of completions, educational institutes establish and implement strategies to improve students' satisfaction and academic development. Technological supports are a strategy that many universities provide to service and assist staff and students. In this study, a prediction model called Electronic Grade (e-Grade) is used to model the likelihood of a student's Grade and achievement for particular subjects. This model aims to assist lecturers to supervise students, and to pay extra attention for students who are likely to get marginal results that could lead to withdrawal prior to completion of that subject. The e-Grade model comprises two sub-models that are Likelihood of Grade Before midterm examination, and Likelihood of Grade After midterm examination. In the experiment, two datasets are used. The results will provide supports to counsel the students on their possible performance and classroom achievement. The usefulness of the proposed e-Grade model for the monitoring of students' progress is verified against benchmark data. The results are interpretation of the performance of the new model of research based on linear regression and clustering techniques. The experiment results found that the proposed model enhances the accuracy of linear regression techniques in comparison to the benchmark model.","2160-133X;2160133X","Electronic:978-1-4799-0260-6; POD:978-1-4799-0259-0","10.1109/ICMLC.2013.6890859","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6890859","Clustering;Data Mining;Likelihood of Grade;Linear Regression;Student Performance","Abstracts;Benchmark testing;Computational modeling;Educational institutions;Medical services;Predictive models","educational administrative data processing;educational institutions;pattern clustering;regression analysis","Thailand;academic development;clustering techniques;e-grade model;educational institutes;electronic grade;likelihood of grade after midterm examination;likelihood of grade before midterm examination;linear regression techniques;prediction model;private university;recommendation model;student classroom achievement;student counselling;student grade;student performance;student progress monitoring;student satisfaction;student supervision;technological supports","","1","","35","","","","14-17 July 2013","","IEEE","IEEE Conferences"
"Learning-based data analytics: Moving towards transparent power grids","K. Chen; Z. He; S. X. Wang; J. Hu; L. Li; J. He","State Key Lab of Power Systems, Department of Electrical Engineering, Tsinghua University, Beijing 100084, China","CSEE Journal of Power and Energy Systems","20180315","2018","4","1","67","82","In this paper, we present the learning-based data analytics moving towards transparent power grids and provide some possible extensions including machine learning, big data analytics, and knowledge transferring. The closed loops of data and knowledge are illustrated and the challenges for establishing the closed loops are discussed. General ideas and recent developments in supervised learning, unsupervised learning, and reinforcement learning are presented together with extensions for power system applications. Furthermore, much emphasis is placed on privacy-preserving data analysis, transfer of knowledge, machine learning for causal inference, scalability and flexibility of data analytics, and efficiency and reliability of computation. Existing integrated solutions in the industry featuring the Industrial Internet and the digital grid are also introduced.","","","10.17775/CSEEJPES.2017.01070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8315224","Data analytics;machine learning;smart grid;transparent power grid","Business;Data analysis;Industries;Monitoring;Smart grids","","","","","","","","","","March 2018","","CSEE","CSEE Journals & Magazines"
"Improving Accuracy for Classifying Selected Medical Datasets with Weighted Nearest Neighbors and Fuzzy Nearest Neighbors Algorithms","M. Qasem; M. Nour","Electron. Res. Inst., Cairo, Egypt","2015 International Conference on Cloud Computing (ICCC)","20150709","2015","","","1","9","Classification algorithms are very important for several fields such as data mining, machine learning, pattern recognition, and other data analysis applications. This work presents the weighted nearest neighbors and fuzzy k-nearest neighbors algorithms to classify chosen medical datasets. This involves several distance functions to calculate the difference between any two instances. Classification approaches based on K-nearest neighbors (KNN), weighted-KNN, frequency, class probability, and fuzzy K-nearest neighbors (fuzzy-KNN) are analyzed and discussed. Some measurable criteria are adopted to evaluate the performance of such algorithms. This includes classification accuracy, time, and confidence values. The algorithms will be tested using four different medical datasets. From the results, the fuzzy-KNN achieved the best accuracy compared to the other adopted algorithms. Following that are the weighted-KNN then the KNN. The longest classification time was for the fuzzy-KNN while the smallest time was for the KNN. The class confidence values of the fuzzy approach were promising. The fuzzy-KNN was also modified using fuzzy entropy. For the chosen datasets and w.r.t. KNN, the modified algorithms improved the classification accuracy. The improvements were up to 25%, 33%, and 38% for the weighted-KNN, fuzzy-KNN, and fuzzy Entropy respectively.","","Electronic:978-1-4673-6618-2; POD:978-1-4673-6619-9","10.1109/CLOUDCOMP.2015.7149644","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7149644","","Accuracy;Algorithm design and analysis;Classification algorithms;Machine learning algorithms;Prediction algorithms;Training;Tumors","entropy;fuzzy set theory;medical administrative data processing;pattern classification;pattern clustering","KNN;fuzzy entropy;fuzzy nearest neighbor algorithm;k-nearest neighbor;medical dataset classification;weighted nearest neighbor algorithm","","0","","25","","","","26-29 April 2015","","IEEE","IEEE Conferences"
"A Multi-dimensional Comparison of Toolkits for Machine Learning with Big Data","A. N. Richter; T. M. Khoshgoftaar; S. Landset; T. Hasanin","Florida Atlantic Univ., Boca Raton, FL, USA","2015 IEEE International Conference on Information Reuse and Integration","20151026","2015","","","1","8","Big data is a big business, and effective modeling of this data is key. This paper provides a comprehensive multidimensional analysis of various open source tools for machine learning with big data. An evaluation standard is proposed along with detailed comparisons of the frameworks discussed, with regard to algorithm availability, scalability, speed, and more. The major tools profiled are Mahout, MLlib, H2O, and SAMOA, along with the big data processing engines they utilize, including Hadoop MapReduce, Apache Spark, and Apache Storm. There is not yet one framework that ""does it all"", but this paper provides insight into each tool's strengths and weaknesses along with guidance on tool choice for specific needs.","","Electronic:978-1-4673-6656-4; POD:978-1-4673-6657-1","10.1109/IRI.2015.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300948","Hadoop;Mahout;Spark;big data;data mining;machine learning","Big data;Clustering algorithms;Data models;Engines;Machine learning algorithms;Sparks;Water","Big Data;data analysis;learning (artificial intelligence)","Apache Spark;Apache Storm;Big Data;H2O tool;Hadoop MapReduce;MLlib tool;Mahout tool;SAMOA tool;evaluation standard;machine learning;multidimensional analysis;toolkit multidimensional comparison","","4","","36","","","","13-15 Aug. 2015","","IEEE","IEEE Conferences"
"High Performance Machine Learning (HPML) Framework to Support DDDAS Decision Support Systems: Design Overview","G. Ditzler; S. Hariri; A. Akoglu","NSF Center for Cloud & Autonomic Comput., Univ. of Arizona, Tucson, AZ, USA","2017 IEEE 2nd International Workshops on Foundations and Applications of Self* Systems (FAS*W)","20171012","2017","","","360","362","This paper presents a design for a High Performance Machine Learning (HPML) framework to support DDDAS decision processes. The HPML framework can provide a high performance computing environment to implement large scale machine learning algorithms that leverages Big Data tools (e.g., SPARK, Hadoop), parallel algorithms, and MapReduce programming paradigm. The framework provides the following capabilities: · High Performance Parallel Algorithms: For a suite of important ML, we will develop three parallel implementations of each algorithm that are based on Message Passing Interface (MPI), Shared Memory (SM) and MapReduce programming model. · High Performance and Scalable Platforms: This will enable us to identify the best high performance platform that maximizes performance and scalability of the parallel ML methods. We will experiment with and evaluate the performance and scalability of different parallel architectures (shared memory and message passing), Clusters of GPUs, and cloud computing systems. By leveraging the emerging Big Data tools and high performance computing algorithms (traditional and emerging paradigm such as MapReduce), we will be able to achieve the following: 1) reduce significantly the ML processing time, 2) enable StreamlinedML users to leverage Big Data tools to perform large scale ML tasks over structured and non-structured data sets; and 3) enable users to identify the best parallel platform and storage allocation and distribution that maximize performance and scalability of the selected ML algorithms.""","","Electronic:978-1-5090-6558-5; POD:978-1-5090-6559-2","10.1109/FAS-W.2017.174","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8064150","DDDAS;Data Analytics;High Performance ML;Machine Learning","Classification algorithms;Computational modeling;Data models;Machine learning algorithms;Programming;Scalability","Big Data;application program interfaces;cloud computing;data analysis;decision support systems;learning (artificial intelligence);message passing;parallel algorithms;parallel architectures;parallel programming","Big Data tools;DDDAS decision processes;DDDAS decision support systems;HPML framework;High Performance Machine Learning framework;High Performance Parallel Algorithms;ML algorithms;ML processing time;MapReduce programming paradigm;Message Passing Interface;Scalable Platforms;cloud computing systems;design overview;high performance computing algorithms;high performance computing environment;leverage Big Data tools;parallel ML methods;parallel architectures;parallel platform;scalability;scale ML tasks;scale machine;shared memory;storage allocation","","","","","","","","18-22 Sept. 2017","","IEEE","IEEE Conferences"
"The interdisciplinary research of big data and wireless channel: A cluster-nuclei based channel model","J. Zhang","Beijing Haidian District, Xitucheng Road No. 10, Mailbox No. 92, 100876, China","China Communications","20160212","2016","13","2","14","26","Recently, internet stimulates the explosive progress of knowledge discovery in big volume data resource, to dig the valuable and hidden rules by computing. Simultaneously, the wireless channel measurement data reveals big volume feature, considering the massive antennas, huge bandwidth and versatile application scenarios. This article firstly presents a comprehensive survey of channel measurement and modeling research for mobile communication, especially for 5th Generation (5G) and beyond. Considering the big data research progress, then a cluster-nuclei based model is proposed, which takes advantages of both the stochastical model and deterministic model. The novel model has low complexity with the limited number of cluster-nuclei while the cluster-nuclei has the physical mapping to real propagation objects. Combining the channel properties variation principles with antenna size, frequency, mobility and scenario dug from the channel data, the proposed model can be expanded in versatile application to support future mobile research.","1673-5447;16735447","","10.1109/CC.2016.7405719","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7405719","5G;big data;channel model;cluster;machine learning;massive MIMO","5G mobile communication;Special issues and sections","","","","","","","","","","N/A 2016","","IEEE","IEEE Journals & Magazines"
"Agile big data analytics: AnalyticsOps for data science","N. W. Grady; J. A. Payne; H. Parker","Advanced Analytics, SAIC, McLean, VA, USA","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","2331","2339","Big data analytic (BDA) systems leverage data distribution and parallel processing across a cluster of resources. This introduces a number of new challenges specifically for analytics. The analytics portion of the complete lifecycle has typically followed a waterfall process - completing one step before beginning the next. While efforts have been made to map different types of analytics to an agile methodology, the steps are often described as breaking activities into smaller tasks while the overall process is still consistent with step-by-step waterfall. BDA changes a number of the activities in the analytics lifecycle, as well as their ordering. The goal of agile analytics - to reach a point of optimality between generating value from data and the time spent getting there. This paper discusses the implications of an agile process for BDA in cleansing, transformation, and analytics.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258187","AnalyticsOps;Deep Learning;DevOps;Knowledge Discovery in Data Science;advanced analytics;agile development;analytics lifecycle;big data analytics;data science;data science process models;machine learning","Analytical models;Big Data;Computational modeling;Data models;Data science;Software;Testing","Big Data;data analysis;parallel processing;software prototyping","AnalyticsOps;BDA;agile big data analytics;agile methodology;agile process;analytics lifecycle;big data analytic systems;data distribution;data science;parallel processing;step-by-step waterfall;waterfall process","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"An efficient parallel topic-sensitive expert finding algorithm using spark","Y. M. Yang; C. D. Wang; J. H. Lai","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, P. R. China","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3556","3562","Expert finding is an important technique to obtain the user authority ranking in community question answering (CQA) websites. ZhihuRank is a topic-sensitive expert finding algorithm, which is based on both LDA and PageRank. Currently, with the amount of participants and documents increasing rapidly in CQA websites, how to parallel expert finding algorithms for big data analysis has received significant attention. In this paper, we find that the Spark framework is more suitable for paralleling expert finding algorithms than the MapReduce framework, which is a memory-based parallel computing model to support complicated iterative algorithms. As an example, we parallel ZhihuRank using MLlib's LDA and GraphX's PageRank in Spark. Experiments have been conducted on large-scale real data from Zhihu<sup>1</sup> (the most popular CQA website in China). And the experimental results confirmed the effectiveness and scalability of our proposed approach.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7841019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841019","Community Question Answering;Distributed Computing;Expert Finding;LDA;PageRank","Algorithm design and analysis;Computational modeling;Heuristic algorithms;Libraries;Machine learning algorithms;Programming;Sparks","Big Data;Web sites;data analysis;iterative methods;parallel algorithms;question answering (information retrieval);search engines","CQA Web sites;LDA;MapReduce framework;PageRank;Spark framework;ZhihuRank;big data analysis;community question answering Web sites;iterative algorithms;memory-based parallel computing model;parallel topic-sensitive expert finding algorithm;user authority ranking","","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conferences"
"Big data and clustering algorithms","V. W. Ajin; L. D. Kumar","Department of Computer Science, SCT College of Engineering, Trivandrum, India","2016 International Conference on Research Advances in Integrated Navigation Systems (RAINS)","20161205","2016","","","1","5","Data mining is the method which is useful for extracting useful information and data is extorted, but the classical data mining approaches cannot be directly used for big data due to their absolute complexity. The data that is been formed by numerous scientific applications and incorporated environment has grown rapidly not only in size but also in variety in recent era. The data collected is of very large amount and there is difficulty in collecting and assessing big data. Clustering algorithms have developed as a powerful meta learning tool which can precisely analyze the volume of data produced by modern applications. The main goal of clustering is to categorize data into clusters such that objects are grouped in the same cluster when they are “similar” according to similarities, traits and behavior. The most commonly used algorithm in clustering are partitioning, hierarchical, grid based, density based, and model based algorithms. A review of clustering and its different techniques in data mining is done considering the criteria's for big data. Where most commonly used and effective algorithms like K-Means, FCM, BIRCH, CLIQUE algorithms are studied and compared on big data perspective.","","Electronic:978-1-5090-1111-7; POD:978-1-5090-1112-4","10.1109/RAINS.2016.7764405","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7764405","BIRCH;BigData;CLIQUE;Clustering;FCM;K-Means","Algorithm design and analysis;Approximation algorithms;Big data;Clustering algorithms;Data mining;Machine learning algorithms;Partitioning algorithms","Big Data;data analysis;data mining;learning (artificial intelligence);natural sciences computing;pattern clustering","Big Data;FCM;birch algorithm;clique algorithms;clustering algorithms;data analysis;data mining;density based algorithm;grid based algorithm;hierarchical algorithm;information extraction;k-means algorithm;meta learning tool;model based algorithms;partitioning algorithm;scientific applications","","2","","","","","","6-7 May 2016","","IEEE","IEEE Conferences"
"A hybrid semi-supervised approach for financial fraud detection","J. M. Liu; J. Tian; Z. X. Cai; Y. Zhou; R. H. Luo; R. R. Wang","Department of Information Technology, China Everbright Bank, Beijing, China","2017 International Conference on Machine Learning and Cybernetics (ICMLC)","20171116","2017","1","","217","222","In this paper, we create a semi-supervised methodology for financial fraud detection in bank wire transactions based on a clustering-based-isolation-forest (CBiForest) algorithm. To test this hybrid model, we experiment on wire transaction data of twelve months from China Everbright Bank. The result of abnormal users is proved to be reliable and outperforms other clustering algorithms. Furthermore, our model can be regarded as a huge improvement for traditional expert system in bank.","","Electronic:978-1-5386-0408-3; POD:978-1-5386-0409-0; Paper:978-1-5386-0406-9; USB:978-1-5386-0407-6","10.1109/ICMLC.2017.8107767","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8107767","Fraud Detection;Isolation Forest;K-means Clustering;Semi-supervised Learning","Algorithm design and analysis;Anomaly detection;Clustering algorithms;Feature extraction;IP networks;Mobile communication;Wires","","","","","","","","","","9-12 July 2017","","IEEE","IEEE Conferences"
"GraphSteal: Dynamic Re-Partitioning for Efficient Graph Processing in Heterogeneous Clusters","D. Kumar; A. Raj; J. Dharanipragada","PayPal, San Jose, CA, USA","2017 IEEE 10th International Conference on Cloud Computing (CLOUD)","20170911","2017","","","439","446","With continuously growing data, clusters also need to grow periodically to accommodate the increased demand of data processing. This is usually done by addition of newer hardware, whose configuration might differ from the existing nodes. As a result, clusters are becoming heterogeneous in nature. For many real world machine learning and data mining applications, data is represented in the form of graphs. Most of the existing distributed graph processing frameworks such as Pregel and Graphlab assume that the computational nodes are homogeneous. These frameworks split the graph into approximately equal subgraphs, which is appropriate for homogeneous clusters. In heterogeneous clusters, these frameworks perform poorly in most of the scenarios. To the best of our knowledge, GraphIVE is the only heterogeneity-aware graph processing framework. It learns the relative capabilities of the nodes based on runtime metrics of previous jobs and partitions the graph proportionally. However, it may not perform well if a new job differs drastically in terms of resource requirements when compared to previous jobs executed on the cluster. To overcome this limitation, we propose GraphSteal, a dynamic graph re-partitioning policy for vertex-cut based graph processing frameworks on heterogeneous clusters. GraphSteal dynamically re-partitions the graph based on the runtime characteristics of the job. To avoid computational skew in the cluster, it migrates edges from slow nodes to fast nodes. To demonstrate our approach, we modify the source code of Graphlab to incorporate dynamic graph re-partitioning strategy. Experimental results show that GraphSteal significantly improves the performance over Graphlab.","","Electronic:978-1-5386-1993-3; POD:978-1-5386-1994-0; USB:978-1-5386-1992-6","10.1109/CLOUD.2017.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030619","Adaptive scheduling;Distributed Computing;Heterogeneity;Partitioning algorithms;Vertex-cut","Clustering algorithms;Computer science;Measurement;Memory management;Mirrors;Runtime;Twitter","data mining;distributed processing;graph theory;learning (artificial intelligence);pattern clustering","GraphIVE;GraphSteal;Graphlab;Pregel;computational skew;data mining;data processing;distributed graph processing frameworks;dynamic graph re-partitioning policy;heterogeneity-aware graph processing framework;heterogeneous clusters;homogeneous clusters;machine learning;runtime job characteristics;source code;subgraphs;vertex-cut based graph processing framework","","","","","","","","25-30 June 2017","","IEEE","IEEE Conferences"
"Java2SDG: Stateful big data processing for the masses","R. C. Fernandez; P. Garefalakis; P. Pietzuch","Imperial College London, United Kingdom","2016 IEEE 32nd International Conference on Data Engineering (ICDE)","20160623","2016","","","1390","1393","Big data processing is no longer restricted to specially-trained engineers. Instead, domain experts, data scientists and data users all want to benefit from applying data mining and machine learning algorithms at scale. A considerable obstacle towards this “democratisation of big data” are programming models: current scalable big data processing platforms such as Spark, Naiad and Flink require users to learn custom functional or declarative programming models, which differ fundamentally from popular languages such as Java, Matlab, Python or C++. An open challenge is how to provide a big data programming model for users that are not familiar with functional programming, while maintaining performance, scalability and fault tolerance. We describe JAVA2SDG, a compiler that translates annotated Java programs to stateful dataflow graphs (SDGs) that can execute on a compute cluster in a data-parallel and fault-tolerant fashion. Compared to existing distributed dataflow models, a distinguishing feature of SDGs is that their computational tasks can access distributed mutable state, thus allowing SDGs to capture the semantics of stateful Java programs. As part of the demonstration, we provide examples of machine learning programs in Java, including collaborative filtering and logistic regression, and we explain how they are translated to SDGs and executed on a large set of machines.","","Electronic:978-1-5090-2020-1","10.1109/ICDE.2016.7498352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7498352","","Big data;Collaboration;Computational modeling;Data models;Java;Mathematical model;Programming","Big Data;Java;collaborative filtering;data mining;learning (artificial intelligence);regression analysis","Java programs;Java2SDG;collaborative filtering;data mining;data-parallel graph;dataflow graphs;logistic regression;machine learning algorithms;stateful Big Data processing","","","","16","","","","16-20 May 2016","","IEEE","IEEE Conferences"
"Using a Rich Context Model for Real-Time Big Data Analytics in Twitter","A. Sotsenko; M. Jansen; M. Milrad; J. Rana","Dept. of Media Technol., Linnaeus Univ., Vaxjo, Sweden","2016 IEEE 4th International Conference on Future Internet of Things and Cloud Workshops (FiCloudW)","20161018","2016","","","228","233","In this paper we present an approach for contextual big data analytics in social networks, particularly in Twitter. The combination of a Rich Context Model (RCM) with machine learning is used in order to improve the quality of the data mining techniques. We propose the algorithm and architecture of our approach for real-time contextual analysis of tweets. The proposed approach can be used to enrich and empower the predictive analytics or to provide relevant context-aware recommendations.","","","10.1109/W-FiCloud.2016.55","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7592729","big data;context analytics;k-means clustering;rich context model;twitter","Big data;Context;Context modeling;Measurement;Real-time systems;Sparks;Twitter","Big Data;data mining;learning (artificial intelligence);social networking (online)","Twitter;contextual big data analytics;data mining;machine learning;real-time big data analytics;rich context model;social network","","","","","","","","22-24 Aug. 2016","","IEEE","IEEE Conferences"
"Temporal association rules for electrical activity detection in residential homes","H. Â. Cao; T. K. Wijaya; K. Aberer; N. Nunes","Department of Computer Science, ETH Zurich, Switzerland","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3097","3106","Attaining energy efficiency requires understanding human behaviors triggering energy consumption within households. In conjunction to providing appliance-level feedback, targeting human activities that involve the usage of electrical appliances can provide a higher abstraction level to bring awareness to the electricity wastage. In this paper, we make use of a large dataset with appliance- and circuit-level power data and provide a framework for determining temporal sequential association rules. Sequences of time intervals where the appliances are in usage can vary in their order, duration and the time elapsed between these events. Our contribution consists in providing a full pipeline for mining frequent sequential itemsets and a novel way to discover the time windows during which these sequences of events occur and to capture their variance in terms of duration and order. Our method is data-driven and relies on the data's statistical properties and allows us to avoid an exhaustive search for the time windows' sizes, by relying instead on machine learning techniques to identify and predict those time windows.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840964","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840964","Activity inference;Algorithms;Appliances states;Clustering;Data mining;Datasets;Energy data analytics;Information search and retrieval;Smart energy;Smartmeters;Time series analysis","Data mining;Electronic mail;Energy consumption;Histograms;Home appliances;Itemsets;Pipelines","data mining;domestic appliances;energy consumption;learning (artificial intelligence);power engineering computing","circuit-level power data;electrical activity detection;frequent sequential itemsets mining;machine learning techniques;residential homes;temporal association rules;temporal sequential association rules","","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conferences"
"Parallelized FPA-SVM: Parallelized parameter selection and classification using Flower Pollination Algorithm and Support Vector Machine","J. C. Coetsier; R. Jiamthapthaksin","Computer Science Department, Assumption University, Bangkok, Thailand","2017 14th International Joint Conference on Computer Science and Software Engineering (JCSSE)","20170907","2017","","","1","6","Support Vector Machine (SVM) is one of the most popular machine learning algorithm to perform classification tasks and help organizations in different ways to improve their efficiency. A lot of studies have been made to improve SVM including speed, accuracy, and/or scalability. The algorithm possesses parameters that need precision tuning to perform well. This work proposes a novel parallelized parameter selection using Flower Pollination Algorithm (FPA) to quickly find the optimal parameters of SVM. In particular, MapReduce algorithm introduced in big data framework is applied to both FPA and SVM, which forms a fully distributed algorithm to support a large dataset. The experimental results of Parallelized FPA-SVM on real datasets show its outstanding speed in generating optimal models while maintaining high accuracy.","","Electronic:978-1-5090-4834-2; POD:978-1-5090-4835-9","10.1109/JCSSE.2017.8025899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8025899","flower pollination algorithm;machine learning;map reduce;parallel algorithms;parameter selections;support vector machine","Algorithm design and analysis;Classification algorithms;Kernel;Machine learning algorithms;Sociology;Statistics;Support vector machines","Big Data;learning (artificial intelligence);parallel algorithms;pattern classification;support vector machines","big data framework;classification task;flower pollination algorithm;fully distributed algorithm;machine learning algorithm;parallelized FPA-SVM;parallelized parameter selection;precision tuning;scalability;support vector machine","","","","","","","","12-14 July 2017","","IEEE","IEEE Conferences"
"SeLINA: A Self-Learning Insightful Network Analyzer","D. Apiletti; E. Baralis; T. Cerquitelli; P. Garza; D. Giordano; M. Mellia; L. Venturini","Dipartimento di Automatica e Informatica, Politecnico di Torino, Turin, Italy","IEEE Transactions on Network and Service Management","20170519","2016","13","3","696","710","Understanding the behavior of a network from a large scale traffic dataset is a challenging problem. Big data frameworks offer scalable algorithms to extract information from raw data, but often require a sophisticated fine-tuning and a detailed knowledge of machine learning algorithms. To streamline this process, we propose self-learning insightful network analyzer (SeLINA), a generic, self-tuning, simple tool to extract knowledge from network traffic measurements. SeLINA includes different data analytics techniques providing self-learning capabilities to state-of-the-art scalable approaches, jointly with parameter auto-selection to off-load the network expert from parameter tuning. We combine both unsupervised and supervised approaches to mine data with a scalable approach. SeLINA embeds mechanisms to check if the new data fits the model, to detect possible changes in the traffic, and to, possibly automatically, trigger model rebuilding. The result is a system that offers human-readable models of the data with minimal user intervention, supporting domain experts in extracting actionable knowledge and highlighting possibly meaningful interpretations. SeLINA's current implementation runs on Apache Spark. We tested it on large collections of real-world passive network measurements from a nationwide ISP, investigating YouTube, and P2P traffic. The experimental results confirmed the ability of SeLINA to provide insights and detect changes in the data that suggest further analyses.","1932-4537;19324537","","10.1109/TNSM.2016.2597443","European Union; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7529210","Mining and statistical methods;machine learning;network data analysis","Algorithm design and analysis;Analytical models;Buildings;Clustering algorithms;Computational modeling;Data mining;Data models","Big Data;Internet;data analysis;data mining;telecommunication traffic;unsupervised learning","Apache Spark;Big data frameworks;ISP;P2P traffic;SeLINA;YouTube;data analytics;data mining;human-readable models;knowledge extraction;large scale traffic dataset;machine learning;network traffic measurements;parameter tuning;self-learning insightful network analyzer","","2","","","","","20160802","Sept. 2016","","IEEE","IEEE Journals & Magazines"
"Inferring Application Type Information from Tor Encrypted Traffic","G. He; M. Yang; J. Luo; X. Gu","China Electr. Power Res. Inst., Nanjing, China","2014 Second International Conference on Advanced Cloud and Big Data","20150813","2014","","","220","227","Tor is a famous anonymity communication system for preserving users' online privacy. It supports TCP applications and packs application data into encrypted equal-sized cells to hide some private information of users, such as the running application type (Web, P2P, FTP, Others). The known of application types is harmful because they can be used to reduce the anonymity set and facilitate other attacks. However, unfortunately, the current Tor design cannot conceal certain application behaviors. For example, P2P applications usually upload and download files simultaneously and this behavioral feature is also kept in Tor traffic. Motivated by this observation, we investigate a new attack against Tor, traffic classification attack, which can recognize application types from Tor traffic. An attacker first carefully selects some flow features, e.g., burst volumes and directions to represent the application behaviors and takes advantage of some efficient machine learning algorithm to model different types of applications. Then these established models can be used to classify target's Tor traffic and infer its application type. We have implemented the traffic classification attack on Tor and our experiments validate the feasibility and effectiveness of the attack.","","Electronic:978-1-4799-8085-7; POD:978-1-4799-8067-3","10.1109/CBD.2014.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7176097","Tor;anonymous communication;privacy;profile HMM;traffic classification","Clustering algorithms;Computational modeling;Feature extraction;Hidden Markov models;Probability;Servers;Training","computer network security;cryptography;peer-to-peer computing","P2P applications;Tor design;Tor encrypted traffic;anonymity communication system;application type information;peer-to-peer applications;traffic classification attack;user online privacy preservation","","0","","28","","","","20-22 Nov. 2014","","IEEE","IEEE Conferences"
"Design and implementation of recommendation system of micro video's topic","D. Jiang; W. Shang","School of Computer Science, Communication University of China, Beijing, China","2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS)","20170629","2017","","","483","485","With the development of Internet technology and the arrival of the era of big data, it is necessary to analyze and excavate the micro video data. It can help micro video creators to create better to analysis micro video data. This paper mainly introduces the structure design, key technical points and specific implementation steps of the micro video topic recommendation system.","","Electronic:978-1-5090-5507-4; POD:978-1-5090-5508-1","10.1109/ICIS.2017.7960040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960040","micro video;recommendation","Algorithm design and analysis;Clustering algorithms;Data mining;Internet;Machine learning algorithms;Partitioning algorithms;Regression analysis","Big Data;Internet;data analysis;recommender systems;video signal processing","Big Data;Internet technology;key technical points;micro video data analysis;micro video topic recommendation system;specific implementation steps;structure design","","","","","","","","24-26 May 2017","","IEEE","IEEE Conferences"
"Determining feature extractors for unsupervised learning on satellite images","B. Hedayatnia; M. Yazdani; M. Nguyen; J. Block; I. Altintas","Department of Electrical and Computer Engineering, UC San Diego, California, USA","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","2655","2663","Advances in satellite imagery presents unprecedented opportunities for understanding natural and social phenomena at global and regional scales. Although the field of satellite remote sensing has evaluated imperative questions to human and environmental sustainability, scaling those techniques to very high spatial resolutions at regional scales remains a challenge. Satellite imagery is now more accessible with greater spatial, spectral and temporal resolution creating a data bottleneck in identifying the content of images. Because satellite images are unlabeled, unsupervised methods allow us to organize images into coherent groups or clusters. However, the performance of unsupervised methods, like all other machine learning methods, depends on features. Recent studies using features from pre-trained networks have shown promise for learning in new datasets. This suggests that features from pre-trained networks can be used for learning in temporally and spatially dynamic data sources such as satellite imagery. It is not clear, however, which features from which layer and network architecture should be used for learning new tasks. In this paper, we present an approach to evaluate the transferability of features from pre-trained Deep Convolutional Neural Networks for satellite imagery. We explore and evaluate different features and feature combinations extracted from various deep network architectures, and systematically evaluate over 2,000 network-layer combinations. In addition, we test the transferability of our engineered features and learned features from an unlabeled dataset to a different labeled dataset. Our feature engineering and learning are done on the unlabeled Draper Satellite Chronology dataset, and we test on the labeled UC Merced Land dataset to achieve near state-of-the-art classification results. These results suggest that even without any or minimal training, these networks can generalize well to other datasets. This method could be useful in the ta- k of clustering unlabeled images and other unsupervised machine learning tasks.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840908","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840908","convolutional neural networks;deep learning;remote sensing;satellite imagery;transfer learning","Feature extraction;Network architecture;Neural networks;Remote sensing;Satellites;Spatial databases;Training","feature extraction;feedforward neural nets;geophysical image processing;pattern clustering;remote sensing;unsupervised learning","feature combination extraction;feature extractors;labeled UC Merced Land dataset;natural phenomena;pretrained deep convolutional neural networks;satellite imagery;satellite images;satellite remote sensing;social phenomena;spatial resolution;spectral resolution;temporal resolution;unlabeled Draper Satellite Chronology dataset;unlabeled image clustering;unsupervised machine learning tasks","","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conferences"
"Software-Based Real-Time Acquisition and Processing of PET Detector Raw Data","B. Goldschmidt; D. Schug; C. W. Lerche; A. Salomon; P. Gebhardt; B. Weissler; J. Wehner; P. M. Dueppenbecker; F. Kiessling; V. Schulz","Department of Physics of Molecular Imaging Systems, RWTH Aachen University, Aachen, Germany","IEEE Transactions on Biomedical Engineering","20160118","2016","63","2","316","327","In modern positron emission tomography (PET) readout architectures, the position and energy estimation of scintillation events (singles) and the detection of coincident events (coincidences) are typically carried out on highly integrated, programmable printed circuit boards. The implementation of advanced singles and coincidence processing (SCP) algorithms for these architectures is often limited by the strict constraints of hardware-based data processing. In this paper, we present a software-based data acquisition and processing architecture (DAPA) that offers a high degree of flexibility for advanced SCP algorithms through relaxed real-time constraints and an easily extendible data processing framework. The DAPA is designed to acquire detector raw data from independent (but synchronized) detector modules and process the data for singles and coincidences in real-time using a center-of-gravity (COG)-based, a leastsquares (LS)-based, or a maximum-likelihood (ML)-based crystal position and energy estimation approach (CPEEA). To test the DAPA, we adapted it to a preclinical PET detector that outputs detector raw data from 60 independent digital silicon photomultiplier (dSiPM)-based detector stacks and evaluated it with a [18F]fluorodeoxyglucose-filled hot-rod phantom. The DAPA is highly reliable with less than 0.1% of all detector raw data lost or corrupted. For high validation thresholds (37.1 ± 12.8 photons per pixel) of the dSiPM detector tiles, the DAPA is real time capable up to 55 MBq for the COG-based CPEEA, up to 31 MBq for the LS-based CPEEA, and up to 28 MBq for the ML-based CPEEA. Compared to the COG-based CPEEA, the rods in the image reconstruction of the hot-rod phantom are only slightly better separable and less blurred for the LSand ML-based CPEEA. While the coincidence time resolution (~550 ps) and energy resolution (~12.3%) are comparable for all three CPEEA, the system sensitivity is up to 2.5× higher for the LS-- and ML-based CPEEA.","0018-9294;00189294","","10.1109/TBME.2015.2456640","Centre of Excellence in Medical Engineering; European Union (European Regional Development Fund&#8212;Investing in your future) and the German federal state North Rhine-Westphalia (NRW); SUBLIMA; Wellcome Trust and EPSRC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7156103","Data acquisition;Parallel Processing;Positron Emission Tomography (PET);Singles & Coincidence Processing;parallel processing;positron emission tomography (PET);real time;singles and coincidence processing","Crystals;Data acquisition;Data processing;Detectors;Estimation;Positron emission tomography;Real-time systems","biomedical equipment;data acquisition;elemental semiconductors;image reconstruction;maximum likelihood estimation;medical image processing;phantoms;photomultipliers;positron emission tomography;printed circuits;semiconductor counters;silicon","COG-based CPEEA;LS-based CPEEA;ML-based CPEEA;PET detector raw data processing;PET energy estimation;Si;[<sup>18</sup>F]-fluorodeoxyglucose-filled hot-rod phantom;center-of-gravity;coincidence processing algorithms;coincident event detection;dSiPM detector tiles;digital silicon photomultiplier-based detector stacks;hardware-based data processing;hot-rod phantom;image reconstruction;maximum-likelihood-based crystal position and energy estimation approach;positron emission tomography readout architectures;preclinical PET detector;programmable printed circuit boards;scintillation events;software-based data acquisition-processing architecture;software-based real-time acquisition","Algorithms;Cluster Analysis;Humans;Image Processing, Computer-Assisted;Models, Theoretical;Phantoms, Imaging;Positron-Emission Tomography;Software","5","","47","","","20150714","Feb. 2016","","IEEE","IEEE Journals & Magazines"
"Design of Palatable Credit Scorecards as a Highly Automated Analytic Service by Combining Machine Learning with Domain Expertise","G. Fahner","Anal. Sci., Fair Isaac Corp., San Jose, CA, USA","2014 IEEE International Conference on Services Computing","20141020","2014","","","850","851","Lenders require accurate and interpretable credit scoring models palatable to regulators, financial services staff and consumers. Expert-designed segmented scorecards fill this need. Building such models is a laborious data-guided task for experienced modelers. It can take weeks to hone a model for deployment. Lenders would like to design, update and test models, predictors and segmentation schemes more frequently, objectively and cost-effectively, as environments change fast and as new data emerge. We propose scorecard design as an automated analytic computing service used by domain experts, comprising data-driven machine learning with expert-imposed palatability restrictions and model visualization, in two stages: Stage I fits a tree ensemble model to render a best-fit score and a list of segmentation candidates. Stage II uses this information to generate optimal palatable segmented scorecards subject to restrictions provided by the experts. When implemented on a computer cluster, our process yields close to deployment-ready scorecards within minutes to hours, which can be rapidly honed and upon approval deployed into a separate scoring service. While motivated by transparency needs of credit scoring, such a service can be valuable for any application requiring highly predictive yet palatable scoring algorithms.","","Electronic:978-1-4799-5066-9; POD:978-1-4799-5067-6","10.1109/SCC.2014.120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6930622","cluster computing;credit scoring;design;domain expertise;scorecard;segmentation;tree ensembles;visualization","Analytical models;Clustering algorithms;Computational modeling;Load modeling;Predictive models;Stochastic processes;Vegetation","data visualisation;expert systems;financial data processing;learning (artificial intelligence)","automated analytic computing service;credit scorecard design;credit scoring models;domain expertise;expert-imposed palatability restrictions;financial services;machine learning;model visualization;tree ensemble model","","0","","2","","","","June 27 2014-July 2 2014","","IEEE","IEEE Conferences"
"Using semi-supervised machine learning to address the Big Data problem in DNS networks","L. Watkins; S. Beck; J. Zook; A. Buczak; J. Chavis; W. H. Robinson; J. A. Morales; S. Mishra","Johns Hopkins University, Information Security Institute, Baltimore, MD USA","2017 IEEE 7th Annual Computing and Communication Workshop and Conference (CCWC)","20170302","2017","","","1","6","The problem of Big Data in cyber security (i.e., too much network data to analyze) compounds itself every day. Our approach is based on a fundamental characteristic of Big Data: an overwhelming majority of the network traffic in a traditionally secured enterprise (i.e., using defense-in-depth) is non-malicious. Therefore, one way of eliminating the Big Data problem in cyber security is to ignore the overwhelming majority of an enterprise's non-malicious network traffic and focus only on the smaller amounts of suspicious or malicious network traffic. Our approach uses simple clustering along with a dataset enriched with known malicious domains (i.e., anchors) to accurately and quickly filter out the non-suspicious network traffic. Our algorithm has demonstrated the predictive ability to accurately filter out approximately 97% (depending on the algorithm used) of the non-malicious data in millions of Domain Name Service (DNS) queries in minutes and identify the small percentage of unseen suspicious network traffic. We demonstrate that the resulting network traffic can be analyzed with traditional reputation systems, blacklists, or in-house threat tracking sources (we used virustotal.com) to identify harmful domains that are being accessed from within the enterprise network. Specifically, our results show that the method can reduce a dataset of 400k query-answer domains (with complete malicious domain ground truth) down to only 3% containing 99% of all malicious domains. Further, we demonstrate that this capability scales to 10 million query-answer pairs, which it can reduce by 97% in less than an hour.","","Electronic:978-1-5090-4228-9; POD:978-1-5090-4229-6","10.1109/CCWC.2017.7868376","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868376","","Approximation algorithms;Big data;Clustering algorithms;Computer security;Semisupervised learning;Sensitivity;Telecommunication traffic","Big Data;learning (artificial intelligence);query processing;security of data","Big Data problem;DNS networks;cyber security;domain name service queries;enterprise nonmalicious network traffic;in-house threat tracking sources;query-answer domains;semi-supervised machine learning;unseen suspicious network traffic","","","","","","","","9-11 Jan. 2017","","IEEE","IEEE Conferences"
"Extracting significant features based on candlestick patterns using unsupervised approach","S. Sangsawad; C. C. Fung","College of Information and Communication Technology, Rangsit University, Phathumthani, Thailand","2017 2nd International Conference on Information Technology (INCIT)","20180115","2017","","","1","5","This paper proposes algorithms for the extraction of features from candlestick patterns for technical analysis of share indices. The significant features consist of: the direction of candlestick, the gap between CLOSE and OPEN price of two candlesticks, the body level of current and previous candlesticks, and the length of the candlesticks. K-Means clustering approach is applied for solving the unclearly defined length of Upper Shadow, Body and Lower Shadow. The Thai SET index OHLC data from 1990 to 2017 are used as the experimental dataset. The results show the similarity between the candlestick chart from raw data and decoding data, which is applied by the proposed algorithms. The output result from the approach can be used as the input to other machine learning methods such as Artificial Neuron Networks, Reinforcement Learning, or Content Based Image Retrieval (CBIR).","","Electronic:978-1-5386-1431-0; POD:978-1-5386-1432-7","10.1109/INCIT.2017.8257862","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8257862","candlestick patterns;feature extraction;k-means clustering;time series forecast;unsupervised learning","Conferences;Data mining;Feature extraction;Genetics;Information technology;Programming;Stock markets","feature extraction;financial data processing;pattern clustering;share prices","CLOSE price;K-Means clustering approach;Lower Shadow;OPEN price;Thai SET index OHLC data;Upper Shadow;candlestick chart;candlestick patterns;share indices;unsupervised approach","","","","","","","","2-3 Nov. 2017","","IEEE","IEEE Conferences"
"From CPU to FPGA — Acceleration of self-organizing maps for data mining","J. Lachmair; T. Mieth; R. Griessl; J. Hagemeyer; M. Porrmann","Cognitronics and Sensor Systems Group CITEC, Bielefeld University, Germany","2017 International Joint Conference on Neural Networks (IJCNN)","20170703","2017","","","4299","4308","Big data and machine learning applications are posing steadily increasing challenges to the used compute platforms in terms of performance and energy efficiency. In this paper we utilize the highly scalable heterogeneous server platform RECS for evaluation of a wide variety of hardware platforms ranging from general purpose CPUs via ARM-based SoCs to GPGPUs and FPGAs. The self-organizing map, a popular neural network model for unsupervised clustering and dimensionality reduction, is used as a typical example for machine learning applications in the big data domain. Optimized implementations of the algorithm have been developed for each of the target architectures. An in-depth analysis of the achieved performance and energy efficiency for a wide variety of application parameters shows that no single architecture performs best in terms of energy efficiency for the complete design space. In our study, ARM-based SoCs achieved the highest efficiency for small network sizes while FPGAs and GPGPUs perform best for large data sets. Compared to an implementation based on the Matlab SOM toolbox, our optimized multi-threaded CPU implementation achieves two orders of magnitude higher performance and energy efficiency. Large simulations especially benefit from the FPGA implementation, which outperforms the optimized CPU implementation by a factor of 220 and provides a 28-times higher energy efficiency.","","Electronic:978-1-5090-6182-2; POD:978-1-5090-6183-9","10.1109/IJCNN.2017.7966400","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966400","ARM-SoC;Big Data;FPGA;FPGA-Cluster;GPGPU;Machine Learning;Performance Evaluation;SOM;Self-Organizing Map","Backplanes;Bandwidth;Computer architecture;Field programmable gate arrays;Neurons;Self-organizing feature maps;Switches","Big Data;data mining;energy conservation;field programmable gate arrays;learning (artificial intelligence);microcontrollers;self-organising feature maps;system-on-chip","ARM-based SoC;Big Data applications;Big Data domain;CPU implementation;FPGA implementation;GPGPU;RECS;data mining;dimensionality reduction;energy efficiency;highly scalable heterogeneous server;machine learning applications;multithreaded CPU implementation;neural network model;self-organizing map;target architectures;unsupervised clustering","","","","","","","","14-19 May 2017","","IEEE","IEEE Conferences"
"CStorage: An efficient classification-based image storage system in cloud datacenters","H. Shen; H. Zhou","Department of Computer Science, University of Virginia, Charlottesville, VA","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","480","485","Image storage systems are designed to support images sharing and retrieving applications, especially for smartphone and other mobile devices. Existing image storage systems store images in random data servers without considering their similarities. When a front-end server sends out an image query, it receives similar images from a large number of data servers, causing possible network incast congestion and long query latency. To solve this problem, we propose CStorage, an efficient classification-based image storage system. In CStorage, similar images are stored in the same data server. Thus, a front-end server receives query results from a single data server, which reduces the occurrence of incast congestion as well as the image retrieval latency. CStorage also leverages the deep learning technique to cluster images and provides a high precision and recall rates. Experimental results show the effectiveness of CStorage in reducing image retrieval latency and improving precision and recall rates of searching results.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8257961","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8257961","Deep learning;Image storage systems;Incast congestion;Locality-sensitive hashing","Clustering algorithms;Feature extraction;Image retrieval;Image storage;Indexes;Machine learning;Servers","cloud computing;content-based retrieval;image retrieval;learning (artificial intelligence);mobile computing;pattern classification;storage management","CStorage;classification-based image storage system;cloud datacenters;cluster images;deep learning;image query;image retrieval;image storage systems;mobile devices;random data servers;smartphone","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Performance Study of Classification Algorithms for Consumer Online Shopping Attitudes and Behavior Using Data Mining","R. A. E. D. Ahmeda; M. E. Shehaba; S. Morsya; N. Mekawiea","","2015 Fifth International Conference on Communication Systems and Network Technologies","20151001","2015","","","1344","1349","The online retail industry is one of the world's largest and fastest growing industries having huge amount of online sales data. This sales data includes information about customer buying history, goods or services offered for the customers. Hidden relationships in sales data can be discovered from the application of data mining techniques. Data mining is an inter disciplinary promising field that focuses on access of information useful for high level decisions and also include machine learning to help online shopping stores to indentify online customer behavior to recommend for him the appropriate products he/she is interesting to them, because the growing popularity and acceptance of e-commerce platforms, users face an ever increasing burden in actually choosing the right product from the large number of online offers. Thus, techniques for personalization and shopping guides are needed by users. For a pleasant and successful shopping experience, users need to know easily which products to buy with high confidence. In this paper eleven data mining classification techniques will be comparatively tested to find the best classifier fit for consumer online shopping attitudes and behavior according to obtained dataset for big agency of online shopping, the results shows that decision table classifier and filtered classifier gives the highest accuracy and the lowest accuracy is achieved by classification via clustering and simple cart, also this paper will provide a recommender system based on decision table classifier helping the customer to find the products he/she is searching for in some ecommerce web sites. Recommender system learns from the information about customers and products and provides appropriate personalized recommendations to customers to find the desired products.","","Electronic:978-1-4799-1797-6; POD:978-1-4799-1798-3","10.1109/CSNT.2015.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280138","Data mining;Machine learning;WEKA;classification;online shopping","Accuracy;Area measurement;Classification algorithms;Clustering algorithms;Data mining;Filtering algorithms;Internet","Internet;electronic commerce;learning (artificial intelligence);retail data processing","classification algorithms;consumer online shopping attitudes;data mining techniques;decision table classifier;e-commerce platforms;high level decisions;machine learning;online customer behavior;online retail industry;recommender system","","2","","16","","","","4-6 April 2015","","IEEE","IEEE Conferences"
"Spark-GPU: An accelerated in-memory data processing engine on clusters","Y. Yuan; M. F. Salmi; Y. Huai; K. Wang; R. Lee; X. Zhang","The Ohio State University","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","273","283","Apache Spark is an in-memory data processing system that supports both SQL queries and advanced analytics over large data sets. In this paper, we present our design and implementation of Spark-GPU that enables Spark to utilize GPU's massively parallel processing ability to achieve both high performance and high throughput. Spark-GPU transforms a general-purpose data processing system into a GPU-supported system by addressing several real-world technical challenges including minimizing internal and external data transfers, preparing a suitable data format and a batching mode for efficient GPU execution, and determining the suitability of workloads for GPU with a task scheduling capability between CPU and GPU. We have comprehensively evaluated Spark-GPU with a set of representative analytical workloads to show its effectiveness. Our results show that Spark-GPU improves the performance of machine learning workloads by up to 16.13x and the performance of SQL queries by up to 4.83x.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840613","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840613","","Computational modeling;Data analysis;Graphics processing units;Java;Parallel processing;Sparks","graphics processing units;learning (artificial intelligence);parallel processing","Apache Spark;CPU;GPU execution;GPU-supported system;SQL queries;Spark-GPU;accelerated in-memory data processing engine;advanced analytics;clusters;general-purpose data processing system;in-memory data processing system;large data sets;machine learning;parallel processing;task scheduling capability","","1","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conferences"
"Statistical Twitter Spam Detection Demystified: Performance, Stability and Scalability","G. Lin; N. Sun; S. Nepal; J. Zhang; Y. Xiang; H. Hassan","School of Information Technology, Deakin University, Geelong, VIC, Australia","IEEE Access","20170630","2017","5","","11142","11154","With the trend that the Internet is becoming more accessible and our devices being more mobile, people are spending an increasing amount of time on social networks. However, due to the popularity of online social networks, cyber criminals are spamming on these platforms for potential victims. The spams lure users to external phishing sites or malware downloads, which has become a huge issue for online safety and undermined user experience. Nevertheless, the current solutions fail to detect Twitter spams precisely and effectively. In this paper, we compared the performance of a wide range of mainstream machine learning algorithms, aiming to identify the ones offering satisfactory detection performance and stability based on a large amount of ground truth data. With the goal of achieving real-time Twitter spam detection capability, we further evaluated the algorithms in terms of the scalability. The performance study evaluates the detection accuracy, the true/false positive rate and the F-measure; the stability examines how stable the algorithms perform using randomly selected training samples of different sizes. The scalability aims to better understand the impact of the parallel computing environment on the reduction of the training/testing time of machine learning algorithms.","","","10.1109/ACCESS.2017.2710540","Australian Government Research Training Program Scholarship; 10.13039/501100001809 - National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7937783","Machine learning;Twitter;parallel computing;scalability;spam detection","Boosting;Feature extraction;Machine learning algorithms;Real-time systems;Scalability;Training;Twitter","invasive software;learning (artificial intelligence);social networking (online);statistical analysis;unsolicited e-mail","F-measure evaluation;OSN;cyber criminals;detection accuracy evaluation;external phishing sites;false-positive rate evaluation;ground truth data;machine learning algorithms;malware downloads;online safety;online social networks;parallel computing environment;statistical Twitter spam detection;testing time reduction;training time reduction;true-positive rate evaluation;user experience","","","","","","","20170601","2017","","IEEE","IEEE Journals & Magazines"
"Online human action recognition based on improved dynamic time warping","H. Pan; J. Li","College of Computer Science, Chongqing University, Chongqing, China","2016 IEEE International Conference on Big Data Analysis (ICBDA)","20160714","2016","","","1","5","Online human action recognition has broad application prospect in many fields of computer vision. Simultaneously, with the advent of depth camera, it brings on a new trend of online human action recognition but still present some unique challenges. In this paper, to solve the lower accuracy of the existing online human action recognition algorithm based on depth camera, we adopt the improved Dynamic Time Warping (DTW) algorithm to reduce the pathologic alignment caused by traditional DTW algorithm in features extraction and template generation, and then the recognition accuracy will be enhanced. Finally, this proposed approach will be evaluated on MSRC-12 Kinect Gesture dataset. The experimental evaluations show that the proposed approach achieves superior performance to the state of the art algorithms.","","CD-ROM:978-1-4673-9589-2; Electronic:978-1-4673-9591-5; POD:978-1-4673-9592-2","10.1109/ICBDA.2016.7509843","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7509843","Dynamic Time Warping;Human Action Recognition;Jointly Sparse Coding;Machine Learning;Spectral Clustering","Cameras;Clustering algorithms;Dynamic programming;Feature extraction;Heuristic algorithms;Hidden Markov models;Skeleton","computer vision;dynamic programming;feature extraction;image recognition","DTW algorithm;computer vision;dynamic time warping;feature extraction;online human action recognition","","","","","","","","12-14 March 2016","","IEEE","IEEE Conferences"
"DHCRF: A Distributed Conditional Random Field Algorithm on a Heterogeneous CPU-GPU Cluster for Big Data","W. Ai; K. Li; C. Chen; J. Peng; K. Li","Coll. of Inf. Sci. & Eng., Hunan Univ., Changsha, China","2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS)","20170717","2017","","","2372","2379","As one of the most recognized models in machine learning, the conditional random fields (CRF) has been widely used in many applications. As the parameter estimation of CRF is highly time-consuming, how to improve the performance of CRF has received significant attention, in particular in the big data environment. To deal with large-scale data, CPU-based or GPU-based parallelization solutions have been proposed to improve performance. However, the problem is an ongoing one. In this paper, we focus on the big data environment and propose a distributed CRF on a heterogeneous CPU-GPU cluster called DHCRF. Our approach differs from previous work. Specifically, it leverages a three-stage heterogeneous Map and Reduce operation to improve the performance, making full use of CPU-GPU collaborative computing capabilities in a big data environment. Furthermore, by combining elastic data partition and intermediate results multiplexing method, the distributed CRF is optimized. Elastic data partition is performed to keep the load balanced, and the intermediate results multiplexing method is adopted to reduce data communication. Experimental results show that the DHCRF outperforms the baseline CRF algorithm and the CPU-based parallel CRF algorithm with notable performance improvement while maintaining competitive correctness at the same time.","1063-6927;10636927","Electronic:978-1-5386-1792-2; POD:978-1-5386-1793-9","10.1109/ICDCS.2017.66","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7980197","Big Data;Conditional Random Fields;Distributed;Heterogeneous CPU-GPU Cluster","Algorithm design and analysis;Big Data;Clustering algorithms;Computational modeling;Graphics processing units;Parameter estimation;Partitioning algorithms","Big Data;distributed algorithms;graphics processing units;microprocessor chips;parameter estimation;random processes;resource allocation","Big Data;CPU-GPU collaborative computing capabilities;CPU-based parallelization;DHCRF;GPU-based parallelization;conditional random fields;data communication;distributed CRF performance;distributed conditional random field algorithm;elastic data partition;heterogeneous CPU-GPU cluster;intermediate results multiplexing method;large-scale data;parameter estimation","","","","","","","","5-8 June 2017","","IEEE","IEEE Conferences"
"Autonomous Data Density based clustering method","P. P. Angelov; X. Gu; G. Gutierrez; J. A. Iglesias; A. Sanchis","School of Computing and Communications, InfoLab21, Lancaster University, LA1 4WA, UK","2016 International Joint Conference on Neural Networks (IJCNN)","20161103","2016","","","2405","2413","It is well known that clustering is an unsupervised machine learning technique. However, most of the clustering methods need setting several parameters such as number of clusters, shape of clusters, or other user- or problem-specific parameters and thresholds. In this paper, we propose a new clustering approach which is fully autonomous, in the sense that it does not require parameters to be pre-defined. This approach is based on data density automatically derived from their mutual distribution in the data space. It is called ADD clustering (Autonomous Data Density based clustering). It is entirely based on the experimentally observable data and is free from restrictive prior assumptions. This new method exhibits highly accurate clustering performance. Its performance is compared on benchmarked data sets with other competitive alternative approaches. Experimental results demonstrate that ADD clustering significantly outperforms other clustering methods yet does not require restrictive user- or problem-specific parameters or assumptions. The new clustering method is a solid basis for further applications in the field of data analytics.","","","10.1109/IJCNN.2016.7727498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727498","data analytics;data density;fully autonomous clustering;mutual distribution","Clustering algorithms;Clustering methods;Data analysis;Data mining;Electronic mail;Kernel;Measurement","data analysis;pattern clustering;unsupervised learning","ADD clustering;autonomous data density based clustering method;data analytics;problem-specific parameters;restrictive prior assumptions;unsupervised machine learning technique","","1","","","","","","24-29 July 2016","","IEEE","IEEE Conferences"
"TencentBoost: A Gradient Boosting Tree System with Parameter Server","J. Jiang; J. Jiang; B. Cui; C. Zhang","Sch. of Software & Microelectron., Peking Univ., Beijing, China","2017 IEEE 33rd International Conference on Data Engineering (ICDE)","20170518","2017","","","281","284","Gradient boosting tree (GBT), a widely used machine learning algorithm, achieves state-of-the-art performance in academia, industry, and data analytics competitions. Although existing scalable systems which implement GBT, such as XGBoost and MLlib, perform well for datasets with medium-dimensional features, they can suffer performance degradation for many industrial applications where the trained datasets contain highdimensional features. The performance degradation derives from their inefficient mechanisms for model aggregation-either mapreduce or all-reduce. To address this high-dimensional problem, we propose a scalable execution plan using the parameter server architecture to facilitate the model aggregation. Further, we introduce a sparse-pull method and an efficient index structure to increase the processing speed. We implement a GBT system, namely TencentBoost, in the production cluster of Tencent Inc. The empirical results show that our system is 2-20× faster than existing platforms.","","Electronic:978-1-5090-6543-1; POD:978-1-5090-6544-8","10.1109/ICDE.2017.87","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7929984","","Boosting;Buildings;Computer architecture;Histograms;Indexes;Servers;Training","file servers;learning (artificial intelligence);workstation clusters","GBT;GBT system;TencentBoost;data analytics;gradient boosting tree system;high-dimensional features;index structure;machine learning algorithm;medium-dimensional features;model aggregation;parameter server architecture;performance degradation;sparse-pull method","","1","","","","","","19-22 April 2017","","IEEE","IEEE Conferences"
"Testing of algorithms for anomaly detection in Big data using apache spark","S. N. Lighari; D. M. A. Hussain","Department of Energy Technology, Aalborg University, Esbjerg, Denmark","2017 9th International Conference on Computational Intelligence and Communication Networks (CICN)","20180319","2017","","","97","100","The constant upsurge in the size of networks and the data massively produced by them has made the data analysis very challenging principally the data attaining the boundaries of big data and it becomes even more difficult to detect intrusions in the case of big data. In this era, the experts find very limited tools and methods to analyze big data for security reasons. Either we need to device new tools or we can use existing tools in a novel manner to achieve the purpose of big data security analysis. In this paper, we are using apache spark a big data tool for analyzing the big dataset for anomaly detection. The anomaly detection is performed by using different machine learning algorithms like Logistic regression, Support vector machine, Naïve bayes, Decision trees, Random forest, and Kmeans. More or less all the aforementioned algorithms are capable to detect anomalies in big data but we need to know how efficiently each performs. The main objective of this investigation is to find the most efficient algorithm in the context of anomaly detection. In this regard, we set to compare their training time, prediction time, and the rate of accuracy. The analysis was implemented on Kddcup99 dataset. Although this dataset is of size in megabytes but it meets our purpose here for big data security analytics.","","CD:978-1-5090-5000-0; Electronic:978-1-5090-5001-7; POD:978-1-5090-5002-4","10.1109/CICN.2017.8319364","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8319364","Big data;Machine learning;Security analytics","Big Data;Classification algorithms;Prediction algorithms;Predictive models;Sparks;Tools;Training","Bayes methods;Big Data;data analysis;decision trees;learning (artificial intelligence);parallel processing;pattern classification;pattern clustering;regression analysis;security of data;support vector machines","Decision trees;Kddcup99 dataset;Kmeans;Naïve bayes;Random forest;Support vector machine;anomaly detection;apache spark;big data security analysis;big data security analytics;big data tool;logistic regression;machine learning algorithms","","","","","","","","16-17 Sept. 2017","","IEEE","IEEE Conferences"
"Hybrid model of rule based and clustering analysis for big data security","S. N. Lighari; D. M. A. Hussain","Department of Energy Technology, Aalborg University, Denmark","2017 First International Conference on Latest trends in Electrical Engineering and Computing Technologies (INTELLECT)","20180201","2017","","","1","5","The most of the organizations tend to accumulate the data related to security, which goes up-to terabytes in every month. They collect this data to meet the security requirements. The data is mostly in the shape of logs like Dns logs, Pcap files, and Firewall data etc. The data can be related to any communication network like cloud, telecom, or smart grid network. Generally, these logs are stored in databases or warehouses which becomes ultimately gigantic in size. Such a huge size of data upsurge the importance of security analytics in big data. In surveys, the security experts grumble about the existing tools and recommend for special tools and methods for big data security analysis. In this paper, we are using a big data analysis tool, which is known as apache spark. Although this tool is used for general purpose but we have used this for security analysis. It offers a very good library for machine learning algorithms including the clustering which is the main algorithm used in our work. In this work, we have developed a novel model, which combines rule based and clustering analysis for security analysis of big dataset. The dataset we are using in our experiment is the Kddcup99 which is a widely used dataset for intrusion detection. It is of MBs in size but can be used as a test case for big data security analysis.","","Electronic:978-1-5386-2969-7; POD:978-1-5386-2970-3","10.1109/INTELLECT.2017.8277627","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8277627","Big data;Communication networks;Machine Learning;Security analysis","Analytical models;Anomaly detection;Big Data;Data models;Security;Sparks;Tools","Big Data;data analysis;learning (artificial intelligence);pattern clustering;security of data","Firewall data;big data analysis tool;big data security analysis;clustering analysis;logs;security analytics;security experts;security requirements","","","","","","","","15-16 Nov. 2017","","IEEE","IEEE Conferences"
"Distributed embedded deep learning based real-time video processing","W. Zhang; Dehai Zhao; L. Xu; Z. Li; Wenjuan Gong; Jiehan Zhou","Department of Software Engineering, China University of Petroleum, No. 66 Changjiang West Road, Qingdao, China, 266580","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","001945","001950","There arises the needs for fast processing of continuous video data using embedded devices, for example the one needed for UAV aerial photography. In this paper, we proposed a distributed embedded platform built with NVIDIA Jetson TX1 using deep learning techniques for real time video processing, mainly for object detection. We design a Storm based distributed real-time computation platform and ran object detection algorithm based on convolutional neural networks. We have evaluated the performance of our platform by conducting real-time object detection on surveillance video. Compared with the high end GPU processing of NVIDIA TITAN X, our platform achieves the same processing speed but a much lower power consumption when doing the same work. At the same time, our platform had a good scalability and fault tolerance, which is suitable for intelligent mobile devices such as unmanned aerial vehicles or self-driving cars.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844524","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844524","Distributed embedded platform;Stream processing;low power consumption;video processing","Machine learning;Neural networks;Object detection;Power demand;Real-time systems;Storms;Streaming media","autonomous aerial vehicles;fault tolerance;graphics processing units;learning (artificial intelligence);object detection;photography;video signal processing","NVIDIA Jetson TX1;NVIDIA TITAN X;UAV aerial photography;continuous video data processing;convolutional neural networks;distributed embedded deep learning;fault tolerance;high-end GPU processing;intelligent mobile devices;object detection algorithm;power consumption","","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conferences"
"HarpLDA+: Optimizing latent dirichlet allocation for parallel efficiency","B. Peng; B. Zhang; L. Chen; M. Avram; R. Henschel; C. Stewart; S. Zhu; E. Mccallum; L. Smith; T. Zahniser; J. Omer; J. Qiu","School of Informatics and Computing, Indiana University","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","243","252","Latent Dirichlet Allocation (LDA) is a widely used machine learning technique in topic modeling and data analysis. Training large LDA models on big datasets involves dynamic and irregular computation patterns and is a major challenge to both algorithm optimization and system design. In this paper, we present a comprehensive benchmarking of our novel synchronized LDA training system HarpLDA+ based on Hadoop and Java. It demonstrates impressive performance when compared to three other MPI/C++ based state-of-the-art systems, which are LightLDA, F+NomadLDA, and WarpLDA. HarpLDA+ uses optimized collective communication with a timer control for load balance, leading to stable scalability in both shared-memory and distributed systems. We demonstrate in the experiments that HarpLDA+ is effective in reducing synchronization and communication overhead and outperforms the other three LDA training systems.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8257932","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8257932","","Algorithm design and analysis;Computational modeling;Data models;Mathematical model;Partitioning algorithms;Synchronization;Training","Big Data;C++ language;Java;data analysis;learning (artificial intelligence);message passing;optimisation;parallel processing;resource allocation;statistical analysis;systems analysis","Hadoop;HarpLDA+;Java;LDA models;LDA training system;LDA training systems;MPI/C++ based state-of-the-art systems;algorithm optimization;big datasets;comprehensive benchmarking;data analysis;distributed systems;dynamic computation patterns;irregular computation patterns;latent dirichlet allocation;load balancing;machine learning technique;parallel efficiency;shared-memory;system design;topic modeling","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Axioms to characterize efficient incremental clustering","S. Bandyopadhyay; M. N. Murty","IBM Research, India","2016 23rd International Conference on Pattern Recognition (ICPR)","20170424","2016","","","450","455","Although clustering is one of the central tasks in machine learning for the last few decades, analysis of clustering irrespective of any particular algorithm was not undertaken for a long time. In the recent literature, axiomatic frameworks have been proposed for clustering and its quality. But none of the proposed frameworks has concentrated on the computational aspects of clustering, which is essential in current big data analytics. In this paper, we propose an axiomatic framework for clustering which considers both the quality and the computational complexity of clustering algorithms. The axioms proposed by us necessarily associate the problem of clustering with the important concept of incremental learning and divide and conquer learning. We also propose an order independent incremental clustering algorithm which satisfies all of these axioms in some constrained manner.","","Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9","10.1109/ICPR.2016.7899675","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899675","","Algorithm design and analysis;Big Data;Clustering algorithms;Computational complexity;Machine learning algorithms;Merging;Partitioning algorithms","computational complexity;divide and conquer methods;learning (artificial intelligence);pattern clustering","computational complexity;divide and conquer learning;incremental learning;order independent incremental clustering algorithm","","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conferences"
"SVM-based association rules for knowledge discovery and classification","A. Anaissi; M. Goyal","Center of Quantum Computation and Intelligent Systems (QCIS), Faculty of Engineering and Information Technology (FEIT), University of Technology Sydney (UTS) Broadway NSW 2007, Australia","2015 2nd Asia-Pacific World Congress on Computer Science and Engineering (APWC on CSE)","20160523","2015","","","1","5","Improving analysis of market basket data requires the development of approaches that lead to recommendation systems that are tailored to specifically benefit grocery chain. The main purpose of that is to find relationships existing among the sales of the products that can help retailer identify new opportunities for cross-selling their products to customers. This paper aims to discover knowledge patterns hidden in large data set that can yield more understanding to the data holders and identify new opportunities for imperative tasks including strategic planning and decision making. This paper delivers a strategy for the implementation of a systematic analysis framework built on the established principles used in data mining and machine learning. The primary goal of that is to form the foundation of what we envisage will be a new recommendation system in the market. Uniquely, our strategy seeks to implement data mining tools that will allow the analyst to interact with the data and address business questions such as promotions advertisement. We employ Apriori algorithm and support vector machine to implement our recommendation systems. Experiments are done using a real market dataset and the 0.632+ bootstrap method is used here in order to evaluate our framework. The obtained results suggest that the proposed framework will be able to generate benefits for grocery chain using a real-world grocery store data.","","Electronic:978-1-5090-0713-4; POD:978-1-5090-0714-1","10.1109/APWCCSE.2015.7476236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7476236","Apriori algorithm;SVM;association rules;data mining;machine learning","Algorithm design and analysis;Clustering algorithms;Data mining;Decision making;Itemsets;Support vector machines","data mining;decision making;learning (artificial intelligence);pattern classification;recommender systems;retail data processing;strategic planning;support vector machines","Apriori algorithm;SVM-based association rules;bootstrap method;data mining;decision making;knowledge classification;knowledge discovery;machine learning;market basket data;real-world grocery store data;recommendation systems;strategic planning;support vector machine","","","","","","","","2-4 Dec. 2015","","IEEE","IEEE Conferences"
"Building a Distributed Generic Recommender Using Scalable Data Mining Library","L. Bhatia; S. S. Prasad","Dept. of CSE, JSS Acad. of Tech. Educ., Noida, India","2015 IEEE International Conference on Computational Intelligence & Communication Technology","20150402","2015","","","98","102","Recommender systems produce list of recommended items through content based or collaborative or hybrid combination of these two approaches. The paper presents a generic approach for performing collaborative filtering using data mining techniques to discover relationships among users and items. Using generic model techniques a single recommender system can produce recommendations about a variety of items. The methodologies reported for development of recommender systems are not efficient for generic application. The difference in the implementations of recommender depends upon how they analyze the big input data to recognize the similarity between users and items that indicates the relevant preferences for that user. Generic user based recommender works with data model encapsulating recommender input data in Apache Mahout which is extensible data mining library. The recommender system framework can use any similarity metric. We have chosen Pearson correlation because the computation would be fast. One of the parameters of user based recommender is User-Neighborhood. Fixed-size Neighborhood has the advantage that the recommendations are based on fewer similar users. Hadoop software library allows distributed processing of big data across multiple clusters of nodes. The paper describes an implementation using Apache Mahout and Hadoop and also explores feasible augmentation that can enhance efficiency of recommendation. This paper reports successful implementation of generic recommender.","","Electronic:978-1-4799-6023-1; POD:978-1-4799-6024-8","10.1109/CICT.2015.129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7078675","Big data;Hadoop;Mahout;collaborative filtering;data mining;generic recommender;machine learning;recommender systems","Collaboration;Conferences;Correlation;Data mining;Data models;Recommender systems","Big Data;collaborative filtering;data mining;data models;parallel processing;recommender systems;software libraries","Apache Mahout;Hadoop software library;Pearson correlation;big data;collaborative filtering;data model;distributed generic recommender;distributed processing;fixed-size neighborhood;generic user-based recommender;recommender system framework;scalable data mining library;user-neighborhood","","3","","16","","","","13-14 Feb. 2015","","IEEE","IEEE Conferences"
"Skill grouping method: Mining and clustering skill differences from body movement BigData","S. Yamagiwa; Y. Kawahara; N. Tabuchi; Y. Watanabe; T. Naruo","Faculty of Engineering, Information and Systems, University of Tsukuba, Tsukuba, Ibaraki, 305-8573 Japan","2015 IEEE International Conference on Big Data (Big Data)","20151228","2015","","","2525","2534","Capturing human movement has become available in detail due to the advancement of motion sensor technology integrated by micro-machine and also due to the one of optical recording by high speed and high resolution image sensors. Therefore, we can easily record the human activity as the body movement BigData and analyze it to quest skill to become an expert of a target body movement. Especially, in the sports activity, the quest for becoming an expert athlete has been tried by using a mathematical model of an ideal body movement experienced from the biomechanics approach. The skill is discussed by comparing the differences from the predicted coordinates of body parts captured during the target performance. However, the approach potentially includes difficulties such as modeling the body control from the dynamics system for all human movements. And also the approach needs for adjusting jitters of the individual characteristics. Therefore, when applying the conventional approach, we must discuss a huge number of combinations of mathematical models and then we would find a model for the ideal body movement. To overcome the difficulty, this paper proposes an approach to visualize skill differences among experts and beginners from the BigData called the skill grouping method. It exploits the skill groups clustered by machine learning approach based on a kernel method. This paper shows applications of the skill grouping method from sports activities. Those show validities for finding the skill differences comparing to the BigData of skillful athletes, and also the one for managing skill transition of an athlete in a timeline.","","Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9","10.1109/BigData.2015.7364049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7364049","BigData analysis;Body movement data;Kernel method;Sports BigData;Support vector machine","Biomechanics;Data visualization;Electronic mail;Image resolution;Kernel;Mathematical model;Support vector machines","Big Data;data mining;data visualisation;learning (artificial intelligence);pattern clustering;sport","biomechanics approach;body control modeling;body movement BigData;body parts coordinates;dynamics system;high resolution image sensors;high speed image sensors;human activity;human movement;jitters;kernel method;machine learning;mathematical model;micromachine;motion sensor technology;optical recording;skill differences clustering;skill differences mining;skill differences visualization;skill grouping method;skill transition;skillful athletes;sports activity","","","","34","","","","Oct. 29 2015-Nov. 1 2015","","IEEE","IEEE Conferences"
"The interdisciplinary research of big data and wireless channel: A cluster-nuclei based channel model","J. Zhang","Beijing Haidian District, Xitucheng Road No. 10, Mailbox No. 92, 100876, China","China Communications","20170126","2016","13","Supplement2","14","26","Recently, internet stimulates the explosive progress of knowledge discovery in big volume data resource, to dig the valuable and hidden rules by computing. Simultaneously, the wireless channel measurement data reveals big volume feature, considering the massive antennas, huge bandwidth and versatile application scenarios. This article firstly presents a comprehensive survey of channel measurement and modeling research for mobile communication, especially for 5th Generation (5G) and beyond. Considering the big data research progress, then a cluster-nuclei based model is proposed, which takes advantages of both the stochastical model and deterministic model. The novel model has low complexity with the limited number of cluster-nuclei while the cluster-nuclei has the physical mapping to real propagation objects. Combining the channel properties variation principles with antenna size, frequency, mobility and scenario dug from the channel data, the proposed model can be expanded in versatile application to support future mobile research.","1673-5447;16735447","","10.1109/CC.2016.7833457","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7833457","5G;big data;channel model;cluster;machine learning;massive MIMO","5G mobile communication;Antenna measurements;Bandwidth;Big data;Channel models;Computational modeling;Wireless communication","","","","","","","","","","N/A 2016","","IEEE","IEEE Journals & Magazines"
"Social media analysis web application","B. Isakovic; D. Keco; N. Dogru","Department of Information Technologies, International Burch University","2017 XXVI International Conference on Information, Communication and Automation Technologies (ICAT)","20171211","2017","","","1","6","Social media is very important factor in analyzing modern society as a whole, their values, norms, and behaviors, as being a part of our everyday life. This study is oriented towards analyzing social media in order to allow users to create their own preferences to follow (analyze) a specific social media source. The web application has been developed to allow a user to follow specific Facebook accounts and categorize the Facebook posts on those accounts based on the user defined taxonomies. Results of this study are various reports generated from the Facebook posts and their statistics that are clustered based on the user defined taxonomies. The benefit of this project is that any user can track in real time when people are talking about some topic, and it enables anyone to have better insight about society as a whole, their values, norms, what they find interesting, and many other things. This tool is also useful for different companies to track the user feedback on social networks for their products.","","CD:978-1-5386-3335-9; Electronic:978-1-5386-3337-3; POD:978-1-5386-3338-0; USB:978-1-5386-3336-6","10.1109/ICAT.2017.8171606","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8171606","big data;facebook;noSQL database;parallel programming;social media analysis","Companies;Engines;Facebook;Machine learning algorithms;Taxonomy","social networking (online)","Facebook posts;social media analysis web application;social networks;specific Facebook accounts;specific social media source;user defined taxonomies;user feedback","","","","","","","","26-28 Oct. 2017","","IEEE","IEEE Conferences"
"Distributed fuzzy rough prototype selection for Big Data regression","S. Vluymans; H. Asfoor; Y. Saeys; C. Cornelis; M. Tolentino; A. Teredesai; M. De Cock","Department of Applied Mathematics, Computer Science and Statistics, Ghent University, Gent, Belgium","2015 Annual Conference of the North American Fuzzy Information Processing Society (NAFIPS) held jointly with 2015 5th World Conference on Soft Computing (WConSC)","20151001","2015","","","1","6","Size and complexity of Big Data requires advances in machine learning algorithms to adequately learn from such data. While distributed shared-nothing architectures (Hadoop/Spark) are becoming increasingly popular to develop such new algorithms, it is quite challenging to adapt existing machine learning algorithms. In this paper, we propose a solution for big data regression, where the aim is to learn the regression model over large high-dimensional datasets. First, a new distributed implementation of the weighted kNN regression method is presented followed by a novel distributed prototype selection method based on fuzzy rough set theory. Experiments demonstrate that our implementations in Apache Spark for the proposed distributed algorithms handle the size and complexity of modern real-world datasets well. We furthermore show that application of our prototype selection method improves the regression accuracy.","","Electronic:978-1-4673-7248-0; POD:978-1-4673-7249-7; USB:978-1-4673-7247-3","10.1109/NAFIPS-WConSC.2015.7284158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284158","","Approximation methods;Big data;Prototypes;Scalability;Set theory;Sparks;Training","Big Data;fuzzy set theory;learning (artificial intelligence);pattern classification;regression analysis;rough set theory","Apache Spark architecture;Big Data regression;Hadoop architecture;distributed fuzzy rough prototype selection;distributed shared-nothing architectures;k-nearest neighbor regression method;machine learning algorithm;prototype selection method;weighted kNN regression method","","","","30","","","","17-19 Aug. 2015","","IEEE","IEEE Conferences"
"Scalable Nearest Neighbor Algorithms for High Dimensional Data","M. Muja; D. G. Lowe","BitLit Media Inc, Vancouver, BC, Canada","IEEE Transactions on Pattern Analysis and Machine Intelligence","20141001","2014","36","11","2227","2240","For many computer vision and machine learning problems, large training sets are key for good performance. However, the most computationally expensive part of many computer vision and machine learning algorithms consists of finding nearest neighbor matches to high dimensional vectors that represent the training data. We propose new algorithms for approximate nearest neighbor matching and evaluate and compare them with previous algorithms. For matching high dimensional features, we find two algorithms to be the most efficient: the randomized k-d forest and a new algorithm proposed in this paper, the priority search k-means tree. We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature. We show that the optimal nearest neighbor algorithm and its parameters depend on the data set characteristics and describe an automated configuration procedure for finding the best algorithm to search a particular data set. In order to scale to very large data sets that would otherwise not fit in the memory of a single machine, we propose a distributed nearest neighbor matching framework that can be used with any of the algorithms described in the paper. All this research has been released as an open source library called fast library for approximate nearest neighbors (FLANN), which has been incorporated into OpenCV and is now one of the most popular libraries for nearest neighbor matching.","0162-8828;01628828","","10.1109/TPAMI.2014.2321376","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6809191","Nearest neighbor search;algorithm configuration;approximate search;big data","Approximation algorithms;Approximation methods;Clustering algorithms;Computer vision;Machine learning algorithms;Partitioning algorithms;Vegetation","computer vision;image matching;learning (artificial intelligence);search problems;trees (mathematics)","FLANN;automated configuration procedure;computer vision;distributed nearest neighbor matching framework;fast library for approximate nearest neighbors;high dimensional data;high dimensional feature matching;high dimensional vectors;machine learning problems;multiple hierarchical clustering trees;open source library;priority search k-means tree;randomized k-d forest algorithm;scalable nearest neighbor algorithms","","232","","65","","","20140501","Nov. 1 2014","","IEEE","IEEE Journals & Magazines"
"A scalable solution for group feature selection","P. Govindan; R. Chen; K. Scheinberg; S. Srinivasan","Rutgers University","2015 IEEE International Conference on Big Data (Big Data)","20151228","2015","","","2846","2848","In many applications, we may want to build a classifier with high confidence, while reducing the number of features. We consider the case where features are assigned to predefined groups and cannot be removed individually. An additional and important constraint is that the datasets may be very large and may not fit in memory. We use logistic regression with group penalty, which results in sparse solutions at the group level. In our implementation, we apply L-BFGS to approximate the quadratic loss function of logistic regression and use Block Co-ordinate Descent to solve for each group. Our contributions can be summarized as follows: (1) we discuss different scalable approaches, depending on characteristics of the dataset, such as, large number of data points or large number of features or large number of groups; (2) for datasets with large number of data points and few groups of features, we identify the bottlenecks for scalability; (3) we present Spark solutions in Python and discuss the advantages of our solution over alternate solutions; (4) we present the experiments and results on synthetic data and real data from manufacturing applications.","","Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9","10.1109/BigData.2015.7364098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7364098","","Approximation methods;Big data;Logistics;Machine learning algorithms;Runtime;Sparks;Sparse matrices","feature selection;pattern classification;regression analysis","L-BFGS;Python;Spark;block coordinate descent;group feature selection;logistic regression","","","","5","","","","Oct. 29 2015-Nov. 1 2015","","IEEE","IEEE Conferences"
"A deep learning methodology to proliferate golden signoff timing","S. S. Han; A. B. Kahng; S. Nath; A. S. Vydyanathan","Dept. of Inf. & Commun. Eng., Myongji Univ., Yongin, South Korea","2014 Design, Automation & Test in Europe Conference & Exhibition (DATE)","20140421","2014","","","1","6","Signoff timing analysis remains a critical element in the IC design flow. Multiple signoff corners, libraries, design methodologies, and implementation flows make timing closure very complex at advanced technology nodes. Design teams often wish to ensure that one tool's timing reports are neither optimistic nor pessimistic with respect to another tool's reports. The resulting “correlation” problem is highly complex because tools contain millions of lines of black-box and legacy code, licenses prevent any reverse-engineering of algorithms, and the nature of the problem is seemingly “unbounded” across possible designs, timing paths, and electrical parameters. In this work, we apply a “big-data” approach to the timer correlation problem. We develop a machine learning-based tool, Golden Timer eXtension (GTX), to correct divergence in flip-flop setup time, cell arc delay, wire delay, stage delay, and path slack at timing endpoints between timers. We propose a methodology to apply GTX to two arbitrary timers, and we evaluate scalability of GTX across multiple designs and foundry technologies / libraries, both with and without signal integrity analysis. Our experimental results show reduction in divergence between timing tools from 139.3ps to 21.1ps (i.e., 6.6×) in endpoint slack, and from 117ps to 23.8ps (4.9× reduction) in stage delay. We further demonstrate the incremental application of our methods so that models can be adapted to any outlier discrepancies when new designs are taped out in the same technology / library. Last, we demonstrate that GTX can also correlate timing reports between signoff and design implementation tools.","1530-1591;15301591","Electronic:978-3-9815370-2-4; POD:978-1-4799-3297-9","10.7873/DATE.2014.273","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800474","","Delays;Integrated circuit modeling;Libraries;Silicon;Training;Wires","Big Data;delays;flip-flops;learning (artificial intelligence)","Big-Data approach;GTX;IC design flow;advanced technology nodes;cell arc delay;deep learning methodology;electrical parameters;endpoint slack;flip-flop setup time;foundry technology-libraries;golden signoff timing analysis;golden timer extension;machine learning-based tool;outlier discrepancies;path slack;signal integrity analysis;stage delay;time 139.3 ps to 21.1 ps;timer correlation problem;timing paths;wire delay","","0","1","40","","","","24-28 March 2014","","IEEE","IEEE Conferences"
"Improving text classification with word embedding","L. Ge; T. S. Moh","Department of Computer Science, San Jose State University, San Jose, CA, USA","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","1796","1805","One challenge in text classification is that it is difficult to make feature reductions based on the definition of the features. An ineffective feature reduction may even worsen the classification accuracy. Word2Vec, a word embedding method, has recently been gaining popularity due to its high precision rate of analyzing the semantic similarity between words at relatively low computational cost. However, there is limited research about feature reduction using Word2Vec. In this project, we developed a method using Word2Vec to reduce the feature size while increasing the classification accuracy. We achieved feature reduction by loosely clustering similar features using graph search techniques. Similarity thresholds above 0.5 were used in our method to pair and cluster the features. Finally, we utilized Multinomial Naïve Bayes classifier, Support Vector Machine, K Nearest Neighbor and Random Forest classifier to evaluate the effect of our method. Four datasets with dimensions up to 100,000 feature size and 400,000 document size were used to evaluate the result of our method. The result showed that around 4-10% feature reduction was achieved with up to 1-4% improvement of classification accuracy in terms of different datasets and classifiers. Meanwhile, we also succeeded in improving feature reduction and classification accuracy by combining our method with other classic feature reduction techniques such as chi-square and mutual information.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258123","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258123","Feature reduction;KNN;Machine learning;Naïve Bayes;SVM;Word embedding;Word2Vec","Computational efficiency;Computational modeling;Feature extraction;Ontologies;Semantics;Support vector machines;Text categorization","Bayes methods;feature extraction;learning (artificial intelligence);pattern classification;support vector machines;text analysis;word processing","K Nearest Neighbor;Multinomial Naïve Bayes classifier;Random Forest classifier;Support Vector Machine;Word2Vec;classic feature reduction techniques;classification accuracy;feature reduction;feature size;ineffective feature reduction;text classification;word embedding method","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Adaptive Cost Efficient Framework for Cloud-Based Machine Learning","R. Pakdel; J. Herbert","Dept. of Comput. Sci., Univ. Coll. Cork, Cork, Ireland","2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC)","20170911","2017","2","","155","160","Machine learning is an increasingly important form of cognitive computing, making progress in several application areas. Machine learning often involves big data sets and is computationally challenging, requiring efficient use of resources. The use of cloud computing as the platform for machine learning offers advantages of scalability and efficient use of hardware. It may, however, be difficult to provision appropriate cost-effective resources for a machine learning task. Our experiments have shown that there can be radical differences between different datasets and different algorithms on the same dataset. The cloud-based machine learning framework presented here aims to provide multiple levels of efficient use of resources and uses a high-level cost model to deal with overall cost-efficiency with respect to cloud service providers. The cost model allows evaluation of trade-offs and supports the choice of appropriate provider resources based on user-defined criteria. A user may choose to prioritize performance, prioritize cost or specify a cost-performance balance. An Amazon AWS cost model for instances is used to illustrate the practical benefits of using the approach - it is seen that large savings can be made by employing this job-specific monitoring and cost-performance analysis. The method can provide all the information for a comparison across different cloud service providers as well as comparisons across the Amazon AWS offerings.","0730-3157;07303157","Electronic:978-1-5386-0367-3; POD:978-1-5386-0368-0","10.1109/COMPSAC.2017.42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029910","","Cloud computing;Computational modeling;Feature extraction;Measurement;Monitoring;Pricing;Unified modeling language","Big Data;cloud computing;costing;learning (artificial intelligence)","Amazon AWS cost model;adaptive cost efficient framework;big data sets;cloud computing;cloud service providers;cloud-based machine learning;cognitive computing;cost-efficiency;cost-performance analysis;cost-performance balance;job-specific monitoring;savings;user-defined criteria","","","","","","","","4-8 July 2017","","IEEE","IEEE Conferences"
"Event detection on large social media using temporal analysis","A. Aldhaheri; Jeongkyu Lee","School of Engineering, University of Bridgeport, Connecticut 06604, United States of America","2017 IEEE 7th Annual Computing and Communication Workshop and Conference (CCWC)","20170302","2017","","","1","6","Social media networks are now considered as one of the major news channels that breaks news as they fold. The problem of event detection based on social media has attracted researchers' attention recently because of the enormous popularity of social media. Existing approaches focus on features that don't reflect full characteristics of the social network. For the purpose of this research, we define an event as an occurrence that has enough force and momentum that could create an observable change of the context of a social network. Such a definition provides us with a wider perspective through which we can view the big picture of the social network. In this research, we propose a novel framework for detecting events on social media. We introduce a temporal approach to detect structural change of the social network that reflects an occurrence of an event using machine learning algorithms. In this study, we show that processing temporal social networks captures the complete complexity of the social network, which results in a higher accuracy of event detection.","","Electronic:978-1-5090-4228-9; POD:978-1-5090-4229-6","10.1109/CCWC.2017.7868467","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868467","Big data;Event detection;Machine learning;Social media analysis;data mining","Clustering algorithms;Event detection;Feature extraction;Media;Shape;Twitter","Big Data;data mining;information retrieval;learning (artificial intelligence);social networking (online)","Big data;event detection;machine learning;news channels;social media networks;temporal analysis;temporal social network processing","","","","","","","","9-11 Jan. 2017","","IEEE","IEEE Conferences"
"Inertia Based Recognition of Daily Activities with ANNs and Spectrotemporal Features","O. Kilinc; A. Dalzell; I. Uluturk; I. Uysal","Electr. Eng. RFID Lab. for Appl. Res., Univ. of South Florida, Tampa, FL, USA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","733","738","As mobile and personal health devices gain in popularity, increasing amounts of data is collected via their embedded sensors such as heart rate monitors and accelerometers. Data analytics and more specifically machine learning algorithms can transform this data into actionable information to improve personal healthcare and quality of life. The main objective of this study is to develop an algorithmic classification framework using feed-forward multilayer perceptrons and statistically rich spectrotemporal features to recognize daily activities based on 3-axis acceleration data. A multitude of MLP topologies and setups, such as different numbers and sizes of hidden layers, supervised output structuring, etc. are tested to comprehensively analyze the clustering capabilities of the artificial neural network for a wide-range of settings. In addition, the contribution of subset of features to classification accuracy is studied to identify respective information potentials and further improve accuracy. Publicly available wrist-worn accelerometer dataset from University of California Irvine's machine learning repository is used for fair comparison with the most recent literature published using the same dataset. Results indicate a significant improvement in recognition rate where the overall accuracy over seven selected activity classes is 91% compared to 54% of the latest publication using the same dataset.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424408","accelerometer;classification;daily activity;health;inertia;machine learning;recognition","Acceleration;Accelerometers;Artificial neural networks;Feature extraction;Mobile communication;Sensors;Time-domain analysis","data analysis;health care;learning (artificial intelligence);medical computing;multilayer perceptrons;pattern classification","3-axis acceleration data;ANN;University of California Irvine machine learning repository;algorithmic classification framework;artificial neural networks;daily activity recognition;data analytics;feedforward multilayer perceptrons;inertia based recognition;machine learning algorithm;mobile health devices;personal health devices;spectrotemporal features;wrist-worn accelerometer dataset","","1","","13","","","","9-11 Dec. 2015","","IEEE","IEEE Conferences"
"Providing efficient, scalable and privacy preserved verification mechanism in remote attestation","T. A. Syed; S. Jan; S. Musa; J. Ali","Universiti Kuala Lumpur, Malaysian Institute of Information Technology, Malaysia","2016 International Conference on Information and Communication Technology (ICICTM)","20170403","2016","","","236","245","Numerous applications are running in a distributed environment in today's large networked world. Corporations really need a mechanism to monitor their own application(s) running on remote devices. One such mechanism by Trusted Computing Group (TCG) called remote attestation that can monitor and verify trustworthiness of remote applications. In this regard, many solutions have been provided on how to monitor remote applications. However, It becomes quite challenging task, when applications are running on millions of devices and it becomes necessary for the corporates to verify all of the applications. In this paper we have provided an efficient, scalable and privacy preserved mechanism to tackle the scalability of all these kinds of verifications. Machine learning algorithms are incorporated as Hadoop/MapReduce functions on the public cloud. The rest of low CPU intensive and privacy preserved verifications are performed on the private cloud.","","Electronic:978-1-5090-0412-6; POD:978-1-5090-0413-3","10.1109/ICICTM.2016.7890807","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890807","Big Data;Hadoop;High Performance Computing;MapReduce;Remote Attestation;TPM;Trusted Computing","Big Data;Cloud computing;Hardware;Monitoring;Organizations;Privacy;Security","cloud computing;data privacy;learning (artificial intelligence);parallel processing;trusted computing","Hadoop;MapReduce;TCG;distributed environment;machine learning algorithms;privacy preserved verification mechanism;private cloud;public cloud;remote applications;remote attestation;trusted computing group","","","","","","","","16-17 May 2016","","IEEE","IEEE Conferences"
"Using Deep Learning to Predict and Optimize Hadoop Data Analytic Service in a Cloud Platform","C. C. Chen; Y. T. Hasio; C. Y. Lin; S. Lu; H. T. Lu; J. Chou","","2017 IEEE 15th Intl Conf on Dependable, Autonomic and Secure Computing, 15th Intl Conf on Pervasive Intelligence and Computing, 3rd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)","20180402","2017","","","909","916","Hadoop is a popular computing framework to deliver timely and cost-effective data processing on a large cluster of commodity machines. It relieves the burden of the programmers dealing with distributed programming, and an ecosystem of Big Data solutions have developed around it. However, Hadoop job execution time can be greatly depending on the its runtime configurations and resource selections. Hence, optimizing Hadoop execution still requires a substantial amount of expertise and experiences. To address this challenge, this paper aims to develop a learning-based technique to predict Hadoop job time based on historical execution data, and built a system to optimize its performance in a shared resource cloud environment. While deep learning has shown successes in many application domains, little attention has been paid to apply such technique in job time prediction. In this work, we conducted extensive experimental studies to compare our deep learning prediction method with three other state-of-art regression-based prediction methods in a cloud platform built by OpenStack. The results showed that our prediction method out-performed traditional approaches in most test cases, and improved job performance and cost significantly.","","Electronic:978-1-5386-1956-8; POD:978-1-5386-1957-5; USB:978-1-5386-1955-1","10.1109/DASC-PICom-DataCom-CyberSciTec.2017.153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8328497","Big Data;Deep learning;MapReduce;Optimization;Prediction","Big Data;Cloud computing;Machine learning;Mathematical model;Measurement;Task analysis","Big Data;cloud computing;data handling;learning (artificial intelligence);parallel processing;regression analysis","Big Data solutions;Hadoop execution;Hadoop job execution time;Hadoop job time prediction;cloud platform;commodity machines;deep learning prediction method;distributed programming;historical execution data;job performance;job time prediction;optimize Hadoop Data analytic service;resource selections;runtime configurations;shared resource cloud environment;state-of-art regression-based prediction methods;timely cost-effective data processing","","","","","","","","6-10 Nov. 2017","","IEEE","IEEE Conferences"
"Towards parameter-less support vector machines","J. Nalepa; K. Siminski; M. Kawulok","Institute of Informatics, Silesian University of Technology, Gliwice, Poland","2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)","20160609","2015","","","211","215","Support vector machines (SVMs) are a widely-used machine learning technique, but they suffer from a significant drawback of high time and memory training complexity, which should be endured especially in big data problems. SVMs incorporate kernel functions - it involves selecting the kernel and induces an additional computational effort. In this paper, we address these issues and propose an SVM framework that automatically determines the kernel and selects data to train SVMs. It embodies the neuro-fuzzy system for creating the kernel along with the memetic algorithm to select training samples. Extensive experiments indicate that our approach enables obtaining high classification scores.","","Electronic:978-1-4799-6100-9; POD:978-1-4799-6101-6; USB:978-1-4799-6099-6","10.1109/ACPR.2015.7486496","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486496","","Clustering algorithms;Kernel;Memetics;Optimization;Sociology;Support vector machines;Training","Big Data;fuzzy neural nets;learning (artificial intelligence);pattern classification;support vector machines","Big Data;SVM;kernel function;machine learning technique;memetic algorithm;neuro-fuzzy system;parameter-less support vector machine","","2","","15","","","","3-6 Nov. 2015","","IEEE","IEEE Conferences"
"Using Iterative MapReduce for Parallel Virtual Screening","L. Ahmed; A. Edlund; E. Laure; O. Spjuth","Dept. of HPCViz, R. Inst. of Technol., Stockholm, Sweden","2013 IEEE 5th International Conference on Cloud Computing Technology and Science","20140306","2013","2","","27","32","Virtual Screening is a technique in chemo informatics used for Drug discovery by searching large libraries of molecule structures. Virtual Screening often uses SVM, a supervised machine learning technique used for regression and classification analysis. Virtual screening using SVM not only involves huge datasets, but it is also compute expensive with a complexity that can grow at least up to O(n2). SVM based applications most commonly use MPI, which becomes complex and impractical with large datasets. As an alternative to MPI, MapReduce, and its different implementations, have been successfully used on commodity clusters for analysis of data for problems with very large datasets. Due to the large libraries of molecule structures in virtual screening, it becomes a good candidate for MapReduce. In this paper we present a MapReduce implementation of SVM based virtual screening, using Spark, an iterative MapReduce programming model. We show that our implementation has a good scaling behaviour and opens up the possibility of using huge public cloud infrastructures efficiently for virtual screening.","","Electronic:978-0-7695-5095-4; POD:978-1-4799-1548-4","10.1109/CloudCom.2013.99","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6735391","Big Data;Chemoinformatics;MapReduce;Parallel SVM;Spark","Cloud computing;Fault tolerance;Fault tolerant systems;Predictive models;Sparks;Support vector machines;Training","Big Data;chemistry computing;computational complexity;data analysis;iterative methods;learning (artificial intelligence);message passing;support vector machines;virtual reality","MPI;SVM;Spark;chemo informatics;classification analysis;commodity clusters;data analysis;drug discovery;iterative MapReduce programming model;molecule structures;parallel virtual screening;public cloud infrastructure;regression analysis;scaling behaviour;supervised machine learning technique","","1","","33","","","","2-5 Dec. 2013","","IEEE","IEEE Conferences"
"MatrixMap: Programming Abstraction and Implementation of Matrix Computation for Big Data Applications","Y. Huangfu; J. Cao; H. Lu; G. Liang","Dept. of Comput., Hong Kong Polytech. Univ., Hong Kong, China","2015 IEEE 21st International Conference on Parallel and Distributed Systems (ICPADS)","20160118","2015","","","19","28","The computation core of many big data applications can be expressed as general matrix computations, including linear algebra operations and irregular matrix operations. However, existing parallel programming systems such as Spark do not have programming abstraction and efficient implementation for general matrix computations. In this paper, we present MatrixMap, a unified and efficient data-parallel system for general matrix computations. MatrixMap provides powerful yet simple abstraction, consisting of a distributed data structure called bulk key matrix and a computation interface defined by matrix patterns. Users can easily load data into bulk key matrices and program algorithms into parallel matrix patterns. MatrixMap outperforms current state-of-the-art systems by employing three key techniques: matrix patterns with lambda functions for irregular and linear algebra matrix operations, asynchronous computation pipeline with optimized data shuffling strategies for specific matrix patterns and in-memory data structure reusing data in iterations. Moreover, it can automatically handle the parallelization and distribute execution of programs on a large cluster. The experiment results show that MatrixMap is 12 times faster than Spark.","","Electronic:978-0-7695-5785-4; POD:978-1-4673-8669-2","10.1109/ICPADS.2015.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7384274","Big Data;Graph Processing;Machine Learning;Matrix Computation;Parallel Programming","Computational modeling;Data structures;Linear algebra;Machine learning algorithms;Programming;Sparks;Sparse matrices","Big Data;data structures;matrix algebra;parallel processing","Big Data applications;MatrixMap;asynchronous computation pipeline;bulk key matrix;data reuse;data shuffling strategies;data-parallel system;distributed data structure;in-memory data structure;lambda functions;linear algebra matrix operations;parallel matrix patterns;programming abstraction","","2","","24","","","","14-17 Dec. 2015","","IEEE","IEEE Conferences"
"A Multifaceted Approach to Bitcoin Fraud Detection: Global and Local Outliers","P. M. Monamo; V. Marivate; B. Twala","Council for Sci. & Ind. Res., Pretoria, South Africa","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","188","194","In the Bitcoin network, lack of class labels tend to cause obscurities in anomalous financial behaviour interpretation. To understand fraud in the latest development of the financial sector, a multifaceted approach is proposed. In this paper, Bitcoin fraud is described from both global and local perspectives using trimmed k-means and kd-trees. The two spheres are investigated further through random forests, maximum likelihood-based and boosted binary regression models. Although both angles show good performance, global outlier perspective outperforms the local viewpoint with exception of random forest that exhibits nearby perfect results from both dimensions. This signifies that features extracted for this study describe the network fairly.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838143","anomaly;data mining;kd-trees;outlier;random forest;regression","Clustering algorithms;Data mining;Electronic mail;Feature extraction;Machine learning algorithms;Online banking;Security","cryptography;feature extraction;financial data processing;regression analysis","Bitcoin fraud detection;Bitcoin network;anomalous financial behaviour interpretation;boosted binary regression models;feature extraction;maximum likelihood-based models","","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conferences"
"Track geometry big data analysis: A machine learning approach","E. N. Martey; L. Ahmed; N. Attoh-Okine","Department of Civil and Environmental Engineering, University of Delaware, Newark, DE, USA","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","3800","3809","Track geometry has a considerable effect on rail travel comfort and safety and deteriorates with age and tonnage. In order to maintain the track geometry quality, maintenance activities such as tamping, stone blowing and ballast undercutting are usually employed. However, these activities are ineffective if the underlying cause of track deformation such as subgrade failure is not addressed. Geosyn-thetics such as geocells and geogrids can be placed in the subballast which strengthens the layer, lowers the stresses on the weak subgrade and invariably enhances track geometry quality. Machine learning techniques are becoming increasingly imperative in processing and analyzing of large volumes of track geometry data which exhibit the classical attributes of big data. Several unsupervised and supervised learning techniques were used to analyze the effect of geocell installation on track geometry quality. Cluster analysis was used to group the track geometry data with major clusters found to differ by surface and alignment features. Principal component analysis was employed as an effective dimension reduction tool to simplify the track geometry data based on the proportion of variance explained. Supervised learning techniques such as multiple linear regression, decision tree regression, random forest regression and support vector regression were subsequently used to estimate and predict the effect of geocell installation on the track geometry quality. Random forest regression was found to the best performing model for both the original and dimensionally-reduced data.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258381","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258381","machine learning;railroad;supervised learning;track geometry;unsupervised learning","Decision trees;Geometry;Maintenance engineering;Rails;Safety;Supervised learning;Support vector machines","Big Data;data analysis;decision trees;geometry;geotechnical engineering;maintenance engineering;mechanical engineering computing;pattern clustering;principal component analysis;rails;railway engineering;railway safety;regression analysis;roads;support vector machines;unsupervised learning","alignment features;cluster analysis;dimension reduction tool;dimensionally-reduced data;geocell installation;geosyn-thetics;machine learning approach;maintenance activities;principal component analysis;rail travel comfort;rail travel safety;random forest regression;subballast;supervised learning techniques;surface features;track deformation;track geometry big data analysis;track geometry quality;unsupervised learning techniques;weak subgrade","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Machine Learning Methods for Solving Complex Ranking and Sorting Issues in Human Resourcing","A. Kumar; A. Pandey; S. Kaushik","Big Data & Analytics Dept., HCL Technol. Noida, Noida, India","2017 IEEE 7th International Advance Computing Conference (IACC)","20170713","2017","","","43","47","Every organization doesn't necessary to have the common point of view of a particular resume while considering for a job description (JD). Keeping the same role in place, while some stress on technical skills, the other give importance to professional experience and domain expertise. Understanding these hiring patterns are becoming important in today's head hunting. The traditional job search engines offers resumes which matches to the input keywords. As the search outcomes from these search engines grows, the problem in selecting the best profile surges. The role of Human Resource (HR) staff becomes more important in understanding these hiring patterns and suggesting the suitable profiles. HR staff proposes these profiles which are ranked manually. The proposed method is to understand the intelligence behind the hiring pattern and apply the machine learning to accommodate the identified intelligence. The proposed method offers the ranking system according to the hiring patterns. Highly trained models along with the traditional search method, predicts the ranking and sorting of resumes with high accuracy and simplifies the job of human resourcing efficiently.","","Electronic:978-1-5090-1560-3; POD:978-1-5090-1561-0","10.1109/IACC.2017.0024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7976758","Cosine Similarity;K-means;Machine learning;association rule mining (ARM);hiring patterns;human resourcing;latent semantic analysis (LSA);supervised clusters;support vector machines (SVM);unsupervised clusters","Communication system security;Companies;Databases;Qualifications;Resumes;Support vector machines","business data processing;human resource management;learning (artificial intelligence);organisational aspects;search engines;sorting","HR staff;domain expertise;head hunting;hiring patterns;human resource staff;human resourcing;job description;job search engines;machine learning;organization;professional experience;ranking system;resumes ranking;resumes sorting;technical skills","","","","","","","","5-7 Jan. 2017","","IEEE","IEEE Conferences"
"APP-SON: Application characteristics-driven SON to optimize 4G/5G network performance and quality of experience","Y. Ouyang; Z. Li; L. Su; W. Lu; Z. Lin","Verizon Wireless, Basking Ridge, NJ, USA","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","1514","1523","Self-Organizing Networks (SON) is an automation technology making the planning, deployment, operation, optimization, and healing of networks simpler and faster. Legacy SON is targeted at network automation and network optimization through certain optimization rules and policies which are globally applied in networks. However, scalable and targeted optimization is not considered yet in 3GPP. Furthermore, SON is driven by performance optimization rather than ultimately improving user Quality of Experience (QoE). The impact of application characteristics on network performance and further on QoE are also not considered in 3GPP SON. This paper presents an application characteristics-driven SON system (APP-SON) to optimize 4G/5G network performance and user Quality of Experience. APP-SON leverages a scalable big data platform for targeted optimization through profiling cell application characteristics in an incremental manner in temporal space. A Hungarian Algorithm Assisted Clustering (HAAC) algorithm and a deep learning-assisted regression algorithm are developed to profile the cell application characteristics and find the targeted KPIs to be optimized for each cell. A similarity-based, parametertuning algorithm is developed to tune the corresponding engineering parameters to optimize the targeted KPIs which further improve QoE. Experimental results demonstrate that the APP-SON system can precisely profile cell traffic and application characteristics to find the targeted KPIs for optimization for each cell. APP-SON can also automatically tune the corresponding engineering parameters to improve the corresponding KPIs, ultimately improving QoE. APP-SON has been successfully implemented in production and applied in a tier-1 operator's 4G network and as a universal SON solution it will be smoothly transitioned and applied in 5G networks for this operator.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258085","Big Data;Clustering;Deep Learning;Ensemble Learning;Machine Learning;SON;Self-Optimization","3GPP;Clustering algorithms;Computer architecture;Microprocessors;Optimization;Wireless communication","3G mobile communication;4G mobile communication;5G mobile communication;Big Data;Long Term Evolution;cellular radio;mobile computing;optimisation;quality of experience;regression analysis","3GPP SON;4G/5G network performance;APP-SON system;Hungarian algorithm assisted clustering algorithm;QoE;SON;legacy SON;network automation;network optimization;performance optimization;profiling cell application characteristics;self-organizing networks;targeted KPI;targeted optimization;universal SON solution","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Deep learning with consumer preferences for recommender system","T. Gao; X. Li; Y. Chai; Y. Tang","Department of Automation, Tsinghua University, Beijing 100084, China","2016 IEEE International Conference on Information and Automation (ICIA)","20170202","2016","","","1556","1561","With the arrival of big data era and the fast development of E-commerce, recommender systems(RSs) are used in more and more application domains to assist customers in the search for their favorite products. Collaborative filtering(CF) is one of the most successful and widely used recommendation approaches. Poor recommender quality is a major challenge in traditional CF. And one reason causing this circumstance is the sparsity of data. In this paper, we propose to a deep learning model to predict the values of null ratings. Moreover, we investigate personal recommendation based on customer preferences and search the neighbors through the customer preferences. Accordingly, it has effectively solved recommendation quality problems of traditional CF algorithm under the condition of data sparse and poor result quality.","","Electronic:978-1-5090-4102-2; POD:978-1-5090-4103-9; USB:978-1-5090-4101-5","10.1109/ICInfA.2016.7832066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832066","Collaborative filtering Deep learning;Consumer Preferences;Recommender systems","Clustering algorithms;Collaboration;History;Machine learning;Machine learning algorithms;Motion pictures;Recommender systems","Big Data;collaborative filtering;consumer behaviour;electronic commerce;learning (artificial intelligence);recommender systems","CF;E-commerce;big data era;collaborative filtering;consumer preferences;data sparse;deep learning model;null ratings;personal recommendation;recommendation quality problems;recommender quality;recommender system","","","","","","","","1-3 Aug. 2016","","IEEE","IEEE Conferences"
"Multi-level clustering for extracting process-related information from email logs","D. Jlailaty; D. Grigori; K. Belhajjame","Paris Dauphine University, France","2017 11th International Conference on Research Challenges in Information Science (RCIS)","20170626","2017","","","455","456","Emails represent a valuable source of information that can be harvested for understanding undocumented business processes of institutions. Towards this aim, a few researchers investigated the problem of extracting process oriented information from email logs to make benefit of the many available process mining techniques. In this work, we go further in this direction, by proposing a new method for mining process models from email logs that leverages unsupervised machine learning techniques. Moreover, our method allows to label emails with activity names, that can be used for activity recognition in new incoming emails. A use case illustrates the usefulness of the proposed solution.","","Electronic:978-1-5090-5476-3; POD:978-1-5090-5477-0","10.1109/RCIS.2017.7956583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7956583","Clustering;Email Analysis;Process Information;Process Mining;Process Model","Activity recognition;Analytical models;Business;Data mining;Electronic mail;Labeling;Tools","business data processing;data mining;electronic mail;pattern clustering;unsupervised learning","activity recognition;email logs;multilevel clustering;process-related information;undocumented business processes;unsupervised machine learning","","","","","","","","10-12 May 2017","","IEEE","IEEE Conferences"
"A Parallel DBSCAN Algorithm Based on Spark","G. Luo; X. Luo; T. F. Gooch; L. Tian; K. Qin","Sch. of Comput. Sci. & Eng., Univ. of Electron. Sci. & Technol. of China, Chengdu, China","2016 IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking (SocialCom), Sustainable Computing and Communications (SustainCom) (BDCloud-SocialCom-SustainCom)","20161031","2016","","","548","553","With the explosive growth of data, we have entered the era of big data. In order to sift through masses of information, many data mining algorithms using parallelization are being implemented. Cluster analysis occupies a pivotal position in data mining, and the DBSCAN algorithm is one of the most widely used algorithms for clustering. However, when the existing parallel DBSCAN algorithms create data partitions, the original database is usually divided into several disjoint partitions, with the increase in data dimension, the splitting and consolidation of high-dimensional space will consume a lot of time. To solve the problem, this paper proposes a parallel DBSCAN algorithm (S_DBSCAN) based on Spark, which can quickly realize the partition of the original data and the combination of the clustering results. It is divided into the following steps: 1) partitioning the raw data based on a random sample, 2) computing local DBSCAN algorithms in parallel, 3) merging the data partitions based on the centroid. Compared with the traditional DBSCAN algorithm, the experimental result shows the proposed S_DBSCAN algorithm provides better operating efficiency and scalability.","","","10.1109/BDCloud-SocialCom-SustainCom.2016.85","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7723739","DBSCAN Algorithm;Data Mining;Data Partition;Parallel Algorithms;Spark","Algorithm design and analysis;Clustering algorithms;Data mining;Machine learning algorithms;Merging;Partitioning algorithms;Sparks","Big Data;data mining;parallel algorithms;pattern clustering;public domain software","Big Data;S_DBSCAN;Spark;cluster analysis;data dimension;data mining algorithms;data partitions;density-based spatial clustering of applications with noise;disjoint partitions;high-dimensional space consolidation;high-dimensional space splitting;parallel DBSCAN algorithm","","1","","","","","","8-10 Oct. 2016","","IEEE","IEEE Conferences"
"Generating Medical Hypotheses Based on Evolutionary Medical Concepts","G. Xun; K. Jha; V. Gopalakrishnan; Y. Li; A. Zhang","Dept. of Comput. Sci. & Eng., SUNY at Buffalo, Buffalo, NY, USA","2017 IEEE International Conference on Data Mining (ICDM)","20171218","2017","","","535","544","Literature based discovery (LBD) is a task that aims to uncover hidden associations between non-interacting scientific concepts by rationally connecting independent nuggets of information. Broadly, prior approaches to LBD include use of: a) distributional statistics and explicit representation, b) graph-theoretic measures, and c) supervised machine learning methods to find associations. However, purely distributional approaches may not necessarily entail semantically meaningful association and graph-theoretic approaches suffer from scalability issues. While supervised machine learning based approaches have the potential to elucidate associations, the training data required is too expensive to generate. In this paper we propose a novel dynamic Medical Subject Heading (MeSH) embedding model which is able to model the evolutionary behavior of medical concepts to uncover latent associations between them. The proposed model allows us to learn the evolutionary trajectories of MeSH embeddings and detect informative terms. Hence, based on the dynamic MeSH embeddings, meaningful medical hypotheses can be efficiently generated. To evaluate the efficacy of the proposed model, we perform both qualitative and quantitative evaluation. The results demonstrate that leveraging the evolutionary features of MeSH concepts is an effective way for predicting novel associations.","","Electronic:978-1-5386-3835-4; POD:978-1-5386-2449-4; USB:978-1-5386-3834-7","10.1109/ICDM.2017.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8215526","","Diseases;Fish;Manuals;Oils;Semantics;Unified modeling language","data mining;graph theory;learning (artificial intelligence);medical information systems;statistical distributions","LBD;MeSH concepts;distributional statistics;dynamic MeSH embeddings;dynamic medical subject heading embedding model;evolutionary behavior;evolutionary features;evolutionary medical concepts;evolutionary trajectories;explicit representation;graph-theoretic approaches;informative terms detection;latent associations;literature based discovery;medical hypotheses generation;semantically meaningful association;supervised machine learning methods","","","","","","","","18-21 Nov. 2017","","IEEE","IEEE Conferences"
"Unified Programming Model and Software Framework for Big Data Machine Learning and Data Analytics","R. Gu; Y. Tang; Q. Dong; Z. Wang; Z. Liu; S. Wang; C. Yuan; Y. Huang","Collaborative Innovation Center of Novel Software Technol. & Industrialization, Nanjing Univ., Nanjing, China","2015 IEEE 39th Annual Computer Software and Applications Conference","20150924","2015","3","","562","567","In a new era of Big Data, the rapid growth of the applications, such as social media and web-search, requires efficient and scalable machine learning and statistical analytical algorithms. However, there lacks easy-to-use and efficient software frameworks or systems that can support fast development of such big data analytical algorithms. To solve these problems, we propose Octopus, an easy-to-use and efficient analytical system for big data. Octopus allows data analysts conduct complex data analytics for big data with traditional programming languages and methods in an easy and efficient way. To achieve the goal of ease-to-use, we propose a matrix-based unified programming model, which is the core of many data-intensive statistical applications such as numerical analysis and data mining. Further, in order to improve the performance, the Octopus software framework adopts various distributed computing platforms, including Hadoop MapReduce, Spark and MPI. On these computing platforms, we design several parallel matrix computation algorithms, which are suitable for various scenarios. Finally, the features of Octopus are encapsulated into a library with matrix-based APIs and exposed to users as an R package. R is a widely-used statistical programming language and supports diversified data analysis tasks through extension packages. Experimental results show that Octopus achieves efficient performance and near linear scalability.","","Electronic:978-1-4673-6564-2; POD:978-1-4673-6565-9","10.1109/COMPSAC.2015.275","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273424","big data analysis;ease-to-use;matrix computation;parallel algorithm","Big data;Distributed databases;Libraries;Machine learning algorithms;Programming;Scalability;Sparks","Big Data;application program interfaces;data analysis;learning (artificial intelligence);matrix algebra;message passing;parallel processing;statistical analysis","Big Data machine learning;Hadoop MapReduce;MPI;Octopus software framework;R package;Spark;Web-search;data analytics;data mining;data-intensive statistical applications;distributed computing platforms;matrix-based APIs;matrix-based unified programming model;numerical analysis;parallel matrix computation algorithms;social media;statistical analytical algorithms;statistical programming language","","1","","11","","","","1-5 July 2015","","IEEE","IEEE Conferences"
"Research on public opinion based on Big Data","S. Shang; M. Shi; W. Shang; Z. Hong","School of Computer Science, Communication University of China, Beijing, China","2015 IEEE/ACIS 14th International Conference on Computer and Information Science (ICIS)","20150727","2015","","","559","562","Public opinion is the people's response for social phenomena, issues, hot topics, attitudes, emotions, and so on. It reflects the focus problems of the current time of the society. By analyzing the public opinion, we can infer what will happen in the next time, and give better decision support for governments and businesses. Big Data technology is becoming a powerful data analyzing tools for massive data in recent years. Hadoop is an open source massive data processing platform based on Big Data. Mahout is a data mining algorithms' set based on Hadoop, which is designed for processing large-scale and complex data. In most instances, the public opinion information contains many text messages. For many traditional text mining algorithms, it is almost impossible to handle high dimensional data concerns large-volume and complex data sets. Hence, this paper uses Mahout text mining algorithms to process public opinion information.","","Electronic:978-1-4799-8679-8; POD:978-1-4799-8680-4","10.1109/ICIS.2015.7166655","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7166655","Big Data;Hadoop;Mahout;data mining;public opinion","Algorithm design and analysis;Big data;Classification algorithms;Clustering algorithms;Data mining;Internet;Machine learning algorithms","Big Data;Internet;data analysis;data mining;text analysis","Big Data technology;Hadoop;Mahout text mining algorithms;data analyzing tools;data mining algorithm;open source massive data processing platform;public opinion","","1","","12","","","","June 28 2015-July 1 2015","","IEEE","IEEE Conferences"
"Towards an effective and efficient malware detection system","C. T. Dan Lo; O. Pablo; C. M. Carlos","Department of Computer Science, Kennesaw State University, Kennesaw, Georgia, USA","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3648","3655","The ubiquitous advance of technology used on the Internet, computers, smart phones and tablets has been conducive to the creation and proliferation of cyber threats resulting in attacks that have grown exponentially. Consequently, anti-virus companies and researchers have developed new approaches for dealing with discovering and classifying malware. Among these, machine learning and big data technologies have been used for feature extraction, detection, and clustering of cyber threats. In this paper a dataset of malware and clean files (goodware) was created and analyzed from the static and dynamic features provided by the online framework VirusTotal. The purpose is to select the smallest number of features that keep classification accuracy as high as possible in order to decrease the use of resources for monitoring as well as extracting features and the time for detection. In this research, it was found that “9” features are enough to distinguish malware from “goodware” files with an accuracy of 99.60%. Selecting the most representative features for malware detection relies on the possibility of creating an embedded program that monitors the processes executed by the operating system (OS) and looks for the characteristics that match malware behavior. In addition, classification algorithms such as Random Forest (RF), Support Vector Machine (SVM) and Neural Networks (NN) were used in a novel combination that not only showed an increase in accuracy, but also in the training speed from hours to just minutes. Finally, the trained model (which was trained with a dataset of malware samples seen before September 2015) was tested on a new dataset of malware samples seen by first time between October 2015 and June 2016 and showed that the model is still effective for detection of unseen malware files.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7841031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841031","Big Data Analytics;Feature selection;Machine Learning;Malware detection;Neural Networks;Random Forest;Support Vector Machine;improving accuracy","Big data;Conferences;DH-HEMTs;High definition video","feature extraction;invasive software;learning (artificial intelligence);operating systems (computers);pattern classification;pattern clustering;support vector machines;system monitoring","Internet;SVM;VirusTotal;big data technology;classification algorithm;computers;cyber threat clustering;cyber threat detection;cyber threat proliferation;dynamic feature;embedded program;feature extraction;goodware files;machine learning;malware behavior;malware classification;malware detection system;malware discovery;malware files;neural networks;operating system;process execution monitoring;random forest;smart phones;static feature;support vector machine;tablets","","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conferences"
"Exploring new privacy approaches in a scalable classification framework","M. Saravanan; A. M. Thoufeeq; S. Akshaya; V. L. Jayasre Manchari","Ericsson Research India, Ericsson India Global Services Pvt. Ltd, Chennai, India","2014 International Conference on Data Science and Advanced Analytics (DSAA)","20150312","2014","","","209","215","Recent advancements in Information and Communication Technologies (ICT) enable many organizations to collect, store and control massive amount of various types of details of individuals from their regular transactions (credit card, mobile phone, smart meter etc.). While using these wealth of information for Personalized Recommendations provides enormous opportunities for applying data mining (or machine learning) tasks, there is a need to address the challenge of preserving individuals privacy during the time of running predictive analytics on Big Data. Privacy Preserving Data Mining (PPDM) on these applications is particularly challenging, because it involves and process large volume of complex, heterogeneous, and dynamic details of individuals. Ensuring that privacy-protected data remains useful in intended applications, such as building accurate data mining models or enabling complex analytic tasks, is essential. Differential Privacy has been tried with few of the PPDM methods and is immune to attacks with auxiliary information. In this paper, we propose a distributed implementation based on Map Reduce computing model for C4.5 Decision Tree algorithm and run extensive experiments on three different datasets using Hadoop Cluster. The novelty of this work is to experiment two different privacy methods: First method is to use perturbed data on decision tree algorithm for prediction in privacy-preserving data sharing and the second method is based on applying raw data to the privacy-preserving decision tree algorithm for private data analysis. In addition to this, we propose the combination of the methods as hybrid technique to maintain accuracy (Utility) and privacy in an acceptable level. The proposed privacy approaches has two potential benefits in the context of data mining tasks: it allows the service providers to outsource data mining tasks without exposing the raw data, and it allows data providers to share data access to third parties while limiting privacy - isks.","","Electronic:978-1-4799-6991-3; POD:978-1-4799-6982-1","10.1109/DSAA.2014.7058075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058075","Hybrid data privacy;Map Reduce Framework;Privacy Approaches;Privacy Preserving data Mining;Scalability","Big data;Classification algorithms;Data privacy;Decision trees;Noise;Privacy;Scalability","data mining;data privacy;decision trees;learning (artificial intelligence)","C4.5 decision tree algorithm;Hadoop Cluster;ICT;big data;differential privacy;information and communication technologies;machine learning;map reduce computing model;personalized recommendation;privacy preserving data mining;private data analysis;scalable classification","","0","","21","","","","Oct. 30 2014-Nov. 1 2014","","IEEE","IEEE Conferences"
"User behavior analysis and commodity recommendation for point-earning apps","Y. C. Chen; C. C. Yang; Y. J. Liau; C. H. Chang; P. L. Chen; P. C. Yang; T. Ku","Department of Computer Science and Information Engineering, National Central University, Taoyuan, Taiwan","2016 Conference on Technologies and Applications of Artificial Intelligence (TAAI)","20170320","2016","","","170","177","In recent years, due to the rapid development of e-commerce, personalized recommendation systems have prevailed in product marketing. However, recommendation systems rely heavily on big data, creating a difficult situation for businesses at initial stages of development. We design several methods - including a traditional classifier, heuristic scoring, and machine learning - to build a recommendation system and integrate content-based collaborative filtering for a hybrid recommendation system using Co-Clustering with Augmented Matrices (CCAM). The source, which include users' persona from action taken in the app & Facebook as well as product information derived from the web. For this particular app, more than 50% users have clicks less than 10 times in 1.5 year leading to insufficient data. Thus, we face the challenge of a cold-start problem in analyzing user information. In order to obtain sufficient purchasing records, we analyzed frequent users and used web crawlers to enhance our item-based data, resulting in F-scores from 0.756 to 0.802. Heuristic scoring greatly enhances the efficiency of our recommendation system.","","Electronic:978-1-5090-5732-0; POD:978-1-5090-5733-7","10.1109/TAAI.2016.7880109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7880109","co-clustering with augmented matrices;matrix factorization;time sequential patterns;user behavior models","Business;Collaboration;Crawlers;Facebook;Filtering;History;Predictive models","collaborative filtering;recommender systems","Big Data;CCAM;classifier method;co-clustering with augmented matrices;commodity recommendation;content-based collaborative filtering;e-commerce;electronic commerce;heuristic scoring method;machine learning method;personalized recommendation systems;point-earning applications;user behavior analysis;user information analysis","","","","","","","","25-27 Nov. 2016","","IEEE","IEEE Conferences"
"Boosting with Adaptive Sampling for Multi-class Classification","J. Chen","Sch. of Electr. Eng. & Comput. Sci., Louisiana State Univ., Baton Rouge, LA, USA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","667","672","Achieving scalability of learning algorithms has become an increasingly critical issue in knowledge discovery from ""big data"". Sampling techniques can be exploited as one of the approaches to address the issue of scalability. We present in this paper a method to employ a newly developed sampling-based ensemble learning method by boosting for multi-class (non-binary) classification. This current research extends our previous work on multi-class classification with sampling-based ensemble learning method, in which the base classifiers are the most simplistic, such as decision stumps. Here we generalize the sampling-based method to handle more complex base classifiers such as decision trees in building an ensemble, which require sampling a set of instances before building a base classifier. We present experimental results using bench-mark data sets from the UC-Irvine ML data repository that confirm the efficiency and competitive prediction accuracy of the proposed adaptive boosting method for the multi-class classification task.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.85","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424395","Adaptive Sampling;Boosting;Decision Trees;Multi-class Classification;Scalable Learning","Boosting;Computer science;Decision trees;Estimation;Knowledge discovery;Sampling methods","learning (artificial intelligence);pattern classification;sampling methods","UC-Irvine ML data repository;adaptive boosting method;adaptive sampling;decision stumps;decision trees;ensemble learning method;multiclass classification","","","","19","","","","9-11 Dec. 2015","","IEEE","IEEE Conferences"
"Finding bottlenecks: Predicting student attrition with unsupervised classifier","S. Sajjadi; B. Shapiro; C. McKinlay; A. Sarkisyan; C. Shubin; E. Osoba","California State University, Northridge","2017 Intelligent Systems Conference (IntelliSys)","20180326","2017","","","1166","1172","With pressure to increase graduation rates and reduce time to degree in higher education, it is important to identify at-risk students early. Automated early warning systems are therefore highly desirable. In this paper, we use unsupervised clustering techniques to predict the graduation status of declared majors in five departments at California State University Northridge (CSUN), based on a minimal number of lower division courses in each major. In addition, we use the detected clusters to identify hidden bottleneck courses.","","Electronic:978-1-5090-6435-9; POD:978-1-5090-6436-6","10.1109/IntelliSys.2017.8324279","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8324279","K-means;Machine learning;classifier;clustering;educational data mining;unsupervised methods","Data mining;Economics;Education;Electrical engineering;Intelligent systems;Logistics","computer aided instruction;educational administrative data processing;educational courses;educational institutions;further education;pattern clustering","CSUN;California State University Northridge;at-risk students;automated early warning systems;declared majors;detected clusters;graduation rates;graduation status;hidden bottleneck courses;higher education;lower division courses;student attrition;unsupervised classifier;unsupervised clustering techniques","","","","","","","","7-8 Sept. 2017","","IEEE","IEEE Conferences"
"Scalability Analysis and Improvement of Hadoop Virtual Cluster with Cost Consideration","Y. He; X. Jiang; Z. Wu; K. Ye; Z. Chen","Coll. of Comput. Sci., Zhejiang Univ. Hangzhou, Hangzhou, China","2014 IEEE 7th International Conference on Cloud Computing","20141204","2014","","","594","601","With the rapid development of big data and cloud computing, big data analytics as a service in the cloud is becoming increasingly popular. More and more individuals and organizations tend to rent virtual cluster to store and analyze data rather than building their own data centers. However, in virtualization environment, whether scaling out using a cluster with more nodes to process big data is better than scaling up by adding more resources to the original virtual machines (VMs) in cluster is not clear. In this paper, we study the scalability performance issues of hadoop virtual cluster with cost consideration. We first present the design and implementation of VirtualMR platform which can provide users with scalable hadoop virtual cluster services for the MapReduce based big data analytics. Then we run a series of hadoop benchmarks and real parallel machine learning algorithms to evaluate the scalability performance, including scale-up method and scale-out method. Finally, we integrate our platform with resource monitoring module and propose a system tuner. By analyzing the monitored data, we dynamically adjust the parameters of hadoop framework and virtual machine configuration to improve resource utilization and reduce rent cost. Experimental results show that the scale-up method outperforms the scale-out method for CPU-bound applications, and it is opposite for I/O-bound applications. The results also verify the efficiency of system tuner to increase resource utilization and reduce rent cost.","2159-6182;21596182","Electronic:978-1-4799-5063-8; POD:978-1-4799-5064-5","10.1109/CLOUD.2014.85","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6973791","MapReduce;Scalability;big data;cloud computing;rent cost","Benchmark testing;Big data;Monitoring;Parallel processing;Resource management;Scalability;Virtualization","Big Data;cloud computing;learning (artificial intelligence);virtual machines","CPU-bound application;Hadoop virtual cluster;I/O-bound application;MapReduce;VirtualMR platform;big data analytics;cloud computing;cost consideration;parallel machine learning algorithm;scalability analysis;scale-out method;scale-up method;virtual machine","","2","","23","","","","June 27 2014-July 2 2014","","IEEE","IEEE Conferences"
"Big Data Stream Learning with SAMOA","A. Bifet; G. D. F. Morales","HUAWEI Noah's Ark Lab., Hong Kong, China","2014 IEEE International Conference on Data Mining Workshop","20150129","2014","","","1199","1202","Big data is flowing into every area of our life, professional and personal. Big data is defined as datasets whose size is beyond the ability of typical software tools to capture, store, manage and analyze, due to the time and memory complexity. Velocity is one of the main properties of big data. In this demo, we present SAMOA (Scalable Advanced Massive Online Analysis), an open-source platform for mining big data streams. It provides a collection of distributed streaming algorithms for the most common data mining and machine learning tasks such as classification, clustering, and regression, as well as programming abstractions to develop new algorithms. It features a pluggable architecture that allows it to run on several distributed stream processing engines such as Storm, S4, and Samza. SAMOA is written in Java and is available at http://samoa-project.net under the Apache Software License version 2.0.","2375-9232;23759232","Electronic:978-1-4799-4274-9; POD:978-1-4799-4273-2","10.1109/ICDMW.2014.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7022733","Classification;Clustering;Data Streams;Distributed Systems;Machine Learning;Regression;Toolbox","Algorithm design and analysis;Big data;Data mining;Digital signal processing;Engines;Machine learning algorithms;Storms","Big Data;data mining;learning (artificial intelligence);pattern classification;pattern clustering;regression analysis","Apache Software License version 2.0;Java;S4;SAMOA;Samza;Storm;big data stream learning;big data stream mining;classification task;clustering task;distributed stream processing engines;distributed streaming algorithms;machine learning tasks;open-source platform;pluggable architecture;programming abstractions;regression task;scalable advanced massive online analysis","","6","","8","","","","14-14 Dec. 2014","","IEEE","IEEE Conferences"
"User-profile-based analytics for detecting cloud security breaches","T. Tiwari; A. Turk; A. Oprea; K. Olcoz; A. K. Coskun","Electrical and Computer Engineering, Boston University, Boston, MA, USA","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","4529","4535","While the growth of cloud-based technologies has benefited the society tremendously, it has also increased the surface area for cyber attacks. Given that cloud services are prevalent today, it is critical to devise systems that detect intrusions. One form of security breach in the cloud is when cyber-criminals compromise Virtual Machines (VMs) of unwitting users and, then, utilize user resources to run time-consuming, malicious, or illegal applications for their own benefit. This work proposes a method to detect unusual resource usage trends and alert the user and the administrator in real time. We experiment with three categories of methods: simple statistical techniques, unsupervised classification, and regression. So far, our approach successfully detects anomalous resource usage when experimenting with typical trends synthesized from published real-world web server logs and cluster traces. We observe the best results with unsupervised classification, which gives an average F1-score of 0.83 for web server logs and 0.95 for the cluster traces.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258494","Cloud Security;Machine Learning;Virtual Machine Resource Usage","Anomaly detection;Cloud computing;Market research;Measurement;Standards;Support vector machines","cloud computing;file servers;regression analysis;security of data;virtual machines","Virtual Machines;anomalous resource usage;cloud security breaches;cloud services;cyber attacks;cyber-criminals;illegal applications;intrusion detection;real-world web server logs;regression analysis;simple statistical techniques;unsupervised classification","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Identification of flight maneuvers and aircraft types utilizing unsupervised learning with big data","Z. Blanks; A. Sedgwick; B. Bone; A. Mayerchak","United States Air Force Academy, United States of America","2017 Systems and Information Engineering Design Symposium (SIEDS)","20170601","2017","","","180","185","Security professionals are interested in knowing with a high level of confidence the details of each flight beyond what the self-broadcasted data provides. A program run by the Air Force is attempting to better characterize flights. From various data sources, this program attempts to determine aircraft identities. The purpose of this study is to test unsupervised machine learning methods to improve the Air Force's aircraft identification process. These alternative methods cluster unlabeled aircraft flight data from which we identify different aircraft types and maneuvers. The alternate methods include k-means and k-medoids. To improve the clustering, we first perform feature engineering. Some of the features built include acceleration rates, G forces, specific excess power, flight path angle, flight efficiency, and winding number. We then cluster the data using the above features and determine the ideal number of clusters via the average silhouette width. The initial clusters identify discrete flight maneuvers, such as takeoff and landing, and from these initial clusters we utilize sub-clustering to identify aircraft. To validate our aircraft identification process, we implement web scraping to develop a labeled dataset to compare the distribution of aircraft within the sub-cluster versus the initial cluster. The final model uses k-means, and 12 of the 14 sub-clusters generated by it have statistically different distributions of aircraft at α = 0.01 from the initial clusters. This indicates that we can better identify aircraft type given in which sub-cluster a data point resides. With some modifications, the method could be used by the Air Force to augment the current identification process.","","Electronic:978-1-5386-1848-6; POD:978-1-5386-1849-3","10.1109/SIEDS.2017.7937712","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7937712","Aircraft Classification;Clustering;Unsupervised Machine Learning","Acceleration;Aircraft;Aircraft propulsion;Atmospheric modeling;Military aircraft;Radar;Windings","Big Data;aircraft;aircraft control;unsupervised learning","Big Data;aircraft identification process;feature engineering;flight maneuver identification;k-means method;k-medoids method;unsupervised machine learning methods","","","","","","","","28-28 April 2017","","IEEE","IEEE Conferences"
"Unsupervised Learning and Image Classification in High Performance Computing Cluster","I. Itauma; M. S. Aslan; F. Villanustre; X. w. Chen","Comput. Sci. Dept., Wayne State Univ., Detroit, MI, USA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","576","581","Feature learning and object classification in machine learning have become very active research areas in recent decades. Identifying good features has various benefits for object classification in respect to reducing the computational cost and increasing the classification accuracy. We propose using a multimodal learning and object identification framework with an alternative platform, called High Performance Computing Cluster (HPCC Systems®), to speed up the optimization stages and to handle data of any dimension. Our framework first learns representative bases (or centroids) over unlabeled data for each model through the K-means unsupervised learning method. Then, to extract the desired features from the labeled data, the correlation between the labeled data and representative bases is calculated. These labeled features are fused to represent the identity and then fed to the classifiers to make the final recognition. In addition, many research studies have focused on improving optimization methods and the use of Graphics Processing Units (GPUs) to improve the training time for machine learning algorithms. This study is aimed at exploring feature learning and object classification ideas in HPCC Systems platform. HPCC Systems is a Big Data processing and massively parallel processing (MPP) computing platform used for solving Big Data problems. Algorithms are implemented in HPCC Systems® with a language called Enterprise Control Language (ECL) which is a declarative, data-centric programming language. It is a powerful, high-level, parallel programming language ideal for Big Data intensive applications. We evaluate our proposed framework in this new platform on various databases such as the CALTECH-101, AR databases, and a subset of wild PubFig83 data that we add multimedia content. For instance, we are able to improve on the classification accuracy result of [3] from 74.3% to 78.9% on AR database using Decision Tree C4.5 classifier.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.83","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424378","","Clustering algorithms;Databases;Feature extraction;High performance computing;Training;Unsupervised learning","Big Data;decision trees;graphics processing units;image classification;learning (artificial intelligence);optimisation;parallel processing","Big Data processing;ECL;GPU;HPCC systems;K-means unsupervised learning method;MPP computing;computational cost;data-centric programming language;decision tree;enterprise control language;feature learning;graphics processing units;high performance computing cluster;image classification;machine learning;massively parallel processing;multimodal learning;object classification;object identification;optimization","","","","17","","","","9-11 Dec. 2015","","IEEE","IEEE Conferences"
"The Big Data Obstacle of Lifelogging","C. Dobbins; M. Merabti; P. Fergus; D. Llewellyn-Jones","Sch. of Comput. & Math. Sci., Liverpool John Moores Univ., Liverpool, UK","2014 28th International Conference on Advanced Information Networking and Applications Workshops","20140626","2014","","","922","926","Living in the digital age has resulted in a data rich society where the ability to log every moment of our lives is now possible. This chronicle is known as a human digital memory and is a heterogeneous record of our lives, which grows alongside its human counterpart. Managing a lifetime of data results in these sets of big data growing to enormous proportions, as these records increase in size the problem of effectively managing them becomes more difficult. This paper explores the challenges of searching such big data sets of human digital memory data and posits a new approach that treats the searching of human digital memory data as a machine learning problem.","","Electronic:978-1-4799-2653-4; POD:978-1-4799-2654-1","10.1109/WAINA.2014.142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6844757","Big Data;Clustering;Human Digital Memory;Lifelogging;Machine Learning;Sensors","Big data;Clustering algorithms;Conferences;Entropy;Memory management;Search problems;Sensors","Big Data;information retrieval;learning (artificial intelligence)","Big Data;data lifetime management;human digital memory data;lifelogging;machine learning problem","","1","","33","","","","13-16 May 2014","","IEEE","IEEE Conferences"
"Gaussian processes for flow modeling and prediction of positioned trajectories evaluated with sports data","Y. Zhao; F. Yin; F. Gunnarsson; F. Hultkratz; J. Fagerlind","Ericsson, Sweden","2016 19th International Conference on Information Fusion (FUSION)","20160804","2016","","","1461","1468","Kernel-based machine learning methods are gaining increasing interest in flow modeling and prediction in recent years. Gaussian process (GP) is one example of such kernel-based methods, which can provide very good performance for nonlinear problems. In this work, we apply GP regression to flow modeling and prediction of athletes in ski races, but the proposed framework can be generally applied to other use cases with device trajectories of positioned data. Some specific aspects can be addressed when the data is periodic, like in sports where the event is split up over multiple laps along a specific track. Flow models of both the individual skier and a cluster of skiers are derived and analyzed. Performance has been evaluated using data from the Falun Nordic World Ski Championships 2015, in particular the Men's cross country 4 × 10 km relay. The results show that the flow models vary spatially for different skiers and clusters. We further demonstrate that GP regression provides powerful and accurate models for flow prediction.","","Electronic:978-0-9964-5274-8; POD:978-1-5090-2012-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7528056","","Computational complexity;Data models;Gaussian processes;Kernel;Predictive models;Training;Trajectory","Gaussian processes;data analysis;learning (artificial intelligence);regression analysis;sport","Falun Nordic World Ski Championships 2015;GP regression;Gaussian processes;flow modeling;kernel-based machine learning;positioned trajectories prediction;ski races;sports data analytics","","","","","","","","5-8 July 2016","","IEEE","IEEE Conferences"
"A Database-Hadoop Hybrid Approach to Scalable Machine Learning","M. Yui; I. Kojima","Inf. Technol. Res. Inst., Nat. Inst. of Adv. Ind. Sci. & Technol., Tsukuba, Japan","2013 IEEE International Congress on Big Data","20130916","2013","","","1","8","There are two popular schools of thought for performing large-scale machine learning that does not fit into memory. One is to run machine learning within a relational database management system, and the other is to push analytical functions into MapReduce. As each approach has its own set of pros and cons, we propose a database-Hadoop hybrid approach to scalable machine learning where batch-learning is performed on the Hadoop platform, while incremental-learning is performed on PostgreSQL. We propose a purely relational approach that removes the scalability limitation of previous approaches based on user-defined aggregates and also discuss issues and resolutions in applying the proposed approach to Hadoop/Hive. Experimental evaluations of classification performance and training speed were conducted using a commercial advertisement dataset provided in the KDD Cup 2012, Track 2. The experimental results show that our scheme has competitive classification performance and superior training speed compared with state-of-the-art scalable machine learning frameworks, 5 and 7.65 times faster than Vow pal Wabbit and Bismarck, respectively, for a regression task.","2379-7703;23797703","Electronic:978-0-7695-5006-0; POD:978-1-4799-0182-1","10.1109/BigData.Congress.2013.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6597112","Hadoop;MapReduce;in-database analytics;iterative parameter mixture;logistic regression;stochastic gradient descent","Aggregates;Computational modeling;Machine learning algorithms;Predictive models;Relational databases;Training","learning (artificial intelligence);parallel programming;regression analysis;relational databases","PostgreSQL;batch learning;classification performance;database-Hadoop hybrid approach;incremental learning;purely relational approach;regression task;relational database management system;scalable machine learning;user-defined aggregates","","6","","27","","","","June 27 2013-July 2 2013","","IEEE","IEEE Conferences"
"The Visualization of Large Graphs Accelerated by the Parallel Nearest Neighbors Algorithm","V. Uher; P. Gajdo; V. Snáel","Dept. of Comput. Sci., VSB-Tech. Univ. of Ostrava, Ostrava, Czech Republic","2016 IEEE Second International Conference on Multimedia Big Data (BigMM)","20160818","2016","","","9","16","The search of k-nearest neighbors (K-NN) is a very common task. The K-NN is utilized in many algorithms and scientific areas like clustering, classification, machine learning, N-body simulation, triangulation, image processing and/or video processing. However, the naive implementation of the K-NN is very slow. There are many novel algorithms for the K-NN search, but they are usually based on the hierarchical clustering. The parallelization of those algorithms is a little bit tricky task. This paper primarily presents a novel parallel method for searching the k-nearest neighbors. An appropriate clustering of a sparse space based on the regular grid and the parallel search of the K-NN using the precomputed clusters are introduced. The whole method is designed for the parallel GPU computation and it is implemented on the CUDA architecture. The presented K-NN is utilized to speed up a force-directed graph layout algorithm, which can visually demonstrate the suitability of found neighbors, because they affect the layout quality. The graphs are widely used in social network analysis, computer networks or large information systems like photographic databases or multimedia databases to visualize relationships between elements. The achieved results and performance tests are presented as well.","","Electronic:978-1-5090-2179-6; POD:978-1-5090-2180-2","10.1109/BigMM.2016.73","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7544990","CUDA;Fruchterman-Reingold;GPU;K-NN;graph layouts;graph visualization;nearest neighbors;point cloud clustering;space-filling curves","Acceleration;Algorithm design and analysis;Artificial neural networks;Clustering algorithms;Graphics processing units;Heuristic algorithms;Layout","graph theory;graphics processing units;parallel architectures","CUDA architecture;K-NN;force-directed graph layout algorithm;k-nearest neighbors;parallel GPU computation;parallel nearest neighbor algorithm","","","","","","","","20-22 April 2016","","IEEE","IEEE Conferences"
"Big data and deep learning","B. M. Wilamowski; B. Wu; J. Korniak","University of Information Technology and Management, Sucharskiego 2, Rzeszow 35-225, Poland. Dept. of Electrical and Computer Engineering, Auburn University, Auburn, AL 36849, USA","2016 IEEE 20th Jubilee International Conference on Intelligent Engineering Systems (INES)","20160829","2016","","","11","16","Traditional data processing algorithms are usually not capable to process big data. As matter of fact, usually big data is being defined as such which cannot be processed with traditional techniques. At the same time a progress of technology makes that humans are now overwhelm by big data. One way of processing big data is to use deep neural networks, which are difficult to train so often a combination of several learning algorithms are used. This approach however requires a lot of human involvement in design of such systems for very specific cases. In this presentations and universal architectures and training methods are described which can handle very complex systems with minimal human interactions. Another important problem is a visualization of multidimensional data sets in 2 or 3 dimensions so humans can see them. A new very effective visualization algorithm is also presented. The third issue is the pattern clustering. Round clusters can be separated relatively easy. In this presentation new and very fast clustering of complex shape is also presented.","","Electronic:978-1-5090-1216-9; POD:978-1-5090-1217-6; USB:978-1-5090-1215-2","10.1109/INES.2016.7555103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7555103","big data;classification;clustering;deep learning;visualization","Big data;Conferences;Data visualization;Decision support systems;Iris recognition;Machine learning;Neurons","Big Data;data visualisation;learning (artificial intelligence);pattern clustering","Big Data;data processing algorithms;deep learning;deep neural networks;multidimensional data set visualization;pattern clustering;visualization algorithm","","2","","","","","","June 30 2016-July 2 2016","","IEEE","IEEE Conferences"
"Deep Near Unsupervised Learning for Data Analysis in Metabolomics, Drug-Drug Interaction Discovery and Human Gait Recognition","S. K. Halgamuge","Dept. of Mech. Eng., Univ. of Melbourne, Melbourne, VIC, Australia","2016 7th International Conference on Intelligent Systems, Modelling and Simulation (ISMS)","20170316","2016","","","5","6","We have been working on the application of Machining Learning in Metabolomics, Drug-Drug Interaction Discovery and Human Gait Recognition [1-5], profiling large data sets. Extraction of vital information about 1) plant metabolomics that can improve the environment and food quality, 2) in vitro neuronal network behavior patterns for various drugs that can be used to characterize drugs for brain deceases and 3) human gait patterns that can reveal various diseases were among these applications. We have demonstrated with considerable success in using unsupervised clustering techniques to analyze genetic and metabolomic data. This includes analysis of drought resistance in wheat [4] and microbial metagenomes [5]. We introduced two methods: Near Unsupervised Learning (NUL) and Sub-sample Error Graphs (SEGs) [5] to analyze large amount of data. Self Organizing Map (SOM), which is one of the widely used Unsupervised Neural Networks, has been used as a data-mining tool due to its ability to map high dimensional data into a two dimensional feature map, which is expected to be topology preserving allowing users to visually identify clusters by their topological relationships in terms of their proximity on the map. The Growing Self Organizing Maps (GSOM) further allows the map size to be determined by the algorithm, which relies upon a user set parameter called Spread Factor (SF) [6]. The wide availability of GPUs for affordable prices allows faster comparison of various SOMs with different maps sizes taking away some of the advantages GSOM claimed. Further development of GSOM into a Dynamic SOM Tree exploits the possibility of varying SF to obtain multiple GSOMs from a small number of compact clusters to a large number of sparse clusters [7]. NUL methods can be applied on GSOM using a small number of labels that should be available for every class. This is however not realistic in some applications where the number of classes cannot be explicitly known. We propose a new metho- call Deep Near Unsupervised Learning (D-NUL), where Dynamic SOM tree is used instead of GSOM and the number of classes are not assumed to be known. The implementation of Dynamic SOM tree methods with varying SF on GPUs will make the computation with D-NUL possible for many problems in big data analytics.","2166-0670;21660670","Electronic:978-1-5090-0665-6; POD:978-1-5090-0666-3","10.1109/ISMS.2016.77","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877179","","Big Data;Bioinformatics;Biological system modeling;Biomedical engineering;Intelligent systems;Metabolomics;Unsupervised learning","Big Data;data mining;gait analysis;self-organising feature maps;trees (mathematics);unsupervised learning","Big Data analysis;D-NUL;EG;GSOM;NUL;SF;big data analytics;data-mining tool;deep near unsupervised learning;drug-drug interaction discovery;dynamic SOM tree;growing self organizing maps;human gait recognition;in vitro neuronal network behavior patterns;machine learning;plant metabolomics;spread factor;sub-sample error graphs;unsupervised clustering techniques","","","","","","","","25-27 Jan. 2016","","IEEE","IEEE Conferences"
"#ChennaiFloods: Leveraging Human and Machine Learning for Crisis Mapping during Disasters Using Social Media","B. Anbalagan; C. Valliyammai","MIT, Dept. of Comput. Technol., Anna Univ., Chennai, India","2016 IEEE 23rd International Conference on High Performance Computing Workshops (HiPCW)","20170202","2016","","","50","59","The recent emergence of ubiquitous smart communication devices accelerate people to post the current trending topics in real time as micro blogs, tweets, posts and multimedia content on social media sites along with geographical location tags (geo-tags). Specifically, during recent floods in Tamilnadu 2015, the early warnings about flooded areas emerged to get posted in popular social media with geo-parsed hash tags continuously. In the humanitarian view, the real-time crisis sparked great interest in designing an innovative methodology using big social media data analysis along with supervised machine learning techniques to actuate immediate disaster response and rescue efforts in near future. The proposed system performs disaster tweet collection based on trending disaster hash tags. Our system performs Naive-Bayesian (multinomial) and SSVM classification on collected tweets to identify the severity of the disaster. Based on location-to-interpolation cluster proximity, disaster geographic map is generated for the affected area. Our approach detects the tweets fitted into correct classifier label, and generate an output with detection rate of 79% to 91% of the time. The predicted disaster mapping results are highly accurate up to 89% for real time geo-parsed tweets that matched with actual location at-risk during the flood.","","Electronic:978-1-5090-5773-3; POD:978-1-5090-5774-0","10.1109/HiPCW.2016.016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7837049","Crowd sourcing;Disaster Management;Emergency Informatics;Geo-coding;Geo-parsing;Hash tags;Naive-Bayesian;SSVM Classifier;Social media","Computational modeling;Earthquakes;Floods;Real-time systems;Support vector machines;Twitter","Bayes methods;Big Data;crowdsourcing;data analysis;disasters;emergency management;floods;learning (artificial intelligence);pattern classification;social networking (online);support vector machines;text analysis;ubiquitous computing","#ChennaiFloods;India;SSVM classification;Tamilnadu;big social media data analysis;crisis mapping;disaster geographic map generation;disaster response;disaster severity identification;disaster tweet collection;flooded area early warning;geo-parsed hash tags;geo-tags;geographical location tags;human learning;location at-risk;location-to-interpolation cluster proximity;microblogs;multimedia content;multinomial classification;naive-Bayesian classification;real time geo-parsed tweets;real-time crisis;rescue effort;social media sites;supervised machine learning technique;trending disaster hash tags;trending topics;ubiquitous smart communication devices","","","","","","","","19-22 Dec. 2016","","IEEE","IEEE Conferences"
"An extension of topic models for text classification: A term weighting approach","S. Lee; J. Kim; S. H. Myaeng","Division of Web Science and Technology, KAIST Daejeon, South Korea","2015 International Conference on Big Data and Smart Computing (BIGCOMP)","20150402","2015","","","217","224","Text classification has become a critical step in big data analytics. For supervised machine learning approaches to text classification, availability of sufficient training data with classification labels attached to individual text units is essential to the performance. Since labeled data are usually scarce, however, it is always desirable to devise a semi-supervised method where unlabeled data are used in addition to labeled ones. A solution is to apply a latent factor model to generate clustered text features and use them for text classification. The main thrust of the current research is to extend Latent Dirichlet Allocation (LDA) for this purpose by considering word weights in sampling and maintaining balances of topic distributions. A series of experiments were conducted to evaluate the proposed method for classification tasks. The result shows that the topic distributions generated by the balance weighted topic modeling method add some discriminative power to feature generations for classification.","2375-933X;2375933X","Electronic:978-1-4799-7303-3; POD:978-1-4799-7304-0; USB:978-1-4799-7302-6","10.1109/35021BIGCOMP.2015.7072834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7072834","Latent Dirichlet Allocation;Topic modeling;feature generation;text classification;text clustering","Data models;Feature extraction;Resource management;Text categorization;Training;Training data;Vocabulary","Big Data;data analysis;learning (artificial intelligence);natural language processing;pattern classification;pattern clustering;text analysis","Big Data analytics;LDA;balance weighted topic modeling method;classification labels;clustered text feature generation;discriminative power;individual text units;labeled data;latent Dirichlet allocation;latent factor model;supervised machine learning approach;term weighting approach;text classification;topic distribution;training data;word weights","","5","","21","","","","9-11 Feb. 2015","","IEEE","IEEE Conferences"
"A Reliability Benchmark for Big Data Systems on JointCloud","Y. Zheng; L. Xu; W. Wang; W. Zhou; Y. Ding","","2017 IEEE 37th International Conference on Distributed Computing Systems Workshops (ICDCSW)","20170717","2017","","","306","310","JointCloud provides a large-scale, flexible, and elastic computing resource platform. Big data systems such as MapReduce and Spark are widely deployed on this platform for big data processing. How to choose a cloud platform in accordance with the need of customers is a problem. Current performance benchmarking suites can choose suitable cloud platforms for customers. However, they do not consider the reliability of applications running atop big data systems. These systems have high scalability, but the applications running atop them often generate runtime errors, such as out of memory errors, I/O exceptions, and task timeouts. For users, they want to know whether the developed applications have potential application faults. For system designers and manag-ers, they want to know whether the deployed/updated systems have potential system faults. In addition, current benchmarks for big data system are also only designed for performance testing. To fill this gap, we propose a reliability benchmark, which contains representative applications, an abnormal data generator, and a configuration combination generator. Differ-ent from performance benchmarks, this benchmark (1) gener-ates abnormal test data according to the application character-istics, and (2) reduces the configuration combination space based on configuration features. Currently, we implemented this benchmark on Spark system. In our preliminary test, we found three types of errors (i.e., out of memory errors, timeout and wrong results) in five SQL, Machine Learning, and Graph applications.","","Electronic:978-1-5386-3292-5; POD:978-1-5386-3293-2","10.1109/ICDCSW.2017.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7979836","Spark;benchmark;big data system;cloud computing;reliability","Benchmark testing;Big Data;Cloud computing;Reliability;Runtime;Sparks;Vegetation","Big Data;cloud computing;reliability","Big Data systems;JointCloud;SQL;Spark system;abnormal test data generation;cloud platform;configuration combination generator;graph applications;machine learning;reliability benchmark","","","","","","","","5-8 June 2017","","IEEE","IEEE Conferences"
"GraphSC: Parallel Secure Computation Made Easy","K. Nayak; X. S. Wang; S. Ioannidis; U. Weinsberg; N. Taft; E. Shi","","2015 IEEE Symposium on Security and Privacy","20150720","2015","","","377","394","We propose introducing modern parallel programming paradigms to secure computation, enabling their secure execution on large datasets. To address this challenge, we present Graph SC, a framework that (i) provides a programming paradigm that allows non-cryptography experts to write secure code, (ii) brings parallelism to such secure implementations, and (iii) meets the need for obliviousness, thereby not leaking any private information. Using Graph SC, developers can efficiently implement an oblivious version of graph-based algorithms (including sophisticated data mining and machine learning algorithms) that execute in parallel with minimal communication overhead. Importantly, our secure version of graph-based algorithms incurs a small logarithmic overhead in comparison with the non-secure parallel version. We build Graph SC and demonstrate, using several algorithms as examples, that secure computation can be brought into the realm of practicality for big data analysis. Our secure matrix factorization implementation can process 1 million ratings in 13 hours, which is a multiple order-of-magnitude improvement over the only other existing attempt, which requires 3 hours to process 16K ratings.","1081-6011;10816011","Electronic:978-1-4673-6949-7; POD:978-1-4673-6950-3","10.1109/SP.2015.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163037","graph algorithms;oblivious algorithms;parallel algorithms;secure computation","Algorithm design and analysis;Clustering algorithms;Computational modeling;Data mining;Machine learning algorithms;Parallel processing;Programming","matrix decomposition;parallel programming;security of data","GraphSC framework;graph-based algorithms;logarithmic overhead;matrix factorization;parallel programming paradigm;parallel secure computation;secure code writing","","9","","78","","","","17-21 May 2015","","IEEE","IEEE Conferences"
"Resource Frequency Prediction in Healthcare: Machine Learning Approach","D. Vieira; J. Hollmén","X-akseli Oy, Espoo, Finland","2016 IEEE 29th International Symposium on Computer-Based Medical Systems (CBMS)","20160818","2016","","","88","93","Determining the minimal amount of resources needed to ensure minimal number of bottlenecks in the patient flow not only promotes patient satisfaction but also provides financial benefits to hospitals. The increase of data gathering by healthcare facilities in the last years have brought new opportunities to apply machine learning techniques to tackle this problem. This work makes use of data gathered from the Oulu University Hospital in Finland between 2011 and 2014 to study the effectiveness of machine learning techniques to predict resources usage. This work investigates the problem of resource frequency prediction and compares the performance of Nearest Neighbours and Random Forest. The application of data clustering as a preprocessing step is also explored as a way to improve the prediction accuracy of resources whose behavior change over time. The results indicate that 1) highly frequented resources can be predicted with higher accuracy than the lowly frequented resources, 2) the Random Forest have similar performance to the Nearest Neighbours although Random Forest performs better, 3) clustering improves the performance of the Nearest Neighbours but not of Random Forest, and 4) if averages are used to determine the resource frequency then cluster averages yields higher accuracy than all data averages.","","Electronic:978-1-4673-9036-1; POD:978-1-4673-9037-8","10.1109/CBMS.2016.59","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7545963","healthcare modelling;hierarchical clustering;regression;supervised learning;time series prediction","Forecasting;Hospitals;Measurement;Optimization;Time series analysis;Vegetation","health care;learning (artificial intelligence);medical administrative data processing;pattern clustering;resource allocation","Finland;Oulu University Hospital;data clustering;healthcare facilities;hospital financial benefits;machine learning;nearest neighbours;patient flow bottleneck;patient satisfaction;random forest;resource frequency prediction;resources usage prediction","","","","","","","","20-24 June 2016","","IEEE","IEEE Conferences"
"An Effective and Efficient Similarity-Matrix-Based Algorithm for Clustering Big Mobile Social Data","G. Bordogna; L. Frigerio; A. Cuzzocrea; G. Psaila","IREA, Milan, Italy","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","514","521","Nowadays a great deal of attention is devoted to the issue of supporting big data analytics over big mobile social data. These data are generated by modern emerging social systems like Twitter, Facebook, Instagram, and so forth. Mining big mobile social data has been of great interest, as analyzing such data is critical for a wide spectrum of big data applications (e.g., smart cities). Among several proposals, clustering is a well-known solution for extracting interesting and actionable knowledge from massive amounts of big mobile (geo-located) social data. Inspired by this main thesis, this paper proposes an effective and efficient similarity-matrix-based algorithm for clustering big mobile social data, called TourMiner, which is specifically targeted to clustering trips extracted from tweets, in order to mine most popular tours. The main characteristic of TourMiner consists in applying clustering over a well-suited similarity matrix computed on top of trips. A comprehensive experimental assessment and analysis over Twitter data finally comfirms the benefits coming from our proposal.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838195","Big Data Analytics; Big Mobile Social Data; Big Data Clustering","Algorithm design and analysis;Clustering algorithms;Data mining;Mobile communication;Proposals;Semantics;Trajectory","Big Data;data analysis;data mining;pattern clustering;social networking (online)","Big Data analytics;TourMiner algorithm;Twitter data;big mobile social data clustering;data mining;similarity matrix based algorithm","","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conferences"
"The Berkeley Data Analytics Stack: Present and future","M. Franklin","UC Berkeley, Berkeley, CA, USA","2013 IEEE International Conference on Big Data","20131223","2013","","","2","3","The Berkeley AMPLab was founded on the idea that the challenges of emerging Big Data applications requires a new approach to analytics systems. Launching in early 2011, the project set out to rethink the traditional analytics stack, breaking down technical and intellectual barriers that had arisen during decades of evolutionary development. The vision of the lab is to seamlessly integrate the three main resources available for making sense of data at scale: Algorithms (such as machine learning and statistical techniques), Machines (in the form of scalable clusters and elastic cloud computing), and People (both individually as analysts and en masse, as with crowdsourced human computation). To pursue this goal, we assembled a research team with diverse interests across computer science, forged relationships with domain experts on campus and elsewhere, and obtained the support of leading industry partners and major government sponsors. The lab is realizing its ideas through the development of a freely-available Open Source software stack called BDAS: the Berkeley Data Analytics Stack. In the nearly three years the lab has been in operation, we've released major components of BDAS. Several of these components have gained significant traction in industry and elsewhere: the Mesos cluster resource manager, the Spark inmemory computation framework, and the Shark query processing system. BDAS shows up prominently in many industry discussions of the future of the Big Data analytics ecosystem - a rare degree of impact for an ongoing academic project. Given this initial success, the lab is continuing on its research path, moving ""up the stack"" to better integrate and support deep machine learning and to make people a full-fledged resource for making sense of Big Data. In this talk, I'll first outline the motivation and insights behind our research approach and describe how we have organized to address the cross-disciplinary nature of Big Data challenges. I will then describe- the current state of BDAS with an emphasis on the key components listed above and will address our current efforts on machine learning scalability and ease of use, and hybrid human/computer processing. Finally I will present our current views of how all the pieces will fit together to form a system that can adaptively bring the right resources to bear on a given data-driven question to meet time, cost and quality requirements throughout the analytics lifecycle.","","Electronic:978-1-4799-1293-3; POD:978-1-4799-1294-0","10.1109/BigData.2013.6691545","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691545","","","Big Data;cloud computing;learning (artificial intelligence)","Berkeley data analytics stack;Mesos cluster resource manager;Shark query processing system;Spark inmemory computation framework;elastic cloud computing;evolutionary development;hybrid human-computer processing;machine learning;ppen source software stack;scalable cluster;statistical technique","","3","","","","","","6-9 Oct. 2013","","IEEE","IEEE Conferences"
"A scalable approach for location-specific detection of Santa Ana conditions","M. H. Nguyen; D. Uys; D. Crawl; C. Cowart; I. Altintas","San Diego Supercomputer Center, University of California, San Diego, La Jolla, CA, USA","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","1340","1345","Santa Ana conditions are hot, dry, windy weather conditions that can greatly increase the dangers of wildfires in southern California. We present a machine learning approach to detect Santa Ana conditions based on sensor measurements from weather stations. Cluster analysis is performed on historical weather data to build models to identify Santa Ana patterns. A separate model is built using data from each weather station to capture the patterns specific to the microclimate of each region. Real-time sensor data from a weather station can then be processed to determine if the region surrounding that station is experiencing Santa Ana conditions. Results can be used as a warning system to focus firefighting efforts on regions with increased wildfire risks. Through the use of the Kepler workflow system and distributed computing with Spark, data from several weather stations can be processed in parallel using a scalable clustering algorithm, allowing our approach to scale to large datasets from multiple weather stations.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840740","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840740","distributed computing;k-means clustering;location-specific sensor processing;scientific workflow;wildfire monitoring","Analytical models;Data models;Distributed computing;Fires;Sparks;Wind","geophysics computing;learning (artificial intelligence);meteorology;parallel processing;pattern clustering;sensors;statistical analysis;wildfires","Kepler workflow system;Santa Ana conditions;Spark;cluster analysis;distributed computing;firefighting efforts;location-specific detection;machine learning;parallel processing;scalable clustering algorithm;sensor measurements;southern California;weather stations;wildfire risks","","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conferences"
"Topic classification and clustering on Indonesian complaint tweets for bandung government using supervised and unsupervised learning","T. Pratama; A. Purwarianti","Institut Teknologi Bandung, Indonesia","2017 International Conference on Advanced Informatics, Concepts, Theory, and Applications (ICAICTA)","20171102","2017","","","1","6","Seeing the public of Bandung city as an active social media user, Bandung government provides channel in Twitter for citizen to report their complaints. In order to make the citizen complaint monitoring easier, there is a need to automatically detect the topics of complaint tweets (written in Indonesian language) in order to assist the government in managing the complaints reported. In this paper, a system to detect the topics of Indonesian complaint tweets automatically using supervised learning and unsupervised learning approaches is proposed. The supervised learning approach is implemented to classify complaint tweets topic, whereas the unsupervised learning approach is used to cluster complaint tweets based on the similarity of detail information contained in the complaints. Both the supervised learning and the unsupervised learning approaches are required to classify the topics of a tweet and to capture the detail information from each detected topic. The topics are classified using single label and multi label classification. The supervised learning approach is evaluated using accuracy, precision, recall, and F1 score. Three supervised machine learning algorithms are evaluated: Sequential Minimal Optimization, Naïve Bayes Multinomial, and Random Forests. The best algorithm for single label topic classification is SMO, with the accuracy average of 95%, whereas the best algorithm for multi-label topic classification is Random Forests, with 97.92% accuracy, 98.74% precision, 98.36% recall, and 98.44% F1 score. In the unsupervised learning approach, Clustering Index Value is used to evaluate the topic clusters detected. Two unsupervised learning algorithms are evaluated; Exemplar Based Topic Detection and Document Pivot Technique using TF-IDF. Exemplar Based Topic Detection has the best performance for detecting detail topic clusters with Clustering Index Value of 0.9653.","","Electronic:978-1-5386-3001-3; POD:978-1-5386-3002-0; USB:978-1-5386-3000-6","10.1109/ICAICTA.2017.8090981","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8090981","Indonesian complaint text;supervised learning;topic classification;topic clustering;unsupervised learning","Classification algorithms;Clustering algorithms;Government;Supervised learning;Training;Twitter;Unsupervised learning","Bayes methods;government data processing;optimisation;pattern classification;pattern clustering;random processes;social networking (online);unsupervised learning","Bandung government;Clustering Index Value;Document Pivot Technique;Exemplar Based Topic Detection;Indonesian complaint tweets;Naïve Bayes Multinomial;Random Forests;Twitter;citizen complaint monitoring;clustering;multilabel topic classification;sequential minimal optimization;single label topic classification;supervised learning approach;supervised machine learning algorithms;unsupervised learning approach","","","","","","","","16-18 Aug. 2017","","IEEE","IEEE Conferences"
"A Novel approach to predict diabetes mellitus using modified Extreme learning machine","R. Priyadarshini; N. Dash; R. Mishra","C.V. Raman College of Engg., Bhubaneswar, India","2014 International Conference on Electronics and Communication Systems (ICECS)","20140908","2014","","","1","5","Data Classification and predictions are one of the prime tasks in Data mining. They continue to play a vital role in the area of computer science and data processing field. Clustering and classifications in Data Mining are used in various domains to give meaning to the available data and give some useful prediction results which can be applied to some of the crucial problem areas of the real world. Diabetes mellitus otherwise known as a slow poison by the medical experts is a major, alarming and gradually becoming a global problem. This paper experimented and used the concept of modified extreme learning machine to identify the patients of being diabetic or non-diabetic basing on some previously given data which in turn helps the medical people to identify whether someone is affected by diabetes or not. It also describes and compares the application of two popular machine learning methods: Back propagation neural network and modified Extreme learning machine which are used as binary classifiers to address the diabetes prediction problem. These two approaches are applied on same type of multi class classification datasets and the work tries to generate some comparative inferences from training and testing results. The datasets which are used in our work is taken from UCI learning repository.","","Electronic:978-1-4799-2320-5; POD:978-1-4799-6638-7","10.1109/ECS.2014.6892740","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6892740","Back propagation algorithm;Classification;Diabetes mellitus;Extreme learning machine","Artificial neural networks;Biology;Classification algorithms;Computer architecture;Databases;Diseases;Prediction algorithms","backpropagation;diseases;medical information systems;neural nets;pattern classification","UCI learning repository;back propagation neural network;binary classifiers;diabetes mellitus prediction;machine learning methods;modified extreme learning machine;multiclass classification datasets;slow poison","","5","","9","","","","13-14 Feb. 2014","","IEEE","IEEE Conferences"
"Cluster graph based model for extracting and searching algorithm in bigdata","G. Veena; J. Joseph","Dept. of Computer Science College of Engineering, Kidangoor Kottayam, India","2017 International Conference on Networks & Advances in Computational Technologies (NetACT)","20171023","2017","","","432","437","Search systems that is used to search for information. Cite Seer was a search engine to search academic documents. Platforms are not available to discover algorithms in scholarly big data. The limitations of these search engines make the searching more difficult. Hence special purpose systems are used. Here proposes a search system to extract algorithm representations. Algorithms can be represented using pseudocode and algorithm procedures. Two methods are used for this purpose. The ways to extract textual metadata for each algorithm is discussed. Based on graph based clustering, context term is extracted. The metadata extraction is done using document element summarization. The synopsis is generated using topic modeling approach and context terms. Finally a synopsis comparison is done.","","CD:978-1-5090-6589-9; Electronic:978-1-5090-6590-5; POD:978-1-5090-6591-2","10.1109/NETACT.2017.8076810","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8076810","Clustering;Context terms;Metadata extraction","Classification algorithms;Clustering algorithms;Context modeling;Feature extraction;Machine learning algorithms;Metadata;Probabilistic logic","Big Data;graph theory;information retrieval;meta data;pattern clustering;search engines;text analysis","Big Data;Cite Seer;algorithm representations;cluster graph;document element summarization;pseudocode;search engine;search system;textual metadata extraction;topic modeling","","","","","","","","20-22 July 2017","","IEEE","IEEE Conferences"
"DeepSim: Cluster Level Behavioral Simulation Model for Deep Learning","Y. Shi; K. Long; K. Balasubramanian; B. Bian; A. Procter; R. Illikkal","","2017 IEEE 15th Intl Conf on Dependable, Autonomic and Secure Computing, 15th Intl Conf on Pervasive Intelligence and Computing, 3rd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)","20180402","2017","","","1248","1253","We are witnessing an explosion of AI based use cases driving the computer industry, and especially datacenter and server architectures. As Intel faces fierce competition in this emerging technology space, it is important that architecture definitions and directions are driven with data from proper tools and methodologies, and insights are drawn from end-to-end holistic analysis at the datacenter levels. In this paper, we introduce DeepSim, a cluster-level behavioral simulation model for deep learning. DeepSim, which is based on the Intel CoFluent simulation framework, uses timed behavioral models to simulate complex interworking between compute nodes, networking, and storage at the datacenter level, providing a realistic performance model of a real-world image recognition applications based on the popular Deep Learning Framework Caffe. The end-to-end simulation data from DeepSim provides insight which can be used for architecture analysis driving future datacenter architecture directions. DeepSim enables scalable system design, deployment, and capacity planning through accurate performance insights. Results from preliminary scaling studies (e.g. node scaling and network scaling) and what-if analyses (e.g., Xeon with HBM and Xeon Phi with dual OPA) are presented in this paper. The simulation results are correlated well with empirical measurements, achieving an accuracy of 95%.","","Electronic:978-1-5386-1956-8; POD:978-1-5386-1957-5; USB:978-1-5386-1955-1","10.1109/DASC-PICom-DataCom-CyberSciTec.2017.200","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8328544","AlexNet;Architecture Analysis;Behavioral Simulation;Datacenter;Deep Learning;Performance Analysis;Server Architecture","Computational modeling;Computer architecture;Correlation;Hardware;Machine learning;Neural networks;Software","computer centres;digital simulation;image recognition;learning (artificial intelligence)","Deep Learning Framework Caffe;DeepSim;Intel CoFluent simulation framework;architecture analysis;architecture definitions;behavioral models;cluster level behavioral simulation model;cluster-level behavioral simulation model;datacenter architecture directions;datacenter level;deep learning;emerging technology space;end-to-end holistic analysis;end-to-end simulation data;performance model;server architectures","","","","","","","","6-10 Nov. 2017","","IEEE","IEEE Conferences"
"An Efficient Hybrid-Clustream Algorithm for Stream Mining","A. Kumar; A. Singh; R. Singh","CSE Dept., DCRUST Murthal, India","2017 13th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)","20180412","2017","","","430","437","Stream clustering is a standout amongst the most imperative fields in machine learning. Traditional unsupervised clustering tasks have been normally carried out in batch mode where data could be somehow fitted in memory and therefore several passes on the data are allowed. However the new Big Data paradigm has created a new environment where data can be potentially non-finite and arrive continuously. Such streams of data can reach computing systems at high speeds and contain data generation processes which might be non-stationary. For clustering tasks, this implies inconceivability to store all information in memory and obscure number and size of clusters. Noise levels can also be high due to either data generation or transmission. All these factors make traditional clustering methods not suitable to cope. As a consequence, stream clustering has emerged as a field of intense research with the aim of tackling these challenges. Clustream is one of the most advanced state of the art stream clustering algorithm. It normally requires two phases: first online micro-clustering phase, where statistics are gathered describing the incoming data; and a second offline macro-clustering phase, where a conventional non-stream clustering algorithm is executed using the high level statistics resulting from the online step. Because of its design, it requires expert-level parametrization or suffers from low runtime performance or has high sensitivity to noise or degrade considerably in high dimensional spaces because of their offline step. We propose a new stream clustering algorithm, the Clustream-hybrid based on Clustream clustering principles. It extends the same process used in Clustream but uses k-means++ instead of k-means in macro-clustering phase enabling it to accomplish quick runtime calculation while additionally keeping accuracy in high dimensional settings. We integrate it in MOA (Massive Online Analysis) tool. We evaluated the results with nine clustering qu- lity metrics and compared the performance with Clustream for both synthetic and real data sets. The results are encproposedaging, outperforming in most of the cases in quality metrics.","","Electronic:978-1-5386-4283-2; POD:978-1-5386-4284-9","10.1109/SITIS.2017.77","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8334783","Clustering;Clustream;Data mining;Data streams;Hybrid-Clustream;MOA","Approximation algorithms;Clustering algorithms;Data mining;Kernel;Machine learning algorithms;Measurement;Task analysis","data handling;data mining;learning (artificial intelligence);pattern clustering;statistical analysis;uncertainty handling","Big Data paradigm;Clustream clustering principles;clustering methods;data generation processes;expert-level parametrization;high dimensional settings;high dimensional spaces;high level statistics;hybrid-Clustream algorithm;imperative fields;incoming data;microclustering phase;noise levels;nonstationary;nonstream clustering algorithm;obscure number;offline macro-clustering phase;stream clustering algorithm;stream mining;unsupervised clustering tasks","","","","","","","","4-7 Dec. 2017","","IEEE","IEEE Conferences"
"[Front cover]","","","2017 12th International Scientific and Technical Conference on Computer Sciences and Information Technologies (CSIT)","20171109","2017","1","","1","1","Presents the front cover or splash screen of the proceedings record.","","CD:978-1-5386-1637-6; Electronic:978-1-5386-1639-0; POD:978-1-5386-1640-6; Paper:978-1-5386-1638-3","10.1109/STC-CSIT.2017.8098852","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8098852","","","Big Data;Internet;Markov processes;Web sites;agriculture;bankruptcy;data analysis;data visualisation;decision support systems;environmental factors;fishing industry;fuzzy set theory;game theory;genetic algorithms;grammars;greedy algorithms;health care;image processing;knowledge based systems;medical computing;neural nets;particle swarm optimisation;patient diagnosis;pattern classification;pattern clustering;quality assurance;recommender systems;simulated annealing;smart cities;software engineering;sustainable development;text analysis;travel industry;wastewater treatment","Arabic text classification;Big Data;Cyrillic handwritten font processing;English sonorant Phonene group;Internet content analysis;Markov process;Ukranian sign language;Web site pages;bankruptcy;biological wastewater treatment;breast cancer diagnosis;combinatorics;company knowledge management system;computer sciences;computing machines;data analysis;data visualization;decision support system;deformation processes;education informatics;enterprise finance management;fishing industry;fuzzy knowledge base;fuzzy neural networks;game theory;gene expression sequences clustering;greedy-genetic algorithm;greenhouse gas emissions;home energy efficiency;hypergraphs;image filtration;information technologies;knowledge extraction;knowledge-oriented systems;machine learning;maritime container traffic;multi-dimensional data arrays;nonlinear physical systems;oncology diagnosis;particle swarm optimization;persuasive technologies;phonostatistical structures;pollution;production systems sustainable development;project management;public transport;relaxation processes;simulated annealing algorithm;smart city;social networks;software metric analysis;vibrating processes;wireless sensor networks","","","","","","","","5-8 Sept. 2017","","IEEE","IEEE Conferences"
"Machine Learning-Based Framework for Resource Management and Modelling for Video Analytic in Cloud-Based Hadoop Environment","M. Al-Rawahi; E. A. Edirisinghe; T. Jeyarajan","Dept. of Comput. Sci., Loughborough Univ., Loughborough, UK","2016 Intl IEEE Conferences on Ubiquitous Intelligence & Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)","20170116","2016","","","801","807","Hadoop framework has recently been adapted for use by the video analytics community for intensive, distributed video processing, storage. However, the challenge is to estimate the required amount of resources to be used in such an environment to fulfil the requirements of a user with requirements constraints. Therefore, it is important to understand how to model the performance of a Hadoop based implementation of video analytic applications in terms of meeting their performance goals. In this paper we propose the use of machine learning approachs in modelling the execution time based on the given resources. The prediction is based on parameters related to typical video analytic applications such as video file characteristics (e, g, resolution, file size, frame rate. etc.), cluster resource consumption,, Hadoop configuration values (reducer slots, tasks). The investigation carried out compares the use of different machine learning classifiers with regard to their best obtainable performance accuracies, show that a decision based model (M5P) outperforms a Linear Regression model, while the Ensemble Classifier, Bagging, out-performs these standard single classifiers. The research conducted bridges an existing research gap in video analytic-related performance predictions, whereby current research focuses on different application types, is largely limited to using standard learning algorithms such as SVM, Linear Regression, Multilayer Perceptron (MLP).","","Electronic:978-1-5090-2771-2; POD:978-1-5090-2772-9","10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816924","Cloud computing;Hadoop;Machine Learning;MapReduce;performance model;resource allocation;video analytic","Cloud computing;Computational modeling;Data models;Forensics;Predictive models;Quality of service;Resource management","cloud computing;image classification;learning (artificial intelligence);parallel processing;resource allocation;video signal processing","cloud-based Hadoop environment;decision based model;distributed video storage;execution time modelling;intensive distributed video processing;machine learning classifiers;performance accuracies;resource management;video analytic applications;video analytic modelling","","","","","","","","18-21 July 2016","","IEEE","IEEE Conferences"
"An evaluation of alternative shared-nothing architecture for analytical processing systems","H. Choi; J. Park; Y. I. Lee; K. Roh; K. La","Gruter Inc.","2015 IEEE International Conference on Big Data (Big Data)","20151228","2015","","","1084","1093","Data analysis, mining, and machine learning on large-scale data sets have gained much attention in the academia and industry. Tremendous computational and storage capacities are required in order to handle such large data sets. In these days, the conventional wisdom is to build a large cluster which consists of a number of commodity x86 machines, each of which is equipped with two or four physical CPUs and several HDD or SSD drives, connected via high-speed network. In this paper, we have asked is there any alternative approach? We introduce MicroBrick cluster, a prototype cluster machine architecture by Samsung Electronics. We investigate the possibility of MicroBricks cluster architecture as an alternative cluster infrastructure for shared-nothing analytical processing systems. A MicroBricks cluster consists of multiple MicroBricks chassis. Unlike commodity x86 clusters where each machine has its own CPUs, memory, and disk drives, a single MicroBricks chassis consists of multiple highly dense and pluggable computing and storage modules, and the modules are connected through high-speed inter connection on a single board. As a result, MicroBricks clusters occupy much smaller space and have high bandwidth connectivity required for shared-nothing distributed processing. In addition, a MicroBricks cluster is likely to consume lower power. These characteristics are very suitable for large clusters built in data centers. In order to prove this possibility, we carried out the comparison experiments of both MicroBricks cluster as well as commodity cluster. We carried out TPC-H benchmark by means of an open source distributed SQL engine in Hadoop in both architectures. In order to deeply analyze both architecture, we collected the profiling information during the TPC-H benchmark, and we conducted micro benchmark with the profile results. The experimental results are promising for the MicroBricks computing, and the results show that the query response times of MicroBricks c- mputing architecture outperforms those of commodity cluster without hurting the innate advantages of the MicroBricks cluster architecture.","","Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9","10.1109/BigData.2015.7363862","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363862","","Benchmark testing;Computer architecture;Databases;Fault tolerance;Fault tolerant systems;Parallel processing;Servers","SQL;data analysis;data mining;learning (artificial intelligence);parallel processing","Hadoop;MicroBrick cluster machine architecture;Samsung Electronics;TPC-H benchmark;alternative shared-nothing architecture;data analysis;data centers;data mining;high-speed interconnection;machine learning;open source distributed SQL engine;shared-nothing analytical processing systems;shared-nothing distributed processing","","","","18","","","","Oct. 29 2015-Nov. 1 2015","","IEEE","IEEE Conferences"
"Automated IT system failure prediction: A deep learning approach","K. Zhang; J. Xu; M. R. Min; G. Jiang; K. Pelechrinis; H. Zhang","School of Information Sciences, University of Pittsburgh","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","1291","1300","In mission critical IT services, system failure prediction becomes increasingly important; it prevents unexpected system downtime, and assures service reliability for end users. While operational console logs record rich and descriptive information on the health status of those IT systems, existing system management technologies mostly use them in a labor-intensive forensics approach, i.e., identifying what went wrong after the fact. Recent efforts on log-based system management take an automation approach with text mining techniques, such as term frequency - inverse document frequency (TF-IDF). However, those techniques lead to a high-dimensional feature space, and are not easily generalizable to heterogeneous log formats. In this paper, we present a novel system that automatically parses streamed console logs and detects early warning signals for IT system failure prediction. In particular, our solution includes a log pattern extraction method by clustering together logs with similar format and content. We then resemble the TF-IDF idea by considering each pattern as a word and the set of patterns in each discretized epoch as a document. This leads to a feature space with significantly lower dimensionality that can provide robust signals for the status of the system. As system failures tend to occur very rare, we apply a recurrent neural network, namely, Long Short-Term Memory (LSTM), to deal with the “rarity” of labeled data in the training process. LSTM is able to capture the long-range dependency across sequences, therefore outperforms traditional supervised learning methods in our application domain. We evaluated and compared our proposed technology with state-of-the-art machine learning approaches using real log traces from two large enterprise systems. The results showed the advantage and potentials of our system in prediction of complex IT failures. To our knowledge, our work is the first that employs LSTM f- r log-based system failure prediction.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840733","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840733","Deep Learning;Failure Prediction;LSTM;Log Analysis;Recurrent Neural Network;System Management;Text Mining","Feature extraction;Layout;Machine learning;Recurrent neural networks;Supervised learning;Text mining;Training","data mining;digital forensics;failure analysis;learning (artificial intelligence);recurrent neural nets;system recovery;text analysis","LSTM;TF-IDF;automated IT system failure prediction;complex IT failures;deep learning;discretized epoch;early warning signals;enterprise systems;health status;heterogeneous log formats;high-dimensional feature space;labor-intensive forensics;log pattern extraction method;log-based system failure prediction;log-based system management;long short-term memory;machine learning;mission critical IT services;operational console logs;recurrent neural network;robust signals;service reliability;streamed console logs;supervised learning methods;system failures;term frequency-inverse document frequency;text mining techniques;unexpected system downtime","","1","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conferences"
"Sentiment expression via emoticons on social media","H. Wang; J. A. Castanon","Silicon Valley Laboratory, IBM San Jose, USA","2015 IEEE International Conference on Big Data (Big Data)","20151228","2015","","","2404","2408","Emoticons (e.g., :) and :( ) have been widely used in sentiment analysis and other NLP tasks as features to machine learning algorithms or as entries of sentiment lexicons. In this paper, we argue that while emoticons are strong and common signals of sentiment expression on social media the relationship between emoticons and sentiment polarity are not always clear. Thus, any algorithm that deals with sentiment polarity should take emoticons into account but extreme caution should be exercised in which emoticons to depend on. First, to demonstrate the prevalence of emoticons on social media, we analyzed the frequency of emoticons in a large recent Twitter data set. Then we carried out four analyses to examine the relationship between emoticons and sentiment polarity as well as the contexts in which emoticons are used. The first analysis surveyed a group of participants for their perceived sentiment polarity of the most frequent emoticons. The second analysis examined clustering of words and emoticons to better understand the meaning conveyed by the emoticons. The third analysis compared the sentiment polarity of microblog posts before and after emoticons were removed from the text. The last analysis tested the hypothesis that removing emoticons from text hurts sentiment classification by training two models with and without emoticons in the text, respectively. The results confirms the arguments that: 1) a few emoticons are strong and reliable signals of sentiment polarity and one should take advantage of them in any sentiment analysis; 2) a large group of the emoticons conveys complicated sentiment hence they should be treated with extreme caution.","","Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9","10.1109/BigData.2015.7364034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7364034","Twitter;emoticon;polarity;sentiment;social media","Big data;Context;Machine learning algorithms;Media;Reliability;Sentiment analysis;Twitter","pattern classification;pattern clustering;sentiment analysis;social networking (online)","NLP tasks;Twitter data set;emoticons;machine learning algorithms;microblog posts;sentiment analysis;sentiment classification;sentiment expression;sentiment lexicons;sentiment polarity;social media;words clustering","","3","","20","","","","Oct. 29 2015-Nov. 1 2015","","IEEE","IEEE Conferences"
"Research of K-MEANS analysis model on high-speed railway CIR device maintenance","J. m. Fan; J. p. Fan; F. Liu","School of Computer and Information Technology, Beijing Jiaotong University, 100044, China","2017 IEEE International Conference on Prognostics and Health Management (ICPHM)","20170803","2017","","","199","204","When High-speed train runs, the strength of data field signal determines whether the CIR device on the High-speed train can work normally. While proper functioning of the CIR device affects the normal operation of the train deeply. The K-MEANS-analysis-model proposed in this paper, applies the K-MEANS method of machine learning to train numerous data duplicate which generated by different locomotives that run in the same interval. In this way, we obtain the duplicate of normal field strength values in this interval. We employ it and combine with the mean-algorithm and the difference-algorithm to analysis the malfunction reason accurately. In this way we have reduced the run-times of the field inspection vehicle successfully, so as to cut down the operating costs effectively.","","Electronic:978-1-5090-5710-8; POD:978-1-5090-5711-5","10.1109/ICPHM.2017.7998328","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7998328","Big data;CIR device;Data Field strength;High-speed railway;Machine Learning","Algorithm design and analysis;Analytical models;Classification algorithms;Clustering algorithms;Communication systems;Feature extraction;Rail transportation","cellular radio;railways","cab integrated radio communication equipment;difference-algorithm;high-speed railway CIR device maintenance;k-means analysis model;machine learning;mean-algorithm;normal field strength values","","","","","","","","19-21 June 2017","","IEEE","IEEE Conferences"
"Fuzzy Based Scalable Clustering Algorithms for Handling Big Data Using Apache Spark","N. Bharill; A. Tiwari; A. Malviya","Department of Computer Science and Engineering, Indian Institute of Technology, Indore, India","IEEE Transactions on Big Data","20161220","2016","2","4","339","352","A huge amount of digital data containing useful information, called Big Data, is generated everyday. To mine such useful information, clustering is widely used data analysis technique. A large number of Big Data analytics frameworks have been developed to scale the clustering algorithms for big data analysis. One such framework called Apache Spark works really well for iterative algorithms by supporting in-memory computations, scalability etc. We focus on the design and implementation of partitional based clustering algorithms on Apache Spark, which are suited for clustering large datasets due to their low computational requirements. In this paper, we propose Scalable Random Sampling with Iterative Optimization Fuzzy c-Means algorithm (SRSIO-FCM) implemented on an Apache Spark Cluster to handle the challenges associated with big data clustering. Experimental studies on various big datasets have been conducted. The performance of SRSIO-FCM is judged in comparison with the proposed scalable version of the Literal Fuzzy c-Means (LFCM) and Random Sampling plus Extension Fuzzy c-Means (rseFCM) implemented on the Apache Spark cluster. The comparative results are reported in terms of time and space complexity, run time and measure of clustering quality, showing that SRSIO-FCM is able to run in much less time without compromising the clustering quality.","","","10.1109/TBDATA.2016.2622288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7723926","Apache Spark;Fuzzy clustering;big data;distributed machine learning;iterative computation","Algorithm design and analysis;Big data;Clustering algorithms;Data mining;Optimization;Partitioning algorithms;Sparks","Big Data;fuzzy set theory;iterative methods;optimisation;pattern clustering","Apache Spark;Big Data analytics frameworks;SRSIO-FCM;fuzzy based scalable clustering algorithms;partitional based clustering algorithms;scalable random sampling with iterative optimization fuzzy c-means algorithm;space complexity;time complexity","","3","","","","","20161027","Dec. 1 2016","","IEEE","IEEE Journals & Magazines"
"Analysis of Malware behavior: Type classification using machine learning","R. S. Pirscoveanu; S. S. Hansen; T. M. T. Larsen; M. Stevanovic; J. M. Pedersen; A. Czech","Aalborg University, Denmark","2015 International Conference on Cyber Situational Awareness, Data Analytics and Assessment (CyberSA)","20150727","2015","","","1","7","Malicious software has become a major threat to modern society, not only due to the increased complexity of the malware itself but also due to the exponential increase of new malware each day. This study tackles the problem of analyzing and classifying a high amount of malware in a scalable and automatized manner. We have developed a distributed malware testing environment by extending Cuckoo Sandbox that was used to test an extensive number of malware samples and trace their behavioral data. The extracted data was used for the development of a novel type classification approach based on supervised machine learning. The proposed classification approach employs a novel combination of features that achieves a high classification rate with a weighted average AUC value of 0.98 using Random Forests classifier. The approach has been extensively tested on a total of 42,000 malware samples. Based on the above results it is believed that the developed system can be used to pre-filter novel from known malware in a future malware analysis system.","","Electronic:978-0-9932-3380-7; POD:978-1-4673-6797-4","10.1109/CyberSA.2015.7166115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7166115","API call;Cuckoo sandbox;Malware;Random Forests;dynamic analysis;feature selection;scalability;supervised machine learning;type-classification","Data mining;Feature extraction;Testing;Training;Trojan horses;Vegetation","invasive software;learning (artificial intelligence);pattern classification","Cuckoo Sandbox;behavioral data;distributed malware testing environment;machine learning;malicious software;malware analysis system;malware behavior analysis;malware samples;random forest classifier;supervised machine learning;type classification approach;weighted average AUC value","","4","","16","","","","8-9 June 2015","","IEEE","IEEE Conferences"
"Pyramid: Enhancing Selectivity in Big Data Protection with Count Featurization","M. Lecuyer; R. Spahn; R. Geambasu; T. K. Huang; S. Sen","Columbia Univ., New York, NY, USA","2017 IEEE Symposium on Security and Privacy (SP)","20170626","2017","","","78","95","Protecting vast quantities of data poses a daunting challenge for the growing number of organizations that collect, stockpile, and monetize it. The ability to distinguish data that is actually needed from data collected “just in case” would help these organizations to limit the latter's exposure to attack. A natural approach might be to monitor data use and retain only the working-set of in-use data in accessible storage; unused data can be evicted to a highly protected store. However, many of today's big data applications rely on machine learning (ML) workloads that are periodically retrained by accessing, and thus exposing to attack, the entire data store. Training set minimization methods, such as count featurization, are often used to limit the data needed to train ML workloads to improve performance or scalability. We present Pyramid, a limited-exposure data management system that builds upon count featurization to enhance data protection. As such, Pyramid uniquely introduces both the idea and proof-of-concept for leveraging training set minimization methods to instill rigor and selectivity into big data management. We integrated Pyramid into Spark Velox, a framework for ML-based targeting and personalization. We evaluate it on three applications and show that Pyramid approaches state-of-the-art models while training on less than 1% of the raw data.","","Electronic:978-1-5090-5533-3; POD:978-1-5090-5534-0","10.1109/SP.2017.60","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7958572","","Big Data;Data models;Data protection;Lakes;Organizations;Training","Big Data;data protection;learning (artificial intelligence);security of data","Big Data protection;ML workloads;Pyramid;Spark Velox;count featurization;limited-exposure data management system;machine learning;selectivity enhancement","","","","","","","","22-26 May 2017","","IEEE","IEEE Conferences"
"Survey of scaling platforms for Deep Neural Networks","A. A. Ratnaparkhi; E. Pilli; R. C. Joshi","Department of Computer Science and Engineering, Graphic Era University, Dehradun, India","2016 International Conference on Emerging Trends in Communication Technologies (ETCT)","20170323","2016","","","1","6","Deep Neural Networks have become a state of the art approach in perception processing like speech recognition, image processing and natural language processing. Many state of the art benchmarks for these algorithms are using deep learning techniques. The deep neural networks in today's applications need to process very large amount of data. Different approaches have been proposed to solve scaling these algorithms. Few approach look for providing a solution over existing big data processing platform which usually runs over a large scale commodity cpu cluster. As training deep learning workload require many small computations to be done and large communication to pass the data between layers, General Purpose GPUs seems to the best platforms to train these networks. Different approaches have been proposed to scale processing on cluster of GPU servers. We have summarized various approaches used in this regard. The human brain is very good in processing perception and takes very little space and energy as compared to today's computing platforms. Neuromorphic hardware has been developed by different research groups to provide a brain like computing platform. We will look into IBM TrueNorth system in detail in this regard. Quantum computing gives another way to look in to this problem. Quantum computer can solve many complex problems as compared to classical computer. Though this field is still very nascent, people have suggested various ways to train neural network using quantum computers We will look in to recent quantum computers developed by different organizations like D-Wave and IBM. We will also look into state of the art proposed approaches to run deep neural network algorithms using quantum computer.","","Electronic:978-1-5090-4505-1; POD:978-1-5090-4506-8","10.1109/ETCT.2016.7882969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7882969","big data;deep learning;deep neural network;neuromorphic machines;quantum computing","Computer architecture;Computers;Data models;Machine learning;Neural networks;Quantum computing;Training","Big Data;brain;general purpose computers;graphics processing units;learning (artificial intelligence);neural nets;quantum computing","Big Data processing;deep learning;deep neural networks;general purpose GPUs;human brain;neuromorphic hardware;perception processing;quantum computing","","1","","","","","","18-19 Nov. 2016","","IEEE","IEEE Conferences"
"Apache spark based distributed self-organizing map algorithm for sensor data analysis","M. Jayaratne; D. Alahakoon; D. De Silva; X. Yu","Research Centre for Data Analytics and Cognition, La Trobe University, Victoria, Australia","IECON 2017 - 43rd Annual Conference of the IEEE Industrial Electronics Society","20171218","2017","","","8343","8349","The proliferation of Internets of Things (IoT) technologies in both industrial and non-industrial settings has led to the accumulation of Big Data sets. Analysis of these high-volume, high-velocity datasets require advanced processing techniques that incorporate parallel and distributed computations. In this paper, we present a novel distributed self-adaptive neural-network algorithm, the Distributed Growing Self-Organizing Map (DGSOM) algorithm to address the growing need for unsupervised machine learning of Big Data sets on distributed computing environments. The algorithm was tested on a Big Data set of sensor recordings of human activity collected from wearable devices, 2.8 million records. Results indicate that the distributed algorithm significantly reduces execution time compared to its serial counterpart. Moreover, the self-adaptive nature and controlled growth of the algorithm demonstrates data-driven structure adaptation and multi-granular pattern analysis. Overall, the proposed algorithm addresses the need for pattern discovery and visualization from Big Data sets generated by IoT devices which are increasingly commonplace in industrial scenarios.","","Electronic:978-1-5386-1127-2; POD:978-1-5386-1128-9; USB:978-1-5386-1126-5","10.1109/IECON.2017.8217465","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8217465","Big Data analysis;Growing Self-Organizing Maps;Industrial IoT;Internet of Things;Resilient Distributed Dataset;Unsupervised Machine Learning","Algorithm design and analysis;Big Data;Clustering algorithms;Neurons;Partitioning algorithms;Self-organizing feature maps;Sparks","Big Data;Internet of Things;data analysis;data visualisation;distributed algorithms;self-organising feature maps;sensor fusion;unsupervised learning","Apache spark based distributed self-organizing map algorithm;Big Data set;DGSOM;Distributed Growing Self-Organizing Map algorithm;Internets of Things;IoT technologies;data-driven structure adaptation;distributed algorithm;distributed computations;distributed computing environments;high-velocity datasets;high-volume datasets;human activity sensor recordings;multi-granular pattern analysis;novel distributed self-adaptive neural-network algorithm;parallel computations;pattern discovery;sensor data analysis;unsupervised machine learning;visualization;wearable devices","","","","","","","","Oct. 29 2017-Nov. 1 2017","","IEEE","IEEE Conferences"
"Automatic Analysis of Large Data Sets: A Walk-Through on Methods from Different Perspectives","M. Hilbrich; M. Weber; R. Tschüter","Center for Inf. Services & High Performance Comput., Tech. Univ. Dresden, Dresden, Germany","2013 International Conference on Cloud Computing and Big Data","20140526","2013","","","373","380","Analyzing data is one of today's hot topics. A complete list of fields of research and buzzwords associated with automatic analysis would extend beyond this document. The importance of this topic stems from the amount of data currently produced in research, engineering, and other fields. The size of these data sets renders manual analysis infeasible. Automatic analysis methods are required to cope with the data sets produced. The algorithms for filtering, categorization, and analysis have a long tradition and are manifold. This raises the question for the best algorithm. The authors of this paper give an overview about manifold automatic analysis approaches along with a classification of these approaches with regard to three different fields of research.","","Electronic:978-1-4799-2830-9; POD:978-1-4799-2831-6","10.1109/CLOUDCOM-ASIA.2013.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6821018","analysis;clustering;comparison;correlation;data mining;genetic algorithm;graph search;machine learning;monitoring;resilience;sequence alignment;visualization","Bioinformatics;Clustering algorithms;Genomics;Heuristic algorithms;Monitoring;Sociology;Statistics","data analysis;pattern classification","categorization;classification;filtering;large data set automatic analysis;manifold automatic analysis approach","","0","","71","","","","16-19 Dec. 2013","","IEEE","IEEE Conferences"
"Serendipity of Sharing: Large-Scale Measurement and Analytics for Device-to-Device (D2D) Content Sharing in Mobile Social Networks","X. Wang; H. Wang; K. Li; S. Yang; T. Jiang","Tianjin Key Lab. of Adv. Networking, Tianjin Univ., Tianjin, China","2017 14th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)","20170703","2017","","","1","9","The heavy multimedia traffic produced by mobile users poses great challenges for the mobile network operators, especially in the areas with large user densities but limited cellular network capacities (e.g. India). Recently, many studies demonstrate that exploiting the device-to- device(D2D) content sharing in offline Mobile Social Networks is a promising solution to cellular data offloading. However, such approaches are based on either unrealistic assumptions, or limited data analytics caused by small data size (e.g. hundreds of MSN users) or single-dimensional feature (e.g. human mobility only), which severely restricts their applications in practice. To address this issue, this paper performs the first large-scale data measurement and multi-feature analytics of D2D content sharing. Specifically, by using Apache Spark over a 20-server cluster, we analyze the behaviors of 30 million users (with 40 billion D2D transmissions and 16 million content files) of Xender, a leading global D2D sharing platform. Several important features are studied, including performance basics, content properties, location relations, meeting dynamics, and social characteristics. Furthermore, as a proof-of-concept study of our analytics, we also develop a multi-feature learning based framework, which demonstrates the large potentials of predicting and recommending D2D sharing activities using machine learning methods.","","Electronic:978-1-5090-6599-8; POD:978-1-5090-6600-1","10.1109/SAHCN.2017.7964925","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7964925","","Cellular networks;Device-to-device communication;Mobile computing;Multimedia communication;Social network services;Videos","cellular radio;learning (artificial intelligence);mobile computing;mobile radio;multimedia communication;social networking (online);telecommunication traffic","Apache Spark;D2D content sharing;Xender;cellular data offloading;cellular network capacities;data analytics;data measurement;device-to-device content sharing;machine learning methods;mobile network operators;mobile social networks;mobile users;multifeature analytics;multifeature learning;multimedia traffic;user densities","","","","","","","","12-14 June 2017","","IEEE","IEEE Conferences"
"Data science for software engineering","T. Menzies; E. Kocaguneli; F. Peters; B. Turhan; L. L. Minku","Lane Department of CS&EE, West Virginia University, Morgantown, WV, USA","2013 35th International Conference on Software Engineering (ICSE)","20130926","2013","","","1484","1486","Target audience: Software practitioners and researchers wanting to understand the state of the art in using data science for software engineering (SE). Content: In the age of big data, data science (the knowledge of deriving meaningful outcomes from data) is an essential skill that should be equipped by software engineers. It can be used to predict useful information on new projects based on completed projects. This tutorial offers core insights about the state-of-the-art in this important field. What participants will learn: Before data science: this tutorial discusses the tasks needed to deploy machine-learning algorithms to organizations (Part 1: Organization Issues). During data science: from discretization to clustering to dichotomization and statistical analysis. And the rest: When local data is scarce, we show how to adapt data from other organizations to local problems. When privacy concerns block access, we show how to privatize data while still being able to mine it. When working with data of dubious quality, we show how to prune spurious information. When data or models seem too complex, we show how to simplify data mining results. When data is too scarce to support intricate models, we show methods for generating predictions. When the world changes, and old models need to be updated, we show how to handle those updates. When the effect is too complex for one model, we show how to reason across ensembles of models. Pre-requisites: This tutorial makes minimal use of maths of advanced algorithms and would be understandable by developers and technical managers.","0270-5257;02705257","Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6","10.1109/ICSE.2013.6606752","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606752","","Data mining;Data models;Educational institutions;Predictive models;Software;Software engineering;Tutorials","data handling;data mining;data privacy;learning (artificial intelligence);pattern clustering;software engineering;statistical analysis","big data;data clustering;data dichotomization;data discretization;data mining;data privacy;data science;machine-learning algorithms;software engineering;software practitioners;software researchers;statistical analysis","","1","","18","","","","18-26 May 2013","","IEEE","IEEE Conferences"
"Sparsity-based representation for categorical data","R. Menon; S. S. Nair; K. Srindhya; M. R. Kaimal","Dept. of Comput. Sci. & Eng., Amrita Vishwa Vidyapeetham, Kollam, India","2013 IEEE Recent Advances in Intelligent Computational Systems (RAICS)","20140220","2013","","","74","79","Over the past few decades, many algorithms have been continuously evolving in the area of machine learning. This is an era of big data which is generated by different applications related to various fields like medicine, the World Wide Web, E-learning networking etc. So, we are still in need for more efficient algorithms which are computationally cost effective and thereby producing faster results. Sparse representation of data is one giant leap toward the search for a solution for big data analysis. The focus of our paper is on algorithms for sparsity-based representation of categorical data. For this, we adopt a concept from the image and signal processing domain called dictionary learning. We have successfully implemented its sparse coding stage which gives the sparse representation of data using Orthogonal Matching Pursuit (OMP) algorithms (both Batch and Cholesky based) and its dictionary update stage using the Singular Value Decomposition (SVD). We have also used a preprocessing stage where we represent the categorical dataset using a vector space model based on the TF-IDF weighting scheme. Our paper demonstrates how input data can be decomposed and approximated as a linear combination of minimum number of elementary columns of a dictionary which so formed will be a compact representation of data. Classification or clustering algorithms can now be easily performed based on the generated sparse coded coefficient matrix or based on the dictionary. We also give a comparison of the dictionary learning algorithm when applying different OMP algorithms. The algorithms are analysed and results are demonstrated by synthetic tests and on real data.","","CD-ROM:978-1-4799-2177-5; Electronic:978-1-4799-2178-2; POD:978-1-4799-4960-1","10.1109/RAICS.2013.6745450","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6745450","OMP;SVD;cholesky decomposition;dictionary learning;sparse coding;sparse representation","Algorithm design and analysis;Dictionaries;Encoding;Matching pursuit algorithms;Matrix decomposition;Sparse matrices;Vectors","Big Data;data structures;encoding;learning (artificial intelligence);matrix algebra;pattern classification;pattern clustering;singular value decomposition;vectors","Big Data analysis;Cholesky based algorithm;OMP algorithms;SVD;TF-IDF weighting scheme;batch based algorithm;categorical dataset;classification algorithm;clustering algorithm;compact data representation;dictionary learning algorithm;dictionary update stage;generated sparse coded coefficient matrix;machine learning;orthogonal matching pursuit algorithms;singular value decomposition;sparse coding stage;sparse data representation;sparsity-based representation;vector space model","","1","","15","","","","19-21 Dec. 2013","","IEEE","IEEE Conferences"
"Building a research data science platform from industrial machines","F. C. Liu; F. Shen; D. H. Chau; N. Bright; M. Belgin","Partnership for An Advanced Computing Environment (PACE), Georgia Institute of Technology, Atlanta, Georgia 30332-0250","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","2270","2275","Data Science research has a long history in academia which spans from large-scale data management, to data mining and data analysis using technologies from database management systems (DBMS's). While traditional HPC offers tools on leveraging existing technologies with data processing needs, the large volume of data and the speed of data generation pose significant challenges. Using the Hadoop platform and tools built on top of it drew immense interest from academia after it gained success in industry. Georgia Institute of Technology received a donation of 200 compute nodes from Yahoo. Turning these industrial machines into a research Data Science Platform (DSP) poses unique challenges, such as: nontrivial hardware design decisions, configuration tool choices, node integration into existing HPC infrastructure, partitioning resource to meet different application needs, software stack choices, etc. We have 40 nodes up and running, 24 running as a Hadoop and Spark cluster, 12 running as a HBase and OpenTSDB cluster, the others running as service nodes. We successfully tested it against Spark Machine Learning algorithms using a 88GB image dataset, Spark DataFrame and GraphFrame with a Wikipedia dataset, and Hadoop MapReduce wordcount on a 300GB dataset. The OpenTSDB cluster is for real-time time series data ingestion and storage for sensor data. We are working on bringing up more nodes. We share our first-hand experience gained in our journey, which we believe will benefit and inspire other academic institutions.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840859","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840859","Data Science Platform;Hadoop;Machine Learning;Software and Hardware Configuration;Spark","Cloud computing;Digital signal processing;Distributed databases;Ecosystems;Hardware;Sparks","Web sites;data analysis;data handling;data mining;database management systems;learning (artificial intelligence);parallel processing;pattern clustering","DBMS;DSP;Georgia Technology Institute;GraphFrame;HBase;HPC;Hadoop MapReduce;Hadoop platform;OpenTSDB cluster;Spark DataFrame;Spark cluster;Spark machine learning algorithms;Wikipedia dataset;Yahoo;data analysis;data generation speed;data management;data mining;data processing;data science research;database management systems;image dataset;industrial machines;research data science platform;sensor data storage","","1","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conferences"
"Table of contents","","","2013 IEEE 13th International Conference on Data Mining Workshops","20140306","2013","","","v","xvi","The following topics are dealt with: astroinformatics; biology computing; health care; causal discovery; data mining for service; biomedical informatics; data mining in networks; domain driven data mining; experimental economics; machine learning; high dimensional data mining; incremental clustering; concept drift; novelty detection; knowledge discovery; cloud computing; distributed computing; Big Data; market of data; optimisation; data privacy; text analysis; information retrieval; information extraction; spatial data mining; and spatio-temporal data mining.","2375-9232;23759232","Electronic:978-1-4799-3142-2; POD:978-1-4799-3144-6","10.1109/ICDMW.2013.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6753875","","","Big Data;astronomy computing;biology computing;cloud computing;computer networks;data mining;distributed processing;health care;information retrieval;learning (artificial intelligence);medical computing;optimisation;pattern clustering;text analysis","Big Data;astroinformatics;biology computing;biomedical informatics;causal discovery;cloud computing;computer networks;concept drift;data market;data privacy;distributed computing;domain driven data mining;experimental economics;health care;high dimensional data mining;incremental clustering;information extraction;information retrieval;knowledge discovery;machine learning;novelty detection;optimisation;spatial data mining;spatio-temporal data mining;text analysis","","0","","","","","","7-10 Dec. 2013","","IEEE","IEEE Conferences"
"Next generation data classification and linkage: Role of probabilistic models and artificial intelligence","G. P. Hettiarachchi; N. N. Hettiarachchi; D. S. Hettiarachchi; A. Ebisuya","Department of Physics, Osaka University, Japan","IEEE Global Humanitarian Technology Conference (GHTC 2014)","20141204","2014","","","569","576","Data classification and linkage is the task of identifying information corresponding to the same entity from one or more data sources. Methods used to tackle data classification and linkage problems fall into two broad categories. One commonly used method is deterministic models, in which sets of often very complex rules are used to classify pairs of entities as links. The other is the probabilistic model, in which statistical or probabilistic approaches are used to classify pairs. However, these models fail to deliver when there are lots of missing values, typographical errors, non-standardized entities, etc. To this end, intelligent routines making use of artificial neural networks, genetic algorithms and clustering algorithms can provide the next generation models for data classification and linkage. An introduction to data linkage, impact on humanity and community, current models, associated pitfalls, new directions and issues both technical and social for next generation data classification and linkage systems are discussed using an example prototype. A new model for linkage is proposed, where it is highlighted that not only the relationships between attributes of different entities, but also identification of relationships within the attributes of an entity is important in handling missing values and can provide better accuracy.","","Electronic:978-1-4799-7193-0; POD:978-1-4799-7194-7","10.1109/GHTC.2014.6970340","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6970340","Big data;classification;data linkage;machine learning;phonetic matching;probabilistic models;string comparison","Accuracy;Artificial neural networks;Couplings;Data models;Joining processes;Next generation networking;Probabilistic logic","genetic algorithms;learning (artificial intelligence);neural nets;pattern classification;probability","artificial intelligence;artificial neural networks;clustering algorithms;complex rules;data sources;deterministic models;entity attributes;genetic algorithms;information identification;missing value handling;next generation data classification;next generation data linkage;probabilistic approach;probabilistic model;relationship identification;social issues;statistical approach;technical issues","","0","","12","","","","10-13 Oct. 2014","","IEEE","IEEE Conferences"
"Super-CWC and super-LCC: Super fast feature selection algorithms","K. Shin; T. Kuboyama; T. Hashimot; D. Shepard","University of Hyogo, Kobe, Japan","2015 IEEE International Conference on Big Data (Big Data)","20151228","2015","","","1","7","Feature selection is a useful tool for identifying which features, or attributes, of a dataset cause or explain phenomena, and improving the efficiency and accuracy of learning algorithms for discovering such phenomena. Consequently, feature selection has been studied intensively in machine learning research. However, advanced feature selection algorithms that can avoid redundant selection of features and can detect interacting features require heavy computation in general and hence are seldom used for big data analysis. To eliminate this limitation, we tried to improve the run-time performance of two of the most advanced feature selection algorithms known in the literature. We have developed two accurate and extremely fast algorithms, namely Super CWC and Super LCC. In experiments with multiple real datasets which are actually studied in big data research, we have demonstrated that our algorithms improve the performance of their original algorithms remarkably. For example, for two datasets, one with 15,568 instances and 15,741 features and another with 200,569 instances and 99,672 features, Super-CWC performed feature selection in 1.4 seconds and in 405 seconds, respectively. This is a remarkable improvement, because it is estimated that the original algorithms would need several hours to a few ten days to perform feature selection on the same datasets.","","Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9","10.1109/BigData.2015.7363742","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363742","categorical feature selection;machine learning;supervised learning","Algorithm design and analysis;Big data;Clustering algorithms;Feature extraction;Machine learning algorithms;Mutual information;Redundancy","Big Data;data mining;learning (artificial intelligence)","Super-CWC;Super-LCC;big data;learning algorithm;machine learning;super fast feature selection algorithm","","5","","16","","","","Oct. 29 2015-Nov. 1 2015","","IEEE","IEEE Conferences"
"Cross-database mammographic image analysis through unsupervised domain adaptation","D. Kumar; C. Kumar; M. Shao","Department of Data Science, University of Massachusetts Dartmouth, Dartmouth, MA, USA","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","4035","4042","World Health Organization report shows 519,000 deaths due to breast cancer in 2014 and it was much more in 2008. Therefore, it is required to take early steps in detection and diagnosis of breast cancer to decrease the associated death rate. Computer Aided Diagnosis (CAD) is useful in mass screening of breast cancer datasets. Data mining and machine learning technologies have already achieved significant success in many knowledge engineering areas including classification, regression and clustering, and most recently, have been employed to assist the diagnosis of cancers with promising outcomes. Traditional machine learning models are characterized by training and testing data with the same input feature space and data distribution. But when distribution changes, most machine learning models need to be modified or rebuilt from scratch to work on newly collected data. In many real world applications, it is expensive or impossible to recollect the needed data and rebuild the models. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred as Transfer Learning. In this paper, we explore the usage of transfer learning, specially, unsupervised domain adaptation for breast cancer diagnosis to address the issues of fewer training data on target image dataset. On the strength of recent developed deep descriptors, we are able to adapt recent transfer learning methodologies, e.g., TCA (Transfer Component Analysis), CORAL (Correlation Alignment), BDA(Balanced Distribution Adaptation) to breast cancer diagnosis across multiple mammographic image databases including CBIS-DDSM, InBreast, MIAS, etc, and evaluate their performance. Experiments demonstrate that, without any labels in the target database, transfer learning is able to help improve the classification accuracy.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258419","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258419","Breast cancer diagnosis;deep learning;mammographic image analysis;transfer learning","Breast cancer;Data models;Databases;Mammography;Solid modeling;Training","CAD;cancer;data mining;feature extraction;image classification;learning (artificial intelligence);mammography;medical image processing","BDA;Balanced Distribution Adaptation;CORAL;Computer Aided Diagnosis;Correlation Alignment;TCA;Transfer Component Analysis;World Health Organization report;associated death rate;breast cancer datasets;breast cancer diagnosis;clustering;cross-database mammographic image analysis;machine learning technologies;multiple mammographic image databases;regression;target image dataset;testing data;traditional machine learning models;training data;transfer learning;unsupervised domain adaptation;world applications","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"The Berkeley Data Analytics Stack (BDAS)","Jayati","Impetus Infotech Pvt. Ltd., Indore, India","2014 Conference on IT in Business, Industry and Government (CSIBIG)","20150312","2014","","","1","1","Summary form only given. The session on “The Berkeley Data Analytics Stack” shall elucidate its current components which include Spark, Shark and Mesos with emphasis on Spark and it's real-time extension called Spark-Streaming which adds stream processing capabilities to Spark. One-liners describing each of these technologies are as follows: 1) BDAS is an open source, next-generation data analytics stack under development at the UC Berkeley AMPLab. 2) Spark, a high-speed cluster computing system compatible with Hadoop that can outperform it by up to 100x thanks to its ability to perform computations in memory. 3) Shark, a port of Apache Hive onto Spark that is compatible with existing Hive warehouses and queries. Shark can answer HiveQL queries up to 100x faster than Hive without modification to the data and queries, and is also open source as part of BDAS. 4) Mesos is a cluster manager that provides efficient resource isolation and sharing across distributed applications or frameworks. It can run Hadoop, MPI, Hypertable, Spark, and other applications on a dynamically shared pool of nodes. 5) Apart, from an elaborate explanation of various facets of Spark, the session would also aim to walk through machine learning algorithm benchmarking and examples that would substantiate the concepts covered.","","CD-ROM:978-1-4799-3062-3; Electronic:978-1-4799-3064-7; POD:978-1-4799-3065-4","10.1109/CSIBIG.2014.7056925","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056925","","Abstracts;Sparks","application program interfaces;data analysis;data warehouses;learning (artificial intelligence);message passing;pattern clustering;public domain software;query processing","Apache Hive;BDAS;Berkeley data analytics stack;Hadoop;Hive queries;Hive warehouses;HiveQL queries;Hypertable;MPI;Mesos cluster manager;Shark;Spark-streaming;high-speed cluster computing system;machine learning algorithm benchmarking;open source next-generation data analytics stack","","1","","","","","","8-9 March 2014","","IEEE","IEEE Conferences"
"Bio-inspired clustering: Basic features and future trends in the era of Big Data","D. Camacho","Computer Science Department, Universidad Aut&#x00F3;noma de Madrid, Applied Intelligence & Data Analysis Group, Spain","2015 IEEE 2nd International Conference on Cybernetics (CYBCONF)","20150806","2015","","","1","6","Clustering is perhaps one of the most popular approaches used in unsupervised machine learning. There's a huge number of different methods and algorithms that have been designed in the last decades related to this “blind pattern search”, some of these approaches are based on bio-inspired methods such as Evolutionary Computation, Swarm Intelligence or Neural Networks among others. In the last years, and due to the fast growing of Big Data problems, some interesting advances and new approaches are currently being developed in this area, new algorithms like online clustering and streaming clustering are appearing. These new algorithms try to solve classical problems in Clustering and deal with the new features of these new kind of problems. This keynote lecture will provide some basics on both, Clustering methods and bio-inspired computation, and how they have been combined to improve the quality of these algorithms, to later show the main features that Big Data needs to obtain reliable clustering approaches. Finally, some practical examples and applications will be described to show how these new algorithms are evolving to be used in the near future in complex and dynamic environments.","","Electronic:978-1-4799-8322-3; POD:978-1-4799-8323-0","10.1109/CYBConf.2015.7175897","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7175897","Big Data methods and applications;Bio-inspired algorithms;Clustering algorithms","Algorithm design and analysis;Big data;Clustering algorithms;Data analysis;Data mining;Particle swarm optimization","Big Data;evolutionary computation;neural nets;pattern clustering;swarm intelligence;unsupervised learning","big data problems;bio-inspired clustering;bio-inspired computation;blind pattern search;evolutionary computation;neural networks;online clustering;streaming clustering;swarm intelligence;unsupervised machine learning","","0","","58","","","","24-26 June 2015","","IEEE","IEEE Conferences"
"HcBench: Methodology, development, and characterization of a customer usage representative big data/Hadoop benchmark","V. A. Saletore; K. Krishnan; V. Viswanathan; M. E. Tolentino","Intel Corp., USA","2013 IEEE International Symposium on Workload Characterization (IISWC)","20140109","2013","","","77","86","Big Data analytics using Map-Reduce over Hadoop has become a leading edge paradigm for distributed programming over large server clusters. The Hadoop platform is used extensively for interactive and batch analytics in ecommerce, telecom, media, retail, social networking, and being actively evaluated for use in other areas. However, to date no industry standard or customer representative benchmarks exist to measure and evaluate the true performance of a Hadoop cluster. Current Hadoop micro-benchmarks such as HiBench-2, GridMix-3, Terasort, etc. are narrow functional slices of applications that customers run to evaluate their Hadoop clusters. However, these benchmarks fail to capture the real usages and performance in a datacenter environment. Given that typical datacenter deployments of Hadoop process a wide variety of analytic interactive and query jobs in addition to batch transform jobs under strict Service Level Agreement (SLA) requirements, performance benchmarks used to evaluate clusters must capture the effects of concurrently running such diverse job types in production environments. In this paper, we present the methodology and the development of a customer datacenter usage representative Hadoop benchmark ""HcBench"" which includes a mix of large number of customer representative interactive, query, machine learning, and transform jobs, a variety of data sizes, and includes compute, storage 110, and network intensive jobs, with inter-job arrival times as in a typical datacenter environment. We present the details of this benchmark and discuss application level, server and cluster level performance characterization collected on an Intel Sandy Bridge Xeon Processor Hadoop cluster.","","Electronic:978-1-4799-0555-3; POD:978-1-4799-0554-6","10.1109/IISWC.2013.6704672","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6704672","Big Data;Hadoop Benchmark;Map-Reduce;Performance;Workload Characterization","Artificial neural networks;Benchmark testing;Lead","data analysis;distributed processing;file servers;learning (artificial intelligence);multiprocessing systems;query processing","GridMix-3;Hadoop microbenchmarks;HcBench;HiBench-2;Intel Sandy Bridge Xeon Processor Hadoop cluster;Map-Reduce;SLA;Terasort;analytic interactive jobs;big data analytics;customer datacenter usage representative Hadoop benchmark;customer usage representative big data;distributed programming;interjob arrival times;machine learning;network intensive jobs;query jobs;server clusters;service level agreement requirements;transform jobs","","2","","21","","","","22-24 Sept. 2013","","IEEE","IEEE Conferences"
"Table of contents","","","2014 Wireless Telecommunications Symposium","20140616","2014","","","1","3","The following topics are dealt with: diversity coded sensor networks; UE power saving; RRC semi-connected state; covert channels; 802.11e wireless networks; near field communication readers; multiparent hierarchical routing protocol; WSN; knapsack problems; radio resource allocation; high altitude platforms; Bayesian quantized network coding; generalized approximate message passing; anti-collision protocol; RFID networks; cognitive radio sensing; pseudo Wishart matrix eigenvalues; channel surfing; jammed single-hope wireless networks; interference-limited cellular systems; GFSK noncoherent detection; extended Kalman filtering; non-Gaussian noise; AF cooperative networks; time-varying links; RF impairments; spectrum sensing; multimedia content delivery; remote patient monitoring; massive MIMO cellular systems; emergency response improvement; crowdsourced and sensor-detected information; data analytics; decode-and-forward relay systems; dual polarized antenna; heterogeneous wireless cellular networks; multichannel cooperative clustering-based MAC protocol; VANET; time-varying links; outage probability; opportunistic access; frequency hopping cognitive radio networks; CBA-EVT; traffic-adaptive energy-efficient MAC protocol; black hole attacks; node-resident expert systems; spectral reharvesting; 4G networks; VAMOS receiver design; wireless interventions; WiFi iLocate; Wifi based indoor localization; smart phone; IEEE 802.15.4e DSME; SIMD implementation; HEVC/H.265 video decoder; k-centers mean-shift reverse mean-shift clustering algorithm; many-to-all priority-based network-coding broadcast; wireless multihop networks; spatial diversity; partial band interference; undersea networks; distributed relay beamforming; fairness-aware amplify-and-forward relaying; correlated relay noise; fractional frequency reuse schemes; worst case signal to interference power ratio; OFDMA uplink; IEEE 802.11ac; lattice-based MMSE; zero forcing MIMO OFDM receivers; successive precoding; us- r selection; MU-MIMO broadcast channel; HPA nonlinearities mitigation; joint predistorter; constellation rotation; discrete constellations; Gaussian multiple access; Gaussian broadcast channel; cellular interference; S-band military radar systems; partial sensing coverage; wireless directional sensor networks; virtual hexagonal partition; generalized interference alignment; partial interference cancellation; multichannel grid networks; source-observation weighted fingerprinting; machine learning based localization; blind multiuser detection; fast relative Newton algorithm; wireless cardiac rhythm management system; radio protocol; LTE-Advanced; and service access integration.","1934-5070;19345070","Electronic:978-1-4799-1297-1; POD:978-1-4799-1295-7","10.1109/WTS.2014.6834988","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6834988","","","4G mobile communication;Gaussian processes;Kalman filters;Long Term Evolution;MIMO communication;OFDM modulation;access protocols;broadcast channels;cellular radio;cooperative communication;decode and forward communication;diversity reception;expert systems;frequency allocation;frequency hop communication;learning (artificial intelligence);least mean squares methods;matrix algebra;military radar;multifrequency antennas;multiuser detection;near-field communication;network coding;patient monitoring;pattern clustering;precoding;probability;radio receivers;radiofrequency identification;radiofrequency interference;relay networks (telecommunication);resource allocation;routing protocols;smart phones;time-varying channels;underwater acoustic communication;vehicular ad hoc networks;video coding;wireless LAN;wireless sensor networks","4G networks;802.11e wireless networks;AF cooperative networks;Bayesian quantized network coding;CBA-EVT;GFSK noncoherent detection;Gaussian broadcast channel;Gaussian multiple access;HEVC-H.265 video decoder;HPA nonlinearities mitigation;IEEE 802.11ac;IEEE 802.15.4e DSME;LTE-Advanced;MU-MIMO broadcast channel;OFDMA uplink;RF impairments;RFID networks;RRC semi-connected state;S-band military radar systems;SIMD implementation;UE power saving;VAMOS receiver design;VANET;WSN;WiFi iLocate;Wifi based indoor localization;anti-collision protocol;black hole attacks;blind multiuser detection;cellular interference;channel surfing;cognitive radio sensing;constellation rotation;correlated relay noise;covert channels;crowdsourced and sensor-detected information;data analytics;decode-and-forward relay systems;discrete constellations;distributed relay beamforming;diversity coded sensor networks;dual polarized antenna;emergency response improvement;extended Kalman filtering;fairness-aware amplify-and-forward relaying;fast relative Newton algorithm;fractional frequency reuse schemes;frequency hopping cognitive radio networks;generalized approximate message passing;generalized interference alignment;heterogeneous wireless cellular networks;high altitude platforms;interference-limited cellular systems;jammed single-hope wireless networks;joint predistorter;k-centers mean-shift reverse mean-shift clustering algorithm;knapsack problems;lattice-based MMSE;machine learning based localization;many-to-all priority-based network-coding broadcast;massive MIMO cellular systems;multichannel cooperative clustering-based MAC protocol;multichannel grid networks;multimedia content delivery;multiparent hierarchical routing protocol;near field communication readers;node-resident expert systems;non-Gaussian noise;opportunistic access;outage probability;partial band interference;partial interference cancellation;partial sensing coverage;pseudo Wishart matrix eigenvalues;radio protocol;radio resource allocation;remote patient monitoring;service access integration;smart phone;source-observation weighted fingerprinting;spatial diversity;spectral reharvesting;spectrum sensing;successive precoding;time-varying links;traffic-adaptive energy-efficient MAC protocol;undersea networks;user selection;virtual hexagonal partition;wireless cardiac rhythm management system;wireless directional sensor networks;wireless interventions;wireless multihop networks;worst case signal to interference power ratio;zero forcing MIMO OFDM receivers","","0","","","","","","9-11 April 2014","","IEEE","IEEE Conferences"
"Big data processing: Is there a framework suitable for economists and statisticians?","G. Bruno; D. Condello; A. Falzone; A. Luciani","Bank of Italy, Economics and statistics Directorate, Rome, Italy","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","2804","2811","The emerging wave of Big Data applications is flooding all branches of scientific knowledge. Economic and statistical applied research carried out in central banks and policy advising institutions is no exception. In this paper we present one of the most promising platform providing a unifying framework for different researchers willing to harness their knowledge of popular and simple computing environment such as R and Python. Along with their Integrated Development Environment (IDE), these are two of the most used numerical computing framework which are open source, provide built-in capabilities for statistical analysis and include a wide array of user contributed packages for an ample set of analytical tools suitable for different scientific applications. In the Big Data framework, we show how to provide researchers with a suitable programming environment allowing them to tame the intrinsic complexity of a High Performance Computing Cluster. Here we provide few empirical applications based on classical econometric and machine learning modeling.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258247","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258247","Apache Spark;Cluster;Distributed Memory;Resource sharing","Big Data;Econometrics;Processor scheduling;Production;Software;Sparks;Tools","Big Data;econometrics;learning (artificial intelligence);parallel processing;programming environments;statistical analysis","Big Data applications;Big Data framework;Big data processing;High Performance Computing Cluster;Integrated Development Environment;central banks;econometric modeling;economists;numerical computing framework;open source;programming environment;scientific applications;scientific knowledge;statistical analysis;statisticians","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"A novel fuzzy SVM based on fuzzy c-means for credit scoring","Shan Yang; Zhi Zhang","Key Laboratory of Universal Wireless Communications Ministry of Education, Wireless Technology Innovation Institute (WTI), Beijing University of Posts and Telecommunications, China","2016 2nd IEEE International Conference on Computer and Communications (ICCC)","20170511","2016","","","1349","1353","Credit scoring has become a very important issue in the financial industry. In order to identify good or bad credit applicants, many literatures with their proposed classification methods had been published to deal with this problem. The support vector machine (SVM) of statistical learning theory was successfully applied in classification. However it still suffers from noise sensitivity originating from the fact that all the data points are treated equally. To tackle this problem, the SVM was extended into a fuzzy SVM (FSVM) by the introduction of fuzzy memberships. In this paper, a new membership calculation method is proposed, which uses a fuzzy c-means (FCM) algorithm to deal with the classification problems with outliers or noises. In the FCM-FSVM algorithm, the training dataset is divided into many clusters by using FCM, then the membership of each of training points belongs to its class can be used to be the membership of FSVM. The proposed algorithm was applied to a credit scoring classification problem, and the results verified the effectiveness of the method.","","CD:978-1-4673-9025-5; Electronic:978-1-4673-9026-2; POD:978-1-4673-9027-9","10.1109/CompComm.2016.7924924","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7924924","credit scoring;data mining;fuzzy c-means(FCM);fuzzy support vactor machine(FSVM)","Classification algorithms;Clustering algorithms;Data mining;Machine learning algorithms;Support vector machines;Training;Training data","financial data processing;fuzzy set theory;learning (artificial intelligence);pattern classification;support vector machines","FCM-FSVM algorihm;credit scoring classification;financial industry;fuzzy SVM;fuzzy c-means algorithm;fuzzy memberships;membership calculation method;noise sensitivity;statistical learning theory;support vector machine;training dataset;training points","","","","","","","","14-17 Oct. 2016","","IEEE","IEEE Conferences"
"Image Denoising via Sparse Representation Over Grouped Dictionaries With Adaptive Atom Size","L. Jia; S. Song; L. Yao; H. Li; Q. Zhang; Y. Bai; Z. Gui","Shanxi Provincial Key Laboratory for Biomedical Imaging and Big Data, North University of China, Taiyuan, China","IEEE Access","20171107","2017","5","","22514","22529","The classic K-SVD based sparse representation denoising algorithm trains the dictionary only with one fixed atom size for the whole image, which is limited in accurately describing the image. To overcome this shortcoming, this paper presents an effective image denoising algorithm with the improved dictionaries. First, according to both geometrical and photometrical similarities, image patches are clustered into different groups. Second, these groups are classified into the flat category, the texture category, and the edge category. In different categories, the atom sizes of dictionaries are designed differently. Then, the dictionary of each group is trained with the atom size determined by the category that the group belongs to and the noisy level. Finally, the denoising method is presented by using sparse representation over the constructed grouped dictionaries with adaptive atom size. Experimental results show that the proposed method achieves better denoising performance than related denoising algorithms, especially in image structure preservation.","","","10.1109/ACCESS.2017.2762760","National Key Scientific Instrument and Equipment Development Project of China; Opening Project of State Key Laboratory of Explosion Science and Technology (Beijing Institute of Technology); Shanxi Province Science Foundation for Youths; 10.13039/501100001809 - National Natural Science Foundation of China; 10.13039/501100004480 - Natural Science Foundation of Shanxi Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8067458","Adaptive dictionary learning;K-SVD;image denoising;non-local grouping;sparse representation","Clustering algorithms;Dictionaries;Image denoising;Kernel;Machine learning;Noise level;Noise reduction","dictionaries;edge detection;image classification;image denoising;image representation;image texture;pattern clustering;singular value decomposition","adaptive atom size;classic K-SVD based sparse representation denoising algorithm;constructed grouped dictionaries;denoising method;denoising performance;dictionary only;edge category;effective image denoising algorithm;fixed atom size;flat category;geometrical similarities;image patches;image structure preservation;improved dictionaries;photometrical similarities;related denoising algorithms;texture category","","","","","","","20171013","2017","","IEEE","IEEE Journals & Magazines"
"Towards Noninvasive Hybrid Brain–Computer Interfaces: Framework, Practice, Clinical Application, and Beyond","G. Müller-Putz; R. Leeb; M. Tangermann; J. Höhne; A. Kübler; F. Cincotti; D. Mattia; R. Rupp; K. R. Müller; J. d. R. Millán","Lab. of Brain-Comput. Interfaces, Graz Univ. of Technol., Graz, Austria","Proceedings of the IEEE","20150601","2015","103","6","926","943","In their early days, brain-computer interfaces (BCIs) were only considered as control channel for end users with severe motor impairments such as people in the locked-in state. But, thanks to the multidisciplinary progress achieved over the last decade, the range of BCI applications has been substantially enlarged. Indeed, today BCI technology cannot only translate brain signals directly into control signals, but also can combine such kind of artificial output with a natural muscle-based output. Thus, the integration of multiple biological signals for real-time interaction holds the promise to enhance a much larger population than originally thought end users with preserved residual functions who could benefit from new generations of assistive technologies. A BCI system that combines a BCI with other physiological or technical signals is known as hybrid BCI (hBCI). In this work, we review the work of a large scale integrated project funded by the European commission which was dedicated to develop practical hybrid BCIs and introduce them in various fields of applications. This article presents an hBCI framework, which was used in studies with nonimpaired as well as end users with motor impairments.","0018-9219;00189219","","10.1109/JPROC.2015.2411333","BMBF project ¿Adaptive BCI¿; Berlin Big Data Center; Bernstein Focus Neurotechnology; BrainLinks-BrainTools, Cluster of Excellence; European FP7 project TOBI; German Research Foundation; 10.13039/501100001321 - National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7109824","Assistive technology;communication;electroencephalogram;hybrid brain–computer interface (hBCI);hybrid brain–computer interface (hBCI);neuroprosthesis","Assistive technology;Bayes methods;Brain-computer interfaces;Computer interfaces;Electroencephalography;Electromyography;Electronic mail;Neuroprosthesis","assisted living;brain-computer interfaces;electroencephalography;medical disorders;medical signal processing;muscle;prosthetics","BCI system;BCI technology;European commission;artificial output;assistive technologies;control channel;control signals;end users;hBCI framework;locked-in state;multidisciplinary progress;multiple biological signal integration;natural muscle-based output;noninvasive hybrid brain-computer interfaces;physiological signals;practical hybrid BCI;preserved residual functions;real-time interaction;severe motor impairments;technical signals","","28","","91","","","20150518","June 2015","","IEEE","IEEE Journals & Magazines"
"Empirical data analysis: A new tool for data analytics","P. Angelov; X. Gu; D. Kangin; J. Principe","Data Science Group, School of Computing and Communications, Lancaster University, UK","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","000052","000059","In this paper, a novel empirical data analysis approach (abbreviated as EDA) is introduced which is entirely data-driven and free from restricting assumptions and pre-defined problem- or user-specific parameters and thresholds. It is well known that the traditional probability theory is restricted by strong prior assumptions which are often impractical and do not hold in real problems. Machine learning methods, on the other hand, are closer to the real problems but they usually rely on problem- or user-specific parameters or thresholds making it rather art than science. In this paper we introduce a theoretically sound yet practically unrestricted and widely applicable approach that is based on the density in the data space. Since the data may have exactly the same value multiple times we distinguish between the data points and unique locations in the data space. The number of data points, k is larger or equal to the number of unique locations, l and at least one data point occupies each unique location. The number of different data points that have exactly the same location in the data space (equal value), f can be seen as frequency. Through the combination of the spatial density and the frequency of occurrence of discrete data points, a new concept called multimodal typicality, τ<sup>MM</sup> is proposed in this paper. It offers a closed analytical form that represents ensemble properties derived entirely from the empirical observations of data. Moreover, it is very close (yet different) from the histograms, from the probability density function (pdf) as well as from fuzzy set membership functions. Remarkably, there is no need to perform complicated pre-processing like clustering to get the multimodal representation. Moreover, the closed form for the case of Euclidean, Mahalanobis type of distance as well as some other forms (e.g. cosine-based dissimilarity) can be expressed recursively making it applicable to data streams and online algorithms.- Inference/estimation of the typicality of data points that were not present in the data so far can be made. This new concept allows to rethink the very foundations of statistical and machine learning as well as to develop a series of anomaly detection, clustering, classification, prediction, control and other algorithms.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844219","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844219","data-driven;empirical data analysis;estimation;inference;multimodal typicality;recursive calculation","Conferences;Cybernetics;Data analysis;Histograms;Meteorology;Probability density function;Temperature distribution","data analysis;estimation theory;fuzzy set theory;inference mechanisms;learning (artificial intelligence);pattern classification;probability","τMM;EDA;data analytics tool;data points;data space;data streams;empirical data analysis;estimation;fuzzy set membership functions;inference;machine learning;multimodal typicality;online algorithms;pdf;probability density function;probability theory;unique locations","","10","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conferences"
"Parallel computation of information gain using Hadoop and MapReduce","E. Zdravevski; P. Lameski; A. Kulakov; S. Filiposka; D. Trajanov; B. Jakimovskik","Faculty of Computer Science and Engineering, Ss.Cyril and Methodius University, Skopje, Macedonia","2015 Federated Conference on Computer Science and Information Systems (FedCSIS)","20151109","2015","","","181","192","Nowadays, companies collect data at an increasingly high rate to the extent that traditional implementation of algorithms cannot cope with it in reasonable time. On the other hand, analysis of the available data is a key to the business success. In a Big Data setting tasks like feature selection, finding discretization thresholds of continuous data, building decision threes, etc are especially difficult. In this paper we discuss how a parallel implementation of the algorithm for computing the information gain can address these issues. Our approach is based on writing Pig Latin scripts that are compiled into MapReduce jobs which then can be executed on Hadoop clusters. In order to implement the algorithm first we define a framework for developing arbitrary algorithms and then we apply it for the task at hand. With intent to analyze the impact of the parallelization, we have processed the FedCSIS AAIA'14 dataset with the proposed implementation of the information gain. During the experiments we evaluate the speedup of the parallelization compared to a one-node cluster. We also analyze how to optimally determine the number of map and reduce tasks for a given cluster. To demonstrate the portability of the implementation we present results using an on-premises and Amazon AWS clusters. Finally, we illustrate the scalability of the implementation by evaluating it on a replicated version of the same dataset which is 80 times larger than the original.","","Electronic:978-8-3608-1065-1; POD:978-1-4799-6747-6; USB:978-8-3608-1067-5","10.15439/2015F89","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7321440","Hadoop;MapReduce;feature ranking;information gain;parallelization","Entropy;Loading;Machine learning algorithms;Mathematical model;Parallel processing;Servers;Writing","Big Data;information dissemination;parallel processing","Amazon AWS cluster;Big data;FedCSIS AAIA'14 dataset;Hadoop;MapReduce;Pig Latin script;information gain;parallel computation","","2","","46","","","","13-16 Sept. 2015","","IEEE","IEEE Conferences"
"Data Mining Applied to Oil Well Using K-Means and DBSCAN","C. Lu; Y. Shi; Y. Chen; S. Bao; L. Tang","Sch. of Inf. & Electron., Beijing Inst. of Technol., Beijing, China","2016 7th International Conference on Cloud Computing and Big Data (CCBD)","20170717","2016","","","37","40","Oil is essential to our life mainly in transportation, and thus the productivity of oil well is very important. Classification of oil wells can make it easier to manage wells to ensure good oil productivity. Machine learning is an emerging technology of analyzing data in which cluster is a good way to do classification. The paper will apply two kinds of cluster method to the data from Dagang oil well and then do analysis on not only the classification results but also the choice of method for future analysis.","","Electronic:978-1-5090-3555-7; POD:978-1-5090-3556-4","10.1109/CCBD.2016.018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7979876","DBSCAN;K-means;PCA;cluster;oil well","Classification algorithms;Clustering algorithms;Clustering methods;Libraries;Oils;Principal component analysis;Productivity","data analysis;data mining;learning (artificial intelligence);oil technology;pattern classification;pattern clustering;production engineering computing","DBSCAN;Dagang oil well;cluster method;data analysis;data mining;k-means;machine learning;oil productivity;oil well classification","","","","","","","","16-18 Nov. 2016","","IEEE","IEEE Conferences"
"Distributed Adaptive Model Rules for mining big data streams","A. T. Vu; G. De Francisci Morales; J. Gama; A. Bifet","Royal Institute of Technology","2014 IEEE International Conference on Big Data (Big Data)","20150108","2014","","","345","353","Decision rules are among the most expressive data mining models. We propose the first distributed streaming algorithm to learn decision rules for regression tasks. The algorithm is available in SAMOA (Scalable Advanced Massive Online Analysis), an open-source platform for mining big data streams. It uses a hybrid of vertical and horizontal parallelism to distribute Adaptive Model Rules (AMRules) on a cluster. The decision rules built by AMRules are comprehensible models, where the antecedent of a rule is a conjunction of conditions on the attribute values, and the consequent is a linear combination of the attributes. Our evaluation shows that this implementation is scalable in relation to CPU and memory consumption. On a small commodity Samza cluster of 9 nodes, it can handle a rate of more than 30000 instances per second, and achieve a speedup of up to 4.7x over the sequential version.","","Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8","10.1109/BigData.2014.7004251","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004251","","Adaptation models;Data mining;Data models;Heat-assisted magnetic recording;Machine learning algorithms;Parallel processing;Predictive models","Big Data;data mining;public domain software","SAMOA;Samza cluster;big data stream mining;decision rules;distributed AMRules;distributed adaptive model rules;distributed streaming algorithm;expressive data mining models;open-source platform;scalable advanced massive online analysis","","3","","14","","","","27-30 Oct. 2014","","IEEE","IEEE Conferences"
"Guest Editorial Special Section on Fuzzy Systems in Data Science","J. Lu; F. Herrera; G. Zhang","University of Technology Sydney, Ultimo, NSW, Australia","IEEE Transactions on Fuzzy Systems","20171129","2017","25","6","1373","1375","The papers in this special section address the use of fuzzy systems in data science. Fuzzy Systems in Data Science data science employs theories and techniques drawn from many fields to achieve knowledge extraction from large volumes of data. To facilitate and automate the achievement of useful insights, predictions, and decisions from collected data sets, scientists and engineers turned to Machine Learning solutions. Currently, with the birth of Big Data, the set of opportunities for inquiry has grown exponentially, thanks to these large and complex data sets. Being able to exploit this massive data effectively provides useful knowledge for decision-making or the exploration and comprehension of the phenomenon that produced the data. Data science includes several problems and tasks depending on the nature of the data and the type of knowledge that is to be extracted. From predictive and descriptive analysis to the application of recommendation systems, scientists and practitioners have considered the use of Soft Computing techniques based on fuzzy sets to overcome their problems. Fuzzy approaches have been applied with much success in several areas related to Data Science. Fuzzy sets can be used to effectively describe and incorporate uncertain data values, data measurement, and data relations; fuzzy systems can directly aid reasoning and inferencing in a learning machine and can be combined with machine learning methodologies to model human behaviors and complex data systems.","1063-6706;10636706","","10.1109/TFUZZ.2017.2770960","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8124115","","Approximation algorithms;Classification algorithms;Clustering algorithms;Data models;Data science;Fuzzy systems;Special issues and sections;Support vector machines","","","","","","","","","","Dec. 2017","","IEEE","IEEE Journals & Magazines"
"A Multivariate Clustering Approach for Infrastructure Failure Predictions","S. Luo; V. W. Chu; J. Zhou; F. Chen; R. K. Wong; W. Huang","DATA61, CSIRO, Eveleigh, NSW, Australia","2017 IEEE International Congress on Big Data (BigData Congress)","20170911","2017","","","274","281","Infrastructure failures have severe consequences which often have a negative impact on the society and the economy. In this paper, we propose a machine learning model to assist in risk management to minimise the cost of infrastructure maintenance. Due to the vast volume and complexity of infrastructure datasets, such problem is often computationally expensive to compute. A Bayesian nonparametric approach has been selected for this problem, as it is highly scalable. We propose a two-stage approach to model failures, such as water pipe failures. The first stage uses an Infinite Gamma-Poisson Mixture Model to group water pipes with similar characteristics together based on the number of failures. The second stage uses the groups created in the first stage as an input to the Hierarchical Beta Process (HBP) to rank water pipes based on their probability of failure. The proposed method is applied to a metropolitan water supply network of a major city. The experiment results have shown that the proposed approach is able to adapt to the complexity of tge large multivariate dataset and there is a double-digit improvement from the grouping created by domain experts.","","Electronic:978-1-5386-1996-4; POD:978-1-5386-1997-1; USB:978-1-5386-1995-7","10.1109/BigDataCongress.2017.42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029335","Big Data;Clustering;Dirichlet Process;Hierarchical Beta Process;Infrastructure Failure Prediction;Sparse Data;Water Pipe Failure Prediction","Adaptation models;Bayes methods;Data models;Mathematical model;Mixture models;Predictive models;Water","Bayes methods;failure analysis;learning (artificial intelligence);mixture models;nonparametric statistics;pattern clustering;pipelines;stochastic processes;water supply","Bayesian nonparametric approach;HBP;double-digit improvement;failure probability;hierarchical beta process;infinite gamma-Poisson mixture model;infrastructure failure prediction;infrastructure maintenance cost minimisation;machine learning model;metropolitan water supply network;multivariate clustering approach;risk management;tge large multivariate dataset;two-stage approach;water pipe failures;water pipe ranking;water pipes","","","","","","","","25-30 June 2017","","IEEE","IEEE Conferences"
"Clustering analysis of malware behavior using Self Organizing Map","R. S. Pirscoveanu; M. Stevanovic; J. M. Pedersen","Department of Electronic Systems, Aalborg University, Denmark","2016 International Conference On Cyber Situational Awareness, Data Analytics And Assessment (CyberSA)","20160704","2016","","","1","6","For the time being, malware behavioral classification is performed by means of Anti-Virus (AV) generated labels. The paper investigates the inconsistencies associated with current practices by evaluating the identified differences between current vendors. In this paper we rely on Self Organizing Map, an unsupervised machine learning algorithm, for generating clusters that capture the similarities between malware behavior. A data set of approximately 270,000 samples was used to generate the behavioral profile of malicious types in order to compare the outcome of the proposed clustering approach with the labels collected from 57 Antivirus vendors using VirusTotal. Upon evaluating the results, the paper concludes on shortcomings of relying on AV vendors for labeling malware samples. In order to solve the problem, a cluster-based classification is proposed, which should provide more accurate results based on the clusters created by competitive and cooperative algorithms like Self Organizing Map that better describe the behavioral profile of malware.","","Electronic:978-1-5090-0703-5; POD:978-1-5090-0704-2","10.1109/CyberSA.2016.7503289","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7503289","Anti-Virus labels;Behavioral Clustering;Dynamic Analysis;Malware;Self Organizing Map","Clustering algorithms;Labeling;Machine learning algorithms;Trojan horses;Unsupervised learning","invasive software;learning (artificial intelligence);pattern clustering;self-organising feature maps","AV generated labels;VirusTotal;antivirus generated labels;behavioral profile;clustering analysis;malicious types;malware behavioral classification;malware samples;self organizing map;unsupervised machine learning algorithm","","1","","17","","","","13-14 June 2016","","IEEE","IEEE Conferences"
"Online Processing of Social Media Data for Emergency Management","D. Pohl; A. Bouchachia; H. Hellwagner","Inst. of Inf. Technol., Alpen-Adria-Univ. Klagenfurt, Klagenfurt, Austria","2013 12th International Conference on Machine Learning and Applications","20140410","2013","1","","408","413","Social media offers an opportunity for emergency management to identify issues that need immediate reaction. To support the effective use of social media, an analysis approach is needed to identify crisis-related hotspots. We consider in this investigation the analysis of social media (i.e., Twitter, Flickr and YouTube) to support emergency management by identifying sub-events. Sub-events are significant hotspots that are of importance for emergency management tasks. Aiming at sub-event detection, recognition and tracking, the data is processed online in real-time. We introduce an incremental feature selection mechanism to identify meaningful terms and use an online clustering algorithm to uncover sub-events on-the-fly. Initial experiments are based on tweets enriched with Flickr and YouTube data collected during Hurricane Sandy. They show the potential of the proposed approach to monitor sub-events for real-world emergency situations.","","CD-ROM:978-1-4799-4154-4; Electronic:978-0-7695-5144-9; POD:978-1-4799-4155-1","10.1109/ICMLA.2013.83","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784653","Crisis Management;Online Clustering;Sub-Event Detection","Clustering algorithms;Feature extraction;Hurricanes;Internet;Media;Twitter;YouTube","emergency management;feature selection;pattern clustering;social networking (online)","Flickr;Hurricane Sandy;Twitter;YouTube;crisis-related hotspot identification;data processing;emergency management;incremental feature selection mechanism;online clustering algorithm;social media data;subevent identification;tweets","","0","","24","","","","4-7 Dec. 2013","","IEEE","IEEE Conferences"
"Opinion extracting and classification from questionnaire comments using HMM-POS Tagger and machine learning techniques","A. Hamzah; N. Widyastuti","Department of Informatics Engineering, Institute of Science and Technology AKPRIND, Yogyakarta, Indonesia","2016 International Conference on Data and Software Engineering (ICoDSE)","20170601","2016","","","1","6","Measurement of academic services using questionnaires with multiple choice answers generally provide comments and advice columns. In results data analysis, comments and suggestions given by the thousands of student cannot be utilized due to the lack of analysis tools. Whereas comments and suggestions actually contain student opinions on various things, such as facilities, faculties, library and others. Opinion mining and sentiment analysis as a new tool in text mining can be applied to utilize comments and suggestions. This research has applied HMM-POS Tagger providing automatic TAG POS to the comments data on the training data using Hidden Markov techniques. By implementing the HMM-POS Tagger, opinion can be extracted from the comments. Furthermore if the comment is opinion, by using rule-based, it can be determined the target of the opinion and also the orientation of the opinion whether it is positive or negative. The data used was 1,000 comments given POS-TAG manually and 500 comments set as test data. Sentiment analysis is applied using four methods of classification, namely SVM, NBC, ME and KM-Clustering. The results showed that HMM-POS Tagger get precision of 0.95 for the detection of opinion and 0.91 for target detection. In the opinion classification results showed accuracy of SVM, NBC, ME and KM-Clustering are 0.84; 0.83; 0.84 and 0.88 respectively.","","Electronic:978-1-5090-5671-2; POD:978-1-5090-5672-9","10.1109/ICODSE.2016.7936121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7936121","HMM-POS Tagger;KM-Clustering;ME;NBC;SVM","Blogs;Colon;Hidden Markov models;Manuals","computer aided instruction;data mining;educational administrative data processing;hidden Markov models;learning (artificial intelligence);pattern classification;pattern clustering;sentiment analysis","HMM-POS tagger;KM-clustering;ME;NBC;SVM;TAG POS;academic services;advice columns;hidden Markov techniques;machine learning techniques;multiple choice answers;opinion classification;opinion extracting;opinion mining;questionnaire comments;sentiment analysis;student opinions;text mining","","","","","","","","26-27 Oct. 2016","","IEEE","IEEE Conferences"
"Large-scale strategic games and adversarial machine learning","T. Alpcan; B. I. P. Rubinstein; C. Leckie","Department of Electrical and Electronic Engineering, The University of Melbourne, Australia","2016 IEEE 55th Conference on Decision and Control (CDC)","20161229","2016","","","4420","4426","Decision making in modern large-scale and complex systems such as communication networks, smart electricity grids, and cyber-physical systems motivate novel game-theoretic approaches. This paper investigates big strategic (non-cooperative) games where a finite number of individual players each have a large number of continuous decision variables and input data points. Such high-dimensional decision spaces and big data sets lead to computational challenges, relating to efforts in non-linear optimization scaling up to large systems of variables. In addition to these computational challenges, real-world players often have limited information about their preference parameters due to the prohibitive cost of identifying them or due to operating in dynamic online settings. The challenge of limited information is exacerbated in high dimensions and big data sets. Motivated by both computational and information limitations that constrain the direct solution of big strategic games, our investigation centers around reductions using linear transformations such as random projection methods and their effect on Nash equilibrium solutions. Specific analytical results are presented for quadratic games and approximations. In addition, an adversarial learning game is presented where random projection and sampling schemes are investigated.","","DVD:978-1-5090-1844-4; Electronic:978-1-5090-1837-6; POD:978-1-5090-1838-3","10.1109/CDC.2016.7798940","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7798940","","Cost function;Game theory;Games;Kernel;Scalability;Security;Support vector machines","decision making;game theory;learning (artificial intelligence);random processes;sampling methods","Nash equilibrium;adversarial machine learning;decision making;random projection;sampling scheme;strategic game","","","","","","","","12-14 Dec. 2016","","IEEE","IEEE Conferences"
"Apache Flink: Stream Analytics at Scale","A. Katsifodimos; S. Schelter","Tech. Univ. Berlin, Berlin, Germany","2016 IEEE International Conference on Cloud Engineering Workshop (IC2EW)","20160804","2016","","","193","193","Summary form only given. Apache Flink is an open source system for expressive, declarative, fast, and efficient data analysis on both historical (batch) and real-time (streaming) data. Flink combines the scalability and programming flexibility of distributed MapReduce-like platforms with the efficiency, out-of-core execution, and query optimization capabilities found in parallel databases. At its core, Flink builds on a distributed dataflow runtime that unifies batch and incremental computations over a true-streaming pipelined execution. Its programming model allows for stateful, fault tolerant computations, flexible user-defined windowing semantics for streaming and unique support for iterations. Flink is converging into a use-case complete system for parallel data processing with a wide range of top level libraries ranging from machine learning through to graph processing. Apache Flink originates from the Stratosphere project led by TU Berlin and has led to various scientific papers (e.g., in VLDBJ, SIGMOD, (P)VLDB, ICDE, and HPDC). In this half-day tutorial we will introduce Apache Flink, and give a tutorial on its streaming capabilities using concrete examples of application scenarios, focusing on concepts such as stream windowing, and stateful operators.","","Electronic:978-1-5090-3684-4; POD:978-1-5090-3685-1","10.1109/IC2EW.2016.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7527842","","Data analysis;Distributed databases;Electronic mail;Programming;Query processing;Terrestrial atmosphere;Tutorials","data analysis;parallel databases;parallel programming;pipeline processing;public domain software;query processing","Apache Flink;Stratosphere project;batch computations;data analysis;distributed MapReduce-like platforms;distributed dataflow runtime;fault tolerant computations;flexible user-defined windowing semantics;graph processing;historical data;incremental computations;machine learning;open source system;parallel data processing;parallel databases;programming flexibility;query optimization capabilities;real-time streaming data;true-streaming pipelined execution;use-case complete system","","","","","","","","4-8 April 2016","","IEEE","IEEE Conferences"
"Java thread and process performance for parallel machine learning on multicore HPC clusters","S. Ekanayake; S. Kamburugamuve; P. Wickramasinghe; G. C. Fox","School of Informatics and Computing, Indiana University, Bloomington","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","347","354","The growing use of Big Data frameworks on large machines highlights the importance of performance issues and the value of High Performance Computing (HPC) technology. This paper looks carefully at three major frameworks Spark, Flink and Message Passing Interface (MPI) both in scaling across nodes and internally over the many cores inside modern nodes. We focus on the special challenges of the Java Virtual Machine (JVM) using an Intel Haswell HPC cluster with 24 cores per node. Two parallel machine learning algorithms, K-Means clustering and Multidimensional Scaling (MDS) are used in our performance studies. We identify three major issues - thread models, affinity patterns, and communication mechanisms - as factors affecting performance by large factors and show how to optimize them so that Java can match the performance of traditional HPC languages like C. Further we suggest approaches that preserve the user interface and elegant dataflow approach of Flink and Spark but modify the runtime so that these Big Data frameworks can achieve excellent performance and realize the goals of HPC-Big Data convergence.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840622","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840622","Big Data;HPC;Java;Machine Learning;Multicore","Big data;Data models;Instruction sets;Java;Message systems;Parallel machines;Sparks","Big Data;Java;learning (artificial intelligence);message passing;multiprocessing systems;operating systems (computers);parallel processing;performance evaluation;virtual machines","Big Data frameworks;Flink;HPC technology;Intel Haswell HPC cluster;JVM;Java thread;Java virtual machine;K-Means clustering;MDS;MPI;Spark;communication mechanisms;dataflow approach;high performance computing;message passing interface;multicore HPC clusters;multidimensional scaling;parallel machine learning;parallel machine learning algorithms;process performance;user interface","","3","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conferences"
"On the architecture of a big data classification tool based on a map reduce approach for hyperspectral image analysis","V. A. Ayma; R. S. Ferreira; P. N. Happ; D. A. B. Oliveira; G. A. O. P. Costa; R. Q. Feitosa; A. Plaza; P. Gamba","Dept. of Electrical Engineering, Pontifical Catholic University of Rio de Janeiro, Brazil","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","20151112","2015","","","1508","1511","Advances in remote sensors are providing exceptional quantities of large-scale data with increasing spatial, spectral and temporal resolutions, raising new challenges in its analysis, e.g. those presents in classification processes. This work presents the architecture of the InterIMAGE Cloud Platform (ICP): Data Mining Package; a tool able to perform supervised classification procedures on huge amounts of data, on a distributed infrastructure. The architecture is implemented on top of the MapReduce framework. The tool has four classification algorithms implemented taken from WEKA's machine learning library, namely: Decision Trees, Naïve Bayes, Random Forest and Support Vector Machines. The SVM classifier was applied on datasets of different sizes (2 GB, 4 GB and 10 GB) for different cluster configurations (5, 10, 20, 50 nodes). The results show the tool as a potential approach to parallelize classification processes on big data.","2153-6996;21536996","Electronic:978-1-4799-7929-5; POD:978-1-4799-7930-1; USB:978-1-4799-7928-8","10.1109/IGARSS.2015.7326066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326066","Big Data;Classification Algorithms;Cloud Computing;MapReduce","Big data;Classification algorithms;Cloud computing;Computer architecture;Data mining;Iterative closest point algorithm;Training","Bayes methods;Big Data;cloud computing;data mining;decision trees;geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence);parallel processing;support vector machines","Big Data classification tool;ICP Data Mining Package;InterIMAGE Cloud Platform;MapReduce;SVM classifier;WEKA machine learning library;cluster configurations;decision trees;hyperspectral image analysis;naive Bayes;random forest;supervised classification;support vector machines","","","","16","","","","26-31 July 2015","","IEEE","IEEE Conferences"
"Managing Cloud Infrastructures by a Multi-layer Data Analytics","A. V. Poghosyan; A. N. Harutyunyan; N. M. Grigoryan","","2016 IEEE International Conference on Autonomic Computing (ICAC)","20160922","2016","","","351","356","Today's IT management faces the problem of “virtualized big environments” with hundreds of thousands of objects/resources as virtual machines, hosts, clusters, etc., evolving into cloud services. Admins of those infrastructures heavily rely on smart data-agnostic approaches to get reliable and accurate information regarding any current or upcoming health deterioration, increasingly requesting more proactive solutions. We architected a multi-layer enterprise analytics that employs statistical and machine learning methods to maximally automate the data center operations for an optimal performance management. We share several experience stories on application of the developed approaches and address noise and complexity reduction requirements to increase the operational efficiency of the analytics.","","Electronic:978-1-5090-1654-9; POD:978-1-5090-1655-6","10.1109/ICAC.2016.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7573164","abnormality degree;alarm ranking;alarming;alteration degree;complexity reduction;correlation analysis;data analytics;data categorization;data center automation;noise reduction;outlier detection;root cause;thresholding","Complexity theory;Data analysis;Entropy;Market research;Measurement;Monitoring;Time series analysis","DP management;cloud computing;computer centres;data analysis;learning (artificial intelligence);statistical analysis;virtualisation","IT management;analytics operational efficiency;cloud infrastructure managing;cloud service;clusters;complexity reduction requirement;data center operation automation;health deterioration;machine learning method;multilayer data analytics;multilayer enterprise analytics;optimal performance management;smart data-agnostic approach;statistical learning method;virtual machines;virtualized big environment","","2","","","","","","17-22 July 2016","","IEEE","IEEE Conferences"
"Fast Modeling of Analytics Workloads for Big Data Services","L. Yang; C. Li; L. Fan; J. Xu","IBM Res. - China, Beijing, China","2014 International Conference on Service Sciences","20151029","2014","","","101","105","Building models to predict analytics workloads' execution is a foundational capability that enables key scenarios for big data services, like SLA-driven service provisioning and elastic auto scaling. Given the various infrastructure and workload characteristics, it's more preferable to build the models in a ""black-box"" fashion, for example, by leveraging machine learning techniques. However, this approach has assumptions on the volume and quality of workloads' existing records to learn from, which require sophisticate benchmark or long time monitoring. In this paper, we present a method to accelerate the modeling process of an analytics workload for its quick time-to-value in the context of big data services. Specifically, clustering and transfer learning techniques are leveraged for this acceleration by shifting the data collection from the online service phase to the offline preparation phase. This paper focuses on the conceived service model and fast modeling techniques. Their feasibility is demonstrated by experiments.","2165-3828;21653828","CD-ROM:978-1-4799-4332-6; Electronic:978-1-4799-4330-2; POD:978-1-4799-4329-6","10.1109/ICSS.2014.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7312298","MapReduce;analytics;big data;cloud computing;machine learning;modeling;service","Acceleration;Analytical models;Benchmark testing;Big data;Data models;Mathematical model;Predictive models","Big Data;cloud computing;learning (artificial intelligence)","SLA-driven service;analytics workloads execution;big data services;black-box fashion;data collection;elastic auto scaling;machine learning;offline preparation phase;online service phase;transfer learning","","","","10","","","","22-23 May 2014","","IEEE","IEEE Conferences"
"Using LSTM networks to predict engine condition on large scale data processing framework","O. Aydin; S. Guldamlasioglu","Big Data Research and Development Group, STM Defense Technologies Engineering and Trade Inc., Ankara, Turkey","2017 4th International Conference on Electrical and Electronic Engineering (ICEEE)","20170601","2017","","","281","285","As the Internet of Things technology is developing rapidly, companies have an ability to observe the health of engine components and constructed systems through collecting signals from sensors. According to output of IoT sensors, companies can build systems to predict the conditions of components. Practically the components are required to be maintained or replaced before the end of life in performing their assigned task. Predicting the life condition of a component is so crucial for industries that have intent to grow in a fast paced technological environment. Recent studies on predictive maintenance help industries to create an alert before the components are corrupted. Thanks to prediction of component failures, companies have a chance to sustain their operations efficiently while reducing their maintenance cost by repairing components in advance. Since maintenance affects production capacity and the service quality directly, optimized maintenance is the key factor for organizations to have more revenue and stay competitive in developing industrialized world. With the aid of well-designed prediction system for understanding current situation of an engine, components could be taken out of active service before malfunction occurs. With the help of inspection, effective maintenance extends component life, improves equipment availability and keeps components in a proper condition while reducing costs. Real time data collected from sensors is a great source to model component deteriorations. Markov Chain models, Survival Analysis, Optimization algorithms and several machine learning approaches have been implemented in order to model predictive maintenance. In this paper Long Short Term Memory (LSTM) networks has been performed to predict the current situation of an engine. LSTM model deals with a sequential input data. Training process of LSTM networks has been performed on large-scale data processing engine with high performance. Since huge amount of data is flowing- into the predictive model, Apache Spark which is offering a distributed clustering environment has been used. The output of the LSTM network is deciding the current life condition of components and offering the alerts for components before the end of their life. The proposed model also trained and tested on an open source data that is about an engine degradation simulation provided by the Prognostics CoE at NASA Ames.","","Electronic:978-1-5090-6789-3; POD:978-1-5090-6790-9","10.1109/ICEEE2.2017.7935834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7935834","ANN;LSTM;apache spark;big data;predictive maintenance","Engines;Fans;Maintenance engineering;Predictive models;Sparks;Temperature sensors","Internet of Things;Markov processes;costing;optimisation","Internet of Things technology;LSTM model;LSTM networks;Markov chain models;NASA Ames;Prognostics CoE;engine components;engine condition;large scale data processing framework;loT sensors;maintenance cost;optimization algorithms;survival analysis;technological environment","","","","","","","","8-10 April 2017","","IEEE","IEEE Conferences"
"Carotene: A Job Title Classification System for the Online Recruitment Domain","F. Javed; Q. Luo; M. McNair; F. Jacob; M. Zhao; T. S. Kang","Data Sci. R&D, Atlanta, GA, USA","2015 IEEE First International Conference on Big Data Computing Service and Applications","20150813","2015","","","286","293","In the online job recruitment domain, accurate classification of jobs and resumes to occupation categories is important for matching job seekers with relevant jobs. An example of such a job title classification system is an automatic text document classification system that utilizes machine learning. Machine learning-based document classification techniques for images, text and related entities have been well researched in academia and have also been successfully applied in many industrial settings. In this paper we present Carotene, a machine learning-based semi-supervised job title classification system that is currently in production at CareerBuilder. Carotene leverages a varied collection of classification and clustering tools and techniques to tackle the challenges of designing a scalable classification system for a large taxonomy of job categories. It encompasses these techniques in a cascade classifier architecture. We first present the architecture of Carotene, which consists of a two-stage coarse and fine level classifier cascade. We compare Carotene to an early version that was based on a flat classifier architecture and also compare and contrast Carotene with a third party occupation classification system. The paper concludes by presenting experimental results on real world industrial data using both machine learning metrics and actual user experience surveys.","","Electronic:978-1-4799-8128-1; POD:978-1-4799-8129-8","10.1109/BigDataService.2015.61","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7184892","job title classification;machine learning;text classification","Accuracy;Computer architecture;Software;Support vector machines;System-on-chip;Taxonomy;Training","classification;learning (artificial intelligence);recruitment;text analysis","Carotene;automatic text document classification system;cascade classifier architecture;job classification;job seekers;job title classification system;machine learning-based document classification techniques;occupation categories;online recruitment domain;related entities;resumes;text entities","","7","","25","","","","March 30 2015-April 2 2015","","IEEE","IEEE Conferences"
"VHT: Vertical hoeffding tree","N. Kourtellis; G. De Francisci Morales; A. Bifet; A. Murdopo","Telefonica I&#x002B;D, Spain","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","915","922","IoT big data requires new machine learning methods able to scale to large size of data arriving at high speed. Decision trees are popular machine learning models since they are very effective, yet easy to interpret and visualize. In the literature, we can find distributed algorithms for learning decision trees, and also streaming algorithms, but not algorithms that combine both features. In this paper we present the Vertical Hoeffding Tree (VHT), the first distributed streaming algorithm for learning decision trees. It features a novel way of distributing decision trees via vertical parallelism. The algorithm is implemented on top of Apache SAMOA, a platform for mining big data streams, and thus able to run on real-world clusters. Our experiments to study the accuracy and throughput of VHT prove its ability to scale while attaining superior performance compared to sequential decision trees.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840687","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840687","Apache SAMOA;IoT;big data;distributed streaming decision tree;hoeffding tree;vertical parallelism","Big data;Decision trees;Machine learning algorithms;Parallel processing;Partitioning algorithms;Radiation detectors;Vegetation","Big Data;Internet of Things;decision trees;distributed algorithms;learning (artificial intelligence);parallel processing","Apache SAMOA;Big Data streams mining;Internet of Things;IoT Big Data;VHT;decision trees;distributed streaming algorithm;machine learning;vertical Hoeffding tree;vertical parallelism","","2","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conferences"
"Predictive Analytics on Facebook Data","J. A. S. Raj; J. L. Fernando; Y. S. Raj","Dept. of Comput. Sci. & Eng., Sastra Univ. Thanjavur, Thanjavur, India","2017 World Congress on Computing and Communication Technologies (WCCCT)","20171019","2017","","","93","96","Data in every organization are increasing at a rapid speed along with the volume of it increases at a large extend through various domains such as education, social network, meteorology, government and much more. Accordingly, Big Data refers to data as traditional data, Machine-generated Sensor data and Social data which are both structured and unstructured. Apache's Hadoop has proven to provide salient properties such as scalability, ease of use, and most notably robustness to node failures. Processing K-Nearest Neighbor queries in high dimensional data has received a lot of attention by researchers in recent years, which provides a way of predicting in a better way as done in this analysis. This research analysis is to find the means to enhance the classification accuracy, utilizing logistic regression and K-Nearest Neighbour a fast process technique in a machine learning technique for classification. The work aims at enhancing the rate of accuracy, true positive, false positive, precision, recall, sensitivity, specificity rate of classification using the HADOOP platform. As a source for analysis, political values are collected from social network users. The Prediction with K-NN neighbor approach is helpful to predict the patterns using opinion.","","CD:978-1-5090-5572-2; Electronic:978-1-5090-5573-9; POD:978-1-5090-5574-6","10.1109/WCCCT.2016.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8074500","Analyics;Big Data;KNN;Prediction","Big Data;Classification algorithms;Facebook;Logistics;Sensitivity;Voting","Big Data;data analysis;learning (artificial intelligence);pattern classification;query processing;regression analysis;social networking (online)","Apache Hadoop;Big Data;K-NN neighbor;Social data;classification accuracy;facebook Data;fast process technique;high dimensional data;k-nearest neighbor query processing;logistic regression;machine learning technique;machine-generated sensor data;pattern prediction;predictive analytics;social network users","","","","","","","","2-4 Feb. 2017","","IEEE","IEEE Conferences"
"Privacy-Preserving Multi-Party Analytics over Arbitrarily Partitioned Data","S. Mehnaz; E. Bertino","Purdue Univ., West Lafayette, IN, USA","2017 IEEE 10th International Conference on Cloud Computing (CLOUD)","20170911","2017","","","342","349","Data-driven business processes are gaining popularity among enterprises now-a-days. In many situations, multiple parties would share data towards a common goal if it were possible to simultaneously protect the privacy of the individuals and organizations described in the data. Existing solutions for multi-party analytics require parties to transfer their raw data to a trusted mediator, who then performs the desired analysis on the global data, and shares the results with the parties. Unfortunately, such a solution does not fit many applications where privacy is a strong concern such as healthcare, finance and the internet-of-things. Motivated by the increasing demands for data privacy, in this paper, we study the problem of privacy-preserving multi-party analytics, where the goal is to enable analytics on multi-party data without compromising the data privacy of each individual party. We propose a secure gradient descent algorithm that enables analytics on data that is arbitrarily partitioned among multiple parties. The proposed algorithm is generic and applies to a wide class of machine learning problems. We demonstrate our solution for a popular use-case (i.e., regression), and evaluate the performance of the proposed secure solution in terms of accuracy, latency and communication cost. We also perform a scalability analysis to evaluate the performance of the proposed solution as the data size and the number of parties increase.","","Electronic:978-1-5386-1993-3; POD:978-1-5386-1994-0; USB:978-1-5386-1992-6","10.1109/CLOUD.2017.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030607","Multi-party Analytics;Privacy","Cryptography;Data privacy;Hospitals;Machine learning algorithms;Organizations;Privacy;Protocols","business data processing;data privacy;learning (artificial intelligence);trusted computing","arbitrarily partitioned data;data privacy;data size;data-driven business processes;enterprises;global data;individual party;internet-of-things;machine learning problems;multiparty data;privacy-preserving multiparty analytics;raw data;secure gradient descent algorithm;trusted mediator","","","","","","","","25-30 June 2017","","IEEE","IEEE Conferences"
"Unbinds data and tasks to improving the Hadoop performance","K. Lu; D. Dai; X. Zhou; M. Sun; C. Li; H. Zhuang","Comput. Sci. & Technol, Univ. of Sci. & Technol. of China, Hefei, China","15th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)","20140901","2014","","","1","6","Hadoop is a popular framework that provides easy programming interface of parallel programs to process large scale of data on clusters of commodity machines. Data intensive programs are the important part running on the cluster especially in large scale machine learning algorithm which executes of the same program iteratively. In-memory cache of input data is an efficient way to speed up these data intensive programs. However, we cannot be able to load all the data in memory because of the limitation of memory capacity. So, the key challenge is how we can accurately know when data should be cached in memory and when it ought to be released. The other problem is that memory capacity may even not enough to hold the input data of the running program. This leads to there is some data cannot be cached in memory. Prefetching is an effective method for such situation. We provide a unbinding technology which do not put the programs and data binded together before the real computation start. With unbinding technology, Hadoop can get a better performance when using caching and prefetching technology. We provide a Hadoop framework with unbinding technology named unbinding-Hadoop which decide the map tasks' input data in the map starting up phase, not at the job submission phase. Prefetching as well can be used in unbinding-Hadoop and can get better performance compared with the programs without unbinding. Evaluations on this system show that unbinding-Hadoop reduces the execution time of jobs by 40.2% and 29.2% with WordCount programs and K-means algorithm.","","Electronic:978-1-4799-5604-3; POD:978-1-4799-5605-0","10.1109/SNPD.2014.6888710","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6888710","cache system;prefetch;unbinding","Acceleration;Clustering algorithms;Electronic mail;Hard disks;Prefetching;Programming;Software algorithms","cache storage;learning (artificial intelligence);parallel programming;pattern clustering;public domain software;software performance evaluation","Hadoop performance improvement;K-means algorithm;WordCount programs;caching technology;commodity machine clusters;data intensive programs;execution time reduction;large scale data processing;large scale machine learning algorithm;memory capacity limitation;parallel programs;prefetching technology;programming interface;unbinding technology","","1","1","20","","","","June 30 2014-July 2 2014","","IEEE","IEEE Conferences"
"Multi-objective K-means evolving spiking neural network model based on differential evolution","H. N. A. Hamed; A. Y. Saleh; S. M. Shamsuddin; A. O. Ibrahim","Soft Computing Research Group1, Universiti Teknologi Malaysia (UTM), Johor, Malaysia","2015 International Conference on Computing, Control, Networking, Electronics and Embedded Systems Engineering (ICCNEEE)","20160114","2015","","","379","383","In this paper, a multi-objective K-means evolving spiking neural network (MO-KESNN) model based on differential evolution for clustering problems has been presented. K-means has been utilized to improve the ESNN model. This model enhances the flexibility of the ESNN algorithm in producing better solutions which is used to overcome the disadvantages of K-means. Several standard data sets from UCI machine learning are used for evaluating the performance of this model. It has been found that MO-KESNN gives competitive results in clustering accuracy performance and the number of pre-synaptic neurons measure simultaneously compared to the standard K-means. More discussion is provided to prove the effectiveness of the new model in clustering problems.","","CD-ROM:978-1-4673-7867-3; Electronic:978-1-4673-7869-7; POD:978-1-4673-7870-3","10.1109/ICCNEEE.2015.7381395","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381395","K- means;MO- KESNN;Multi-objective","Clustering algorithms;Computational modeling;Neurons;Optimization;Sociology;Statistics","evolutionary computation;learning (artificial intelligence);neural nets;pattern clustering","MO-KESNN;UCI machine learning;clustering problems;differential evolution;multi-objective K-means evolving spiking neural network","","","","24","","","","7-9 Sept. 2015","","IEEE","IEEE Conferences"
"An architecture for efficiently establishing the classroom cloud","J. W. Rau; Y. S. Chen","Department of Electrical Engineering, Yuan Ze University, Chung-Li 32003, Taoyuan, Taiwan, ROC","2013 International Conference on Machine Learning and Cybernetics","20140908","2013","03","","1418","1422","Cloud computing is the key for Big Data analytics. Apache Hadoop makes it feasible to process the immense data set. An experimental environment with flexibility is required to do education and research with Big Data. However, this solution brings with the challenges that are how to manage large number of computers in the Hadoop cluster, and how to reuse the idle computer resources. In this paper, we propose the classroom cloud infrastructure to overcome the challenges. With this infrastructure, we can reuse the idle computer resources in the classroom for education and research. The case study of such a classroom cloud implemented in YZU is presented and discussed to help reader who can follow architecture to set up their personal Hadoop experimental environment in the classroom.","2160-133X;2160133X","Electronic:978-1-4799-0260-6; POD:978-1-4799-0259-0","10.1109/ICMLC.2013.6890805","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6890805","Classroom Cloud;Computer resource management;Hadoop;Virtual machine","Cloud computing;Computer architecture;Computers;Hardware;Servers;User interfaces;Virtual machining","Big Data;cloud computing;computer aided instruction;data analysis;public domain software","Apache Hadoop cluster;Big Data analytics;YZU;classroom cloud infrastructure;cloud computing;data set;education;idle computer resources;personal Hadoop experimental environment;research","","2","","14","","","","14-17 July 2013","","IEEE","IEEE Conferences"
"A prediction model based on Big Data analysis using hybrid FCM clustering","S. Yang; J. Kim; M. Chung","Dept. of Computer Eng., Pukyong National University Busan 608-737, Korea","The 9th International Conference for Internet Technology and Secured Transactions (ICITST-2014)","20150212","2014","","","337","339","The prediction models based on unsupervised learning are fast and need not have labeled data. However, the analysis for prediction is quite difficult, since no information about the data is given to us for learning. This paper proposes a prediction model based on Big Data analysis using hybrid FCM clustering algorithm to address these problems. The proposed model conducts automatic classification without external interference and shows the advantages of both supervised and unsupervised learning. We expect that the proposed model might contribute to enhance automation standards in various intelligent systems which need appropriate prediction using proposed framework, Co-Biz.","","Electronic:978-1-908320-39-1; POD:978-1-4799-1825-6; USB:978-1-908320-40-7","10.1109/ICITST.2014.7038833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7038833","Big Data Analysis;FCM Clustering;Framework;Machine Learning","Algorithm design and analysis;Classification algorithms;Clustering algorithms;Data models;Prediction algorithms;Predictive models;Unsupervised learning","Big Data;data analysis;pattern classification;pattern clustering;unsupervised learning","Big Data analysis;Co-Biz;automatic classification;hybrid FCM clustering algorithm;intelligent systems;prediction model;supervised learning;unsupervised learning","","0","","6","","","","8-10 Dec. 2014","","IEEE","IEEE Conferences"
"An Improved Incremental Training Approach for Large Scaled Dataset Based on Support Vector Machine","J. Guo","Grad. Sch. of Inf., Waseda Univ., Fukuoka, Japan","2016 IEEE/ACM 3rd International Conference on Big Data Computing Applications and Technologies (BDCAT)","20170316","2016","","","149","157","The Support Vector Machine(SVM) is well known in machine learning and artificial intelligence for its high performance in data classification, regression and forecasting. Usually for large scaled dataset, an incremental training algorithm is applied for tuning or balancing the training cost and the accuracy in SVM applications. This paper presents an improved incremental training approach for large scaled dataset on SVM. We focus on data's own distribution information to unfold our research, we proposed a self adaptive clustering method to extract the area and density information of data, a border detection technologies and uncertainty strategy is applied to maintain the border and some potential samples. Our proposed method can greatly reduce the training error for incremental training on SVM, especially for some uneven distribution dataset. We can greatly tuning or balancing the training cost and the accuracy of algorithms to achieve a better performance.","","Electronic:978-1-4503-4617-7; POD:978-1-5090-4468-9","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877062","Classification;Data distribution;Incremental learning;Self adaptive clustering;Uncertainty","Data mining;Organizations;Support vector machines;Training;Training data;Tuning;Uncertainty","learning (artificial intelligence);pattern classification;pattern clustering;regression analysis;support vector machines","SVM;artificial intelligence;border detection technologies;data classification;data distribution information;incremental training approach;large scaled dataset;machine learning;regression;self adaptive clustering method;support vector machine;uncertainty strategy","","","","","","","","6-9 Dec. 2016","","IEEE","IEEE Conferences"
"A Cloud System for Machine Learning Exploiting a Parallel Array DBMS","Y. Zhang; C. Ordonez; L. Johnsson","Dept. of Comput. Sci., Univ. of Houston, Houston, TX, USA","2017 28th International Workshop on Database and Expert Systems Applications (DEXA)","20170928","2017","","","22","26","Computing machine learning models in the cloud remains a central problem in big data analytics. In this work, we introduce a cloud analytic system exploiting a parallel array DBMS based on a classical shared-nothing architecture. Our approach combines in-DBMS data summarization with mathematical processing in an external program. We study how to summarize a data set in parallel assuming a large number of processing nodes and how to further accelerate it with GPUs. In contrast to most big data analytic systems, we do not use Java, HDFS, MapReduce or Spark: our system is programmed in C++ and C on top of a traditional Unix system. In our system, models are efficiently computed using a suite of innovative parallel matrix operators, which compute comprehensive statistical summaries of a large input data set (matrix) in one pass, leaving the remaining mathematically complex computations, with matrices that fit in RAM, to R. In order to be competitive with the Hadoop ecosystem (i.e. HDFS and Spark RDDs) we also introduce a parallel load operator for large matrices and an automated, yet flexible, cluster configuration in the cloud. Experiments compare our system with Spark, showing orders of magnitude time improvement. A GPU with many cores widens the gap further. In summary, our system is a competitive solution.","","Electronic:978-1-5386-1051-0; POD:978-1-5386-2207-0; USB:978-1-5386-1050-3","10.1109/DEXA.2017.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8049679","DBMS;GPU;cloud;machine learning;matrix;parallel","Arrays;Cloud computing;Computational modeling;Data models;Loading;Random access memory;Sparks","C++ language;Unix;cloud computing;data analysis;graphics processing units;learning (artificial intelligence);parallel databases","C;C++;GPU;Spark;Unix file system;cloud analytic system;cloud computing;in-DBMS data summarization;machine learning;mathematical processing;parallel array DBMS;parallel matrix operators;shared-nothing architecture","","","","","","","","28-31 Aug. 2017","","IEEE","IEEE Conferences"
"Graphine: Programming Graph-Parallel Computation of Large Natural Graphs for Multicore Clusters","J. Yan; G. Tan; Z. Mo; N. Sun","State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences","IEEE Transactions on Parallel and Distributed Systems","20160512","2016","27","6","1647","1659","Graph-parallel computation has become a crucial component in emerging applications of web search, data analytics and machine learning. In practice, most graphs derived from real-world phenomena are very large and scale-free. Unfortunately, distributed graph-parallel computation of these natural graphs still suffers strong scalability issues on contemporary multicore clusters. To embrace the multicore architecture in distributed graph-parallel computation, we propose the framework Graphine, which features (i) A Scatter-Combine computation abstraction that is evolved from the traditional vertex-centric approach by fusing the paired scatter and gather operations, executed separately on two edge sides, into a one-sided scatter. Further coupled with active message mechanism, it potentially reduces intermediate message cost and enables fine-grained parallelism on multicore architecture. (ii) An Agent-Graph data model, which leverages an idea similar to vertex-cut but conceptually splits the remote replica into two agent types of scatter and combiner, resulting in less communication. We implement the Graphine framework and evaluate it using several representative algorithms on six large real-world graphs and a series of synthetic graphs with power-law degree distributions. We show that Graphine achieves sublinear scalability with the number of cores per node, number of nodes, and graph sizes (up to one billion vertices), and is 2~15 times faster than the state-of-the-art PowerGraph on a cluster of 16 multicore nodes.","1045-9219;10459219","","10.1109/TPDS.2015.2453978","973 Program; National 863 Program; 10.13039/501100001809 - National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7152922","Computational Model;Graph-Parallel;Graph-parallel;Parallel Framework;computational model;parallel framework","Computational modeling;Data models;Mirrors;Multicore processing;Runtime;Scalability","multiprocessing systems;parallel programming","Graphine framework;Web search;active message mechanism;agent-graph data model;data analytics;graph-parallel computation programming;machine learning;multicore architecture;multicore clusters;natural graphs;paired scatter-and-gather operations;power-law degree distribution;scatter-combine computation abstraction;vertex-centric approach","","1","","49","","","20150708","June 1 2016","","IEEE","IEEE Journals & Magazines"
"Study on big data analytics research domains","S. Malgaonkar; S. Soral; S. Sumeet; T. Parekhji","Research and Development, Mulund, Mumbai, India","2016 5th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)","20161219","2016","","","200","206","Data Analytics is the trending domain that analyses data to observe patterns and predict future outcomes. The outcomes are based upon analysis of past and current trends and behaviors. Data analytics deals with both descriptive and predictive analyses of data. Descriptive Data Analytics summarizes the data, it's behavior and draws useful conclusion from it. Predictive Data Analytics is the branch of data analytics that predicts future outcomes based on the current and historical data. These future predictions are drawn by observing patterns followed for past data and outcomes for the past events for similar scenarios. In this paper, various branches of data analytics have been discussed. Big data analytics architecture gives an overview of the various tools and system structure involved in big data analytics. Big data analytics is closely related to data mining and hence, implements data mining algorithms. Latter part of the paper covers machine learning algorithms and neural networks for training the dataset to recognize patterns for the modeled data and predict outcomes based on the training and pattern recognition. Modeling of data using neural networks helps in generating accurate and exhaustive outcomes.","","Electronic:978-1-5090-1489-7; POD:978-1-5090-1490-3","10.1109/ICRITO.2016.7784952","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7784952","analytics;big data;classification;cloud;clustering;neural;regression","","Big Data;data analysis;data mining;learning (artificial intelligence);neural nets;pattern recognition","big data analytics;data mining;descriptive data analytics;future outcomes prediction;machine learning;neural networks;pattern recognition;predictive data analytics","","","","","","","","7-9 Sept. 2016","","IEEE","IEEE Conferences"
"Learning From Hidden Traits: Joint Factor Analysis and Latent Clustering","B. Yang; X. Fu; N. D. Sidiropoulos","Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN, USA","IEEE Transactions on Signal Processing","20161104","2017","65","1","256","269","Dimensionality reduction techniques play an essential role in data analytics, signal processing, and machine learning. Dimensionality reduction is usually performed in a preprocessing stage that is separate from subsequent data analysis, such as clustering or classification. Finding reduced-dimension representations that are well-suited for the intended task is more appealing. This paper proposes a joint factor analysis and latent clustering framework, which aims at learning cluster-aware low-dimensional representations of matrix and tensor data. The proposed approach leverages matrix and tensor factorization models that produce essentially unique latent representations of the data to unravel latent cluster structure-which is otherwise obscured because of the freedom to apply an oblique transformation in latent space. At the same time, latent cluster structure is used as prior information to enhance the performance of factorization. Specific contributions include several custom-built problem formulations, corresponding algorithms, and discussion of associated convergence properties. Besides extensive simulations, real-world datasets such as Reuters document data and MNIST image data are also employed to showcase the effectiveness of the proposed approaches.","1053-587X;1053587X","","10.1109/TSP.2016.2614491","10.13039/100000001 - National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7579562","Factor analysis;clustering;clustering prior;dimensionality reduction;factorial $K$ -means;identifiability;matrix factorization;reduced $K$ -means;subspace learning;tensor factorization","Algorithm design and analysis;Analytical models;Clustering algorithms;Convergence;Data models;Signal processing;Tensile stress","learning (artificial intelligence);matrix algebra;pattern clustering;signal processing","cluster structure;data analytics;dimensionality reduction techniques;hidden traits;joint factor analysis;latent clustering framework;learning;machine learning;matrix data;matrix factorization models;reduced dimension representations;signal processing;subsequent data analysis;tensor data;tensor factorization models","","","","","","","20160929","Jan.1, 1 2017","","IEEE","IEEE Journals & Magazines"
"Automated Medical Diagnosis from Clinical Data","V. S. Pendyala; S. Figueira","Dept. of Comput. Eng., Santa Clara Univ., Santa Clara, CA, USA","2017 IEEE Third International Conference on Big Data Computing Service and Applications (BigDataService)","20170612","2017","","","185","190","A significant portion of the world population does not have access to proper healthcare. The key factor for healthcare's success is the physician's expertise. In this paper, we examine if that expertise can be modeled as an information corpus, a flavor of Big Data and extracted using text mining techniques, particularly using the Vector Space Model, to perform diagnosis. Using cloud and mobile technologies, medical diagnosis can then be made available everywhere there is Internet connectivity, reducing costs, increasing coverage and improving quality of life. The key to the possibility of performing medical diagnosis using an information retrieval approach is the data. This paper therefore focuses on the suitability of the dataset for automating diagnosis using text mining. We use various text mining tools relevant to the Vector Space Model to perform operations on the data to see if meaningful conclusions can be drawn from it. We present some of our observations from the experiments conducted and conclude with future directions.","","Electronic:978-1-5090-6318-5; POD:978-1-5090-6319-2","10.1109/BigDataService.2017.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7944937","Cluster Analysis;Information Retrieval;K-means;Machine Learning;Medical Diagnosis;TF-IDF;Text Mining;Vector Space Model","Cloud computing;Diseases;Medical diagnosis;Medical diagnostic imaging;Text mining","Big Data;cloud computing;data mining;health care;information retrieval;medical information systems;mobile computing;patient diagnosis;text analysis","Big Data;Internet connectivity;automated medical diagnosis;clinical data;cloud technologies;healthcare;information corpus;information retrieval approach;mobile technologies;physician expertise;text mining techniques;text mining tools;vector space model;world population","","","","","","","","6-9 April 2017","","IEEE","IEEE Conferences"
"Machine learning at the limit","J. Canny; H. Zhao; B. Jaros; Y. Chen; J. Mao","UC Berkeley, Berkeley, CA 94720, USA","2015 IEEE International Conference on Big Data (Big Data)","20151228","2015","","","233","242","Many systems have been developed for machine learning at scale. Performance has steadily improved, but there has been relatively little work on explicitly defining or approaching the limits of performance. In this paper we describe the application of roofline design, an approach borrowed from computer architecture, to large-scale machine learning. In roofline design, one exposes ALU, memory, and network limits, and the constraints they imply for algorithms. Using roofline design, we have developed a system called BIDMach which has demonstrated the highest performance to date for many ML problems. On one GPU-accelerated node, it generally outperforms other single-machine toolkits and cluster toolkits running on 100s of nodes. This performance level is enabled by a relatively small number of rooflined matrix primitives. Such performance implies a dramatic reduction in the energy used to perform these calculations. Beyond matrix kernels, roofline design can be applied to the end-to-end design of machine learning algorithms which minimize memory usage to optimize speed. This approach offers a further 2x to 3x gain in performance. Roofline design can also be applied to network primitives. We describe recent work on a sparse allreduce primitive called Kylix. We have shown that Kylix approaches the practical network throughput limit for allreduce, a basic primitive for distributed machine learning. Using Kylix, we describe an efficient transformation from model-parallel to data-parallel calculations. This transformation uses a secondary storage roofline, with similar parameters to the network. Finally, we describe several deployments of these techniques on real-world problems in two large internet companies. Once again, single node rooflined design demonstrated substantial gains over alternatives on either single nodes or clusters.","","Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9","10.1109/BigData.2015.7363760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363760","Big Data;Distributed Systems;Scalable Machine Learning","Algorithm design and analysis;Benchmark testing;Graphics processing units;Machine learning algorithms;Scalability;Sparse matrices;Throughput","graphics processing units;learning (artificial intelligence);matrix algebra;parallel processing","ALU;BIDMach;GPU-accelerated node;Kylix;ML problems;data-parallel calculation;distributed machine learning;end-to-end design;matrix kernels;memory usage minimization;model-parallel calculation;rooflined matrix primitives;secondary storage roofline;single node rooflined design;sparse allreduce primitive","","2","","13","","","","Oct. 29 2015-Nov. 1 2015","","IEEE","IEEE Conferences"
"Solving cold-start problem in large-scale recommendation engines: A deep learning approach","J. Yuan; W. Shalaby; M. Korayem; D. Lin; K. AlJadda; J. Luo","Department of Computer Science, University of Rochester, New York","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","1901","1910","Collaborative Filtering (CF) is widely used in large-scale recommendation engines because of its efficiency, accuracy and scalability. However, in practice, the fact that recommendation engines based on CF require interactions between users and items before making recommendations, make it inappropriate for new items which haven't been exposed to the end users to interact with. This is known as the cold-start problem. In this paper we introduce a novel approach which employs deep learning to tackle this problem in any CF based recommendation engine. One of the most important features of the proposed technique is the fact that it can be applied on top of any existing CF based recommendation engine without changing the CF core. We successfully applied this technique to overcome the item cold-start problem in Careerbuilder's CF based recommendation engine. Our experiments show that the proposed technique is very efficient to resolve the cold-start problem while maintaining high accuracy of the CF recommendations.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840810","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840810","Cold-Start;Deep Learning;Document Similarity;Job Search;Recommendation System","Big data;Correlation;Electronic mail;Engines;Filtering;Machine learning;Semantics","collaborative filtering;learning (artificial intelligence);recommender systems","Careerbuilder CF based recommendation engine;collaborative filtering;deep learning;item cold-start problem;large-scale recommendation engines","","1","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conferences"
"An unsupervised fuzzy clustering method for twitter sentiment analysis","H. Suresh; Gladston Raj S.","School of Computer Sciences, Mahatma Gandhi University, Kottayam, India","2016 International Conference on Computation System and Information Technology for Sustainable Solutions (CSITSS)","20161212","2016","","","80","85","Cluster based techniques on sentiment analysis is a novel approach for analyzing sentiments expressed in social media sites. It is a main task of exploratory data mining, and a common technique used in machine learning. In contrast to supervised learning technique, the cluster based techniques produce essentially accurate experimental results without manual processing, linguistic knowledge or training time. This paper presents a novel fuzzy clustering model to analyze twitter feeds regarding the sentiments of a particular brand using the real dataset collected over a period of one year. Then a comparative analysis is made with the existing partitioning clustering techniques namely K Means and Expectation Maximization algorithms based on metrics namely accuracy, precision, recall and execution time. According to the experimental analysis, the proposed approach is tested to be practicable in performing high quality twitter sentiment analysis results.","","Electronic:978-1-5090-1022-6; POD:978-1-5090-1023-3; Paper:978-1-5090-1020-2; USB:978-1-5090-1021-9","10.1109/CSITSS.2016.7779444","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7779444","Expectation Maximization (EM);Sentiment Analysis (SA);Simple K-Means;partitioning clustering techniques","Adaptation models;Analytical models;Support vector machine classification;Tagging;Twitter;Uniform resource locators","data mining;expectation-maximisation algorithm;fuzzy set theory;marketing data processing;pattern clustering;sentiment analysis;social networking (online);unsupervised learning","K means clustering;Twitter feeds;expectation-maximization algorithms;exploratory data mining;high quality Twitter sentiment analysis;machine learning;partitioning clustering;social media sites;unsupervised fuzzy clustering method","","","","","","","","6-8 Oct. 2016","","IEEE","IEEE Conferences"
"Online machine learning for cloud resource provisioning of microservice backend systems","H. Alipour; Y. Liu","Concordia University, Electronic and Computer Engineering, Montreal, Canada","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","2433","2441","Microservices are bundled and generating traffic on the backend systems that need to scale on demand. When microservices generate variant and unexpected, the challenge is to classify the workload on the backend systems and adjust the scaling policy to reflect the resource demand timely and accurately. In this paper, we propose a microservice architecture that encapsulates functions of monitoring metrics and learning workload pattern. Then this service architecture is used to predict the future workload for decision making on resource provisioning. We deploy two machine learning algorithms and predict the resource demand of the backend systems of microservices emulated by a Netflix workload benchmark application. This service architecture presents an integrated solution of implementing self-managing cloud data services under variant workload.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258201","Auto-scaling;Cloud Computing;Machine learning","Cloud computing;Data models;Measurement;Monitoring;Predictive models;Training;Unified modeling language","cloud computing;decision making;learning (artificial intelligence);pattern clustering","Netflix workload benchmark application;cloud resource provisioning;decision making;machine learning algorithms;microservice architecture;microservice backend systems;monitoring metrics;online machine learning;resource demand;scaling policy;self-managing cloud data services;variant workload;workload pattern learning","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"MPLEX: In-Situ Big Data Processing with Compute-Storage Multiplexing","J. Rahman; P. Lama","Dept. of Comput. Sci., Univ. of Texas at San Antonio, San Antonio, TX, USA","2017 IEEE 25th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)","20171116","2017","","","43","52","Cloud-based services are increasingly popular for big data analytics due to the flexibility, scalability, and cost-effectiveness of provisioning elastic resources on-demand. However, data analytics-as-a-service suffers from the overheads of data movement between compute and storage clusters, due to their decoupled architecture in existing cloud infrastructure. In this work, we propose a novel approach of in-situ big data processing on cloud storage by dynamically offloading data-intensive jobs from compute cluster to storage cluster, and improve job throughput. However, it is challenging to achieve this goal since introducing additional workload on the storage cluster can significantly impact interactive web requests that fetch cloud storage data, with strict SLA (service-level agreement) for tail latency. In this work, we present MPLEX, a system that augments data analytics-as-a-service by efficiently multiplexing compute and storage cluster to improve job throughput without violating the SLA of cloud storage service in terms of tail response time. It applies an SLA-aware opportunistic job scheduling technique supported by a machine learning based prediction model to exploit the dynamic workload conditions in the compute, and storage cluster. Performance evaluations on an OpenStack Swift cluster, and an OpenStack based virtual cluster of Hadoop VMs built atop NSFCloud's Chameleon testbed show that MPLEX improves the Hadoop job throughput by up to 1.7X, while maintaining the SLA for cloud storage service requests.","","Electronic:978-1-5386-2764-8; POD:978-1-5386-2765-5; USB:978-1-5386-2763-1","10.1109/MASCOTS.2017.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8107430","Hadoop;Map Reduce;Object Storage;big data;cloud;multiplexing","Big Data;Cloud computing;Computational modeling;Multiplexing;Predictive models;Throughput;Time factors","Big Data;cloud computing;contracts;data analysis;learning (artificial intelligence);scheduling","Hadoop VMs;MPLEX;NSFCloud's Chameleon testbed;OpenStack Swift cluster;big data analytics;cloud storage data;cloud storage service requests;cloud-based services;compute cluster;compute-storage multiplexing;data analytics-as-a-service;data analytics-as-a-service suffers;data movement;data-intensive jobs;in-situ big data processing;interactive Web requests;job throughput;machine learning;offloading data-intensive jobs;performance evaluations;prediction model;service-level agreement;storage cluster;tail latency","","","","","","","","20-22 Sept. 2017","","IEEE","IEEE Conferences"
"Incremental knowledge acquisition and self-learning for autonomous video surveillance","R. Nawaratne; T. Bandaragoda; A. Adikari; D. Alahakoon; D. De Silva; X. Yu","Research Centre for Data Analytics and Cognition, La Trobe University, Victoria, Australia","IECON 2017 - 43rd Annual Conference of the IEEE Industrial Electronics Society","20171218","2017","","","4790","4795","The world is witnessing a remarkable increase in the usage of video surveillance systems. Besides fulfilling an imperative security and safety purpose, it also contributes towards operations monitoring, hazard detection and facility management in industry/smart factory settings. Most existing surveillance techniques use hand-crafted features analyzed using standard machine learning pipelines for action recognition and event detection. A key shortcoming of such techniques is the inability to learn from unlabeled video streams. The entire video stream is unlabeled when the requirement is to detect irregular, unforeseen and abnormal behaviors, anomalies. Recent developments in intelligent high-level video analysis have been successful in identifying individual elements in a video frame. However, the detection of anomalies in an entire video feed requires incremental and unsupervised machine learning. This paper presents a novel approach that incorporates high-level video analysis outcomes with incremental knowledge acquisition and self-learning for autonomous video surveillance. The proposed approach is capable of detecting changes that occur over time and separating irregularities from re-occurrences, without the prerequisite of a labeled dataset. We demonstrate the proposed approach using a benchmark video dataset and the results confirm its validity and usability for autonomous video surveillance.","","Electronic:978-1-5386-1127-2; POD:978-1-5386-1128-9; USB:978-1-5386-1126-5","10.1109/IECON.2017.8216826","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8216826","Anomaly Detection;Autonomous Video Surveillance;Incremental Learning;Industry 4.0;Intelligent Video Analysis;Self-Organizing Maps;Unsupervised Learning","Clustering algorithms;Cognition;Feature extraction;Hazards;Heuristic algorithms;Video surveillance","knowledge acquisition;object detection;unsupervised learning;video signal processing;video streaming;video surveillance","action recognition;anomaly detection;autonomous video surveillance;benchmark video dataset;change detection;event detection;hazard detection;imperative security;incremental knowledge acquisition;incremental machine learning;industry/smart factory settings;intelligent high-level video analysis;safety purpose;self-learning;standard machine learning pipelines;unlabeled video streams;unsupervised machine learning;video feed;video frame;video stream;video surveillance systems","","","","","","","","Oct. 29 2017-Nov. 1 2017","","IEEE","IEEE Conferences"
"Emotion-based social computing platform for streaming big-data: Architecture and application","L. Zhang; J. Zhao; K. Xu","State Key Lab of Software Development Environment, Beihang University, Beijing, China","2016 13th International Conference on Service Systems and Service Management (ICSSSM)","20160811","2016","","","1","6","Exploration of user generated content in the epoch of Web 2.0 brings unprecedented challenge to the social computing, which has to provide real-time solution in the circumstance of massive data volumes and evolving application scenarios. This paper presents an emotion-based social computing platform namely ESC for streaming big-data. The main aim of ESC is to provide sentiment analysis as the foundation of social computing and enable both real-time computation on streaming big-data and batch computation on off-line big-data with high performance and low risk. Different from conventional data processing technologies, ESC is designed as a scalable and QoS-optimized adaptive platform for developers to only focus on business models instead of being distracted by details of the computing infrastructure. In addition, continuous streaming computing is emphasized in ESC to keep tracking on long term dynamic evolution in social media, which can provide a valuable proxy for in-depth social analytics. The architecture of ESC is implemented by distributed storage, sentiment analysis, data parallelism and routing, real-time streaming computation, batch computation and distributed machine learning. And the evaluation results from real-time and batch computations testify the high performance and scalability of ESC. Moreover, a few applications based on it further demonstrates its usability in enacting on different streaming big-data and variety of social computations.","","Electronic:978-1-5090-2842-9; POD:978-1-5090-2843-6","10.1109/ICSSSM.2016.7538620","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7538620","","Computer architecture;Logic gates;Quality of service;Reliability","Big Data;Internet;learning (artificial intelligence);quality of service;sentiment analysis;social networking (online);social sciences computing","ESC;QoS-optimized adaptive platform;Web 2.0;batch computation;big data streaming;business models;continuous streaming computing;data parallelism;distributed machine learning;distributed storage;emotion-based social computing platform;in-depth social analytics;offline big data;real-time streaming computation;routing;sentiment analysis;social media;user generated content exploration","","","","","","","","24-26 June 2016","","IEEE","IEEE Conferences"
"Scalable sentiment classification for Big Data analysis using Naïve Bayes Classifier","B. Liu; E. Blasch; Y. Chen; D. Shen; G. Chen","Intell. Fusion Technol., Inc., Germantown, MD, USA","2013 IEEE International Conference on Big Data","20131223","2013","","","99","104","A typical method to obtain valuable information is to extract the sentiment or opinion from a message. Machine learning technologies are widely used in sentiment classification because of their ability to “learn” from the training dataset to predict or support decision making with relatively high accuracy. However, when the dataset is large, some algorithms might not scale up well. In this paper, we aim to evaluate the scalability of Naïve Bayes classifier (NBC) in large datasets. Instead of using a standard library (e.g., Mahout), we implemented NBC to achieve fine-grain control of the analysis procedure. A Big Data analyzing system is also design for this study. The result is encouraging in that the accuracy of NBC is improved and approaches 82% when the dataset size increases. We have demonstrated that NBC is able to scale up to analyze the sentiment of millions movie reviews with increasing throughput.","","Electronic:978-1-4799-1293-3; POD:978-1-4799-1294-0","10.1109/BigData.2013.6691740","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691740","Big data;Cloud computing;Polarity mining;sentiment classification","Accuracy;Data handling;Data storage systems;Information management;Mathematical model;Motion pictures;Training","Bayes methods;Big Data;data analysis;data mining;learning (artificial intelligence);pattern classification;text analysis","Big Data analysis;Mahout;NBC;dataset size;decision making;machine learning;movie reviews;naive Bayes classifier;opinion extraction;scalable sentiment classification;sentiment extraction","","8","","18","","","","6-9 Oct. 2013","","IEEE","IEEE Conferences"
"An Industrial-Strength Pipeline for Recognizing Fasteners","N. Sephus; S. Bhagavatula; P. Shastri; E. Gabriel","Partpic Inc., Atlanta, GA, USA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","781","786","Image classification and computer vision for search are rapidly emerging in today's technology and consumer markets. Specifically, startup companies have leveraged state-of-the-art image search capabilities in automating recognition of logos and titles, pop-up advertisements based on video content, and recommendations of products in the fashion industry. Partpic focuses on image search for replacement parts, and we present our industrial pipeline for such, with application to fasteners. We discuss how we have aimed to overcome issues such as acquiring enough training data, training and classification of many different types of fasteners, identification of customized specifications of fasteners (such as finish type, dimensions, etc.), establishing constraints for the user to take an good-enough image, and scalability of many pieces of data associated with thousands of fasteners.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.191","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424417","fasteners;image recognition;industrial;machine learning","Computer vision;Databases;Fasteners;Image recognition;Image segmentation;Imaging;Training","advertising data processing;computer vision;fasteners;image classification;image retrieval;learning (artificial intelligence);object recognition;pipelines","automatic logo recognition;automatic title recognition;computer vision;consumer markets;customized specification identification;fashion industry;fastener classification;fastener recognition;fastener training;good-enough image;image classification;image search;industrial pipeline;industrial-strength pipeline;pop-up advertisements;product recommendation;replacement parts;training data;video content","","","","8","","","","9-11 Dec. 2015","","IEEE","IEEE Conferences"
"A quality based automated admission system for educational domain","S. Mishra; S. Sahoo; B. K. Mishra; S. Satapathy","C.V Raman College of Engineering, Bhubaneswar, India","2016 International Conference on Signal Processing, Communication, Power and Embedded System (SCOPES)","20170626","2016","","","221","223","In last two decades several educational institutes have started gaining momentum while many of them are in self financing mode. Every institute wants to have good student strength to allow a smooth academic session. This paper proposes the use of machine learning techniques in educational domain to enhance the quality of student admissions in any higher educational institute. The focus of this paper is to identify those admissions inquires which most likely to turn into actual admissions. The result of analysis will assist the academic planners to focus their efforts on the set of students that are likely to take admission in the institution after initial enquiry.","","Electronic:978-1-5090-4620-1; POD:978-1-5090-4621-8","10.1109/SCOPES.2016.7955824","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7955824","Clustering;Euclidean distance component;K-Means algorithm;Machine learning","Algorithm design and analysis;Clustering algorithms;Dairy products;Data mining;Prediction algorithms;Signal processing;Signal processing algorithms","educational administrative data processing;educational institutions;further education;learning (artificial intelligence)","academic planner assistance;academic session;educational domain;educational institutes;higher educational institute;machine learning techniques;quality based automated admission system;self financing mode;student admission quality","","","","","","","","3-5 Oct. 2016","","IEEE","IEEE Conferences"
"Quantum Computing in Big Data Analytics: A Survey","T. A. Shaikh; R. Ali","Dept. of Comput. Eng., Aligarh Muslim Univ., Aligarh, India","2016 IEEE International Conference on Computer and Information Technology (CIT)","20170313","2016","","","112","115","Big Data is a term which denotes data that is beyond storage capacity and processing capabilities of classical computer and getting some insight from large amount of data is a very big challenge at hand. Quantum Computing comes to rescue by offering a lot of promises in information processing systems, particularly in Big Data Analytics. In this paper, we have reviewed the available literature on Big Data Analytics using Quantum Computing for Machine Learning and its current state of the art. We categorized the Quantum Machine learning in different subfields depending upon the logic of their learning followed by a review in each technique. Quantum Walks used to construct Quantum Artificial Neural Networks, which exponentially speed-up the quantum machine learning algorithm is discussed. Quantum Supervised and Unsupervised machine learning and its benefits are compared with that of Classical counterpart. The limitations of some of the existing Machine learning techniques and tools are enunciated, and the significance of Quantum computing in Big Data Analytics is incorporated. Being in its infancy as a totally new field, Quantum computing comes up with a lot of open challenges as well. The challenges, promises, future directions and techniques of the Quantum Computing in Machine Learning are also highlighted.","","Electronic:978-1-5090-4314-9; POD:978-1-5090-4315-6","10.1109/CIT.2016.79","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7876324","Big Data Analytics;Machine Learning;Quantum Artificial Intelligence;Quantum Clustering;Quantum Computing;Qubits","Big data;Computers;Logic gates;Machine learning algorithms;Quantum computing;Quantum mechanics;Support vector machines","Big Data;data analysis;learning (artificial intelligence);neural nets;quantum computing","Big Data analytics;information processing systems;quantum artificial neural networks;quantum computing;quantum unsupervised machine learning;quantum walks","","","","","","","","8-10 Dec. 2016","","IEEE","IEEE Conferences"
"Medical Imaging Processing on a Big Data Platform Using Python: Experiences with Heterogeneous and Homogeneous Architectures","E. Serrano; J. G. Blas; J. Carretero; M. Abella; M. Desco","Comput. Archit. & Technol. Group, Univ. Carlos III of Madrid, Leganes, Spain","2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)","20170713","2017","","","830","837","The apparition of new paradigms, programming models, and languages that offer better programmability and better performance turns the implementation of current scientific applications into a less time-consuming task than years ago. One significant example of this trend is the MapReduce programming model and its implementation using Apache Spark. Nowadays, this programming model is mainly used for data analysis and machine learning applications, although it has been expanded to its usage in the HPC community. On the side of programming languages, Python has positioned itself as an alternative to other scientific programming languages, such as Matlab or Julia. In this work we explore the capabilities of Python and Apache Spark as partners in the implementation of the backprojection operator of a CT reconstruction application. We present two interesting approaches with two different types of architectures: a heterogeneous architecture including NVidia GPUs and a full performance CPU mode with the compatibility with C/C++ native source code. We experimentally demonstrate that current CPU-based implementations scale with the number of computational units.","","Electronic:978-1-5090-6611-7; POD:978-1-5090-5980-5","10.1109/CCGRID.2017.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7973788","Apache Spark;Big Data;CT;CUDA;Python;backprojection","Big Data;Computational modeling;Computed tomography;Computer architecture;Image reconstruction;Programming;Sparks","Big Data;C++ language;data analysis;graphics processing units;learning (artificial intelligence);medical image processing;microprocessor chips;parallel processing;programming languages;public domain software;source code (software);workstation clusters","Apache Spark;Big Data platform;C/C++ native source code;CPU mode;HPC community;MapReduce programming model;NVidia GPU;Python;data analysis;heterogeneous architectures;homogeneous architectures;machine learning;medical imaging processing;programming languages","","","","","","","","14-17 May 2017","","IEEE","IEEE Conferences"
"Smart fog: Fog computing framework for unsupervised clustering analytics in wearable Internet of Things","D. Borthakur; H. Dubey; N. Constant; L. Mahler; K. Mankodiya","Wearable Biosensing Lab, University of Rhode Island, Kingston, RI-02881, USA","2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP)","20180308","2017","","","472","476","The increasing use of wearables in smart telehealth system led to the generation of large medical big data. Cloud and fog services leverage these data for assisting clinical procedures. IoT Healthcare has been benefited from this large pool of generated data. This paper suggests the use of low-resource machine learning on Fog devices kept close to wearables for smart telehealth. For traditional telecare systems, the signal processing and machine learning modules are deployed in the cloud that processes physiological data. This paper presents a Fog architecture that relied on unsupervised machine learning big data analysis for discovering patterns in physiological data. We developed a prototype using Intel Edison and Raspberry Pi that was tested on real-world pathological speech data from telemonitoring of patients with Parkinson's disease (PD). Proposed architecture employed machine learning for analysis of pathological speech data obtained from smart watches worn by the patients with PD. Results show that proposed architecture is promising for low-resource machine learning. It could be useful for other applications within wearable IoT for smart telehealth scenarios by translating machine learning approaches from the cloud backend to edge computing devices such as Fog.","","Electronic:978-1-5090-5990-4; POD:978-1-5090-5991-1; USB:978-1-5090-5989-8","10.1109/GlobalSIP.2017.8308687","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8308687","Dysarthria;Edge Computing;Fog Computing;K-means Clustering;Parkinson's Disease;Speech Disorders","Biomedical monitoring;Cloud computing;Computer architecture;Edge computing;Feature extraction;Parkinson's disease;Speech","Big Data;Internet of Things;biomedical communication;cloud computing;data analysis;diseases;health care;learning (artificial intelligence);patient monitoring;speech processing;telemedicine;watches;wearable computers","Fog architecture;IoT Healthcare;architecture employed machine;big data analysis;clinical procedures;cloud backend;computing devices;fog computing framework;fog services;generated data;low-resource machine learning;machine learning approaches;machine learning modules;medical big data;physiological data;real-world pathological speech data;signal processing;smart fog;smart telehealth scenarios;smart telehealth system;smart watches;traditional telecare systems;unsupervised clustering analytics;unsupervised machine learning;wearable Internet of Things;wearable IoT","","","","","","","","14-16 Nov. 2017","","IEEE","IEEE Conferences"
"Recommender system for big data in education","S. Dwivedi; V. S. K. Roshni","Centre for Development of Advanced Computing, Bengaluru, India","2017 5th National Conference on E-Learning & E-Learning Technologies (ELELTECH)","20171019","2017","","","1","4","With the advent of web based e-learning systems, a huge amount of educational data is getting generated. These massive data gave rise to Big data in educational sectors. Currently, big data analytics techniques are being used to analyze these educational data and generate different predictions and recommendations for students, teachers and schools. Recommendation systems are already very helpful in e-commerce, service industry and social networking sites. Recently recommendation systems are proved to be efficient for education sector as well. In this work we are using recommendation system for Big data in education. This work uses collaborative filtering based recommendation techniques to recommend elective courses to students, depending upon their grade points obtained in other subjects. We are using item based recommendation of Mahout machine learning library on top of Hadoop to generate set of recommendations. Similarity Log-likelihood is used to discover patterns among grades and subjects. Root Mean Square Error between actual grade and recommended grade is used to test the recommendation system. The output of this study can be used by schools, colleges or universities to suggest alternative elective courses to students.","","Electronic:978-1-5386-1922-3; POD:978-1-5386-1923-0","10.1109/ELELTECH.2017.8074993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8074993","Educational data mining;big data analytics;recommender systems","Big Data;Collaboration;Data mining;Education;Libraries;Recommender systems","Big Data;Internet;collaborative filtering;computer aided instruction;data analysis;data mining;educational institutions;electronic commerce;learning (artificial intelligence);parallel processing;pattern clustering;recommender systems;social networking (online)","Hadoop;Mahout machine learning library;Web-based e-learning systems;big data analytics techniques;collaborative filtering based recommendation techniques;e-commerce;education sector;educational data analysis;educational sectors;item based recommendation;massive data;recommendation system;recommender system;root mean square error;service industry;similarity log-likelihood;social networking sites","","","","","","","","3-4 Aug. 2017","","IEEE","IEEE Conferences"
"Massively parallel learning of Bayesian networks with MapReduce for factor relationship analysis","W. Chen; T. Wang; D. Yang; K. Lei; Y. Liu","School of EECS, Peking University","The 2013 International Joint Conference on Neural Networks (IJCNN)","20140109","2013","","","1","5","Bayesian Network (BN) is one of the most popular models in data mining technologies. Most of the algorithms of BN structure learning are developed for the centralized datasets, where all the data are gathered into a single computer node. They are often too costly or impractical for learning BN structures from large scale data. Through a simple interface with two functions, map and reduce, MapReduce facilitates parallel implementation of many real-world tasks such as data processing for search engines and machine learning. In this paper, we present a parallel algorithm for BN structure leaning from large-scale dateset by using a MapReduce cluster. We discuss the benefits of using MapReduce for BN structure learning, and demonstrate the performance of this approach by applying it to a real world financial factor relationships learning task from the domain of financial analysis.","2161-4393;21614393","Electronic:978-1-4673-6129-3; POD:978-1-4673-6127-9","10.1109/IJCNN.2013.6706814","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6706814","","Algorithm design and analysis;Bayes methods;Computational modeling;Data mining;Educational institutions;Mutual information;Training","Bayes methods;data mining;directed graphs;financial data processing;learning (artificial intelligence);parallel algorithms;search engines","BN structure learning;MapReduce cluster;centralized datasets;data mining technologies;data processing;financial analysis;financial factor relationship learning task analysis;large-scale dateset;machine learning;massively parallel learning;parallel algorithm;parallel implementation;search engines","","3","","8","","","","4-9 Aug. 2013","","IEEE","IEEE Conferences"
"Predict failures in production lines: A two-stage approach with clustering and supervised learning","D. Zhang; B. Xu; J. Wood","International Center for Automotive Research, Clemson University, Greenville, USA","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","2070","2074","The implementation of advanced technologies in manufacturing has created large amounts of data. The data can be utilized to create predictive models for quality control, which allows manufacturers to produce higher quality products at a lower cost. Bosch has provided a large-scale data set of a production line and hosted a challenge on Kaggle aiming to predict the manufacturing failures using the anonymized features. We proposed a two-stage method first to cluster the data into groups based on the manufacturing process and then use supervised learning to predict the failed product in each cluster. This approach reduces the sparsity of the data set. Various algorithms were compared. The random forest algorithm achieved the highest performance score and was chosen as the final model.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840832","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840832","clustering;manufacturing;quality control;supervised learning","Classification algorithms;Clustering algorithms;Machine learning algorithms;Manufacturing processes;Prediction algorithms;Supervised learning","data handling;failure analysis;learning (artificial intelligence);manufacturing data processing;pattern clustering;product quality;production engineering computing;quality control","Bosch;Kaggle;data clustering;data set sparsity reduction;manufacturing failure prediction;manufacturing process;predictive model;product quality;production line failure prediction;quality control;random forest algorithm;supervised learning","","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conferences"
"Grassmann Averages for Scalable Robust PCA","S. Hauberg; A. Feragen; M. J. Black","DTU Compute, Lyngby, Denmark","2014 IEEE Conference on Computer Vision and Pattern Recognition","20140925","2014","","","3810","3817","As the collection of large datasets becomes increasingly automated, the occurrence of outliers will increase -- ""big data"" implies ""big outliers"". While principal component analysis (PCA) is often used to reduce the size of data, and scalable solutions exist, it is well-known that outliers can arbitrarily corrupt the results. Unfortunately, state-of-the-art approaches for robust PCA do not scale beyond small-to-medium sized datasets. To address this, we introduce the Grassmann Average (GA), which expresses dimensionality reduction as an average of the subspaces spanned by the data. Because averages can be efficiently computed, we immediately gain scalability. GA is inherently more robust than PCA, but we show that they coincide for Gaussian data. We exploit that averages can be made robust to formulate the Robust Grassmann Average (RGA) as a form of robust PCA. Robustness can be with respect to vectors (subspaces) or elements of vectors, we focus on the latter and use a trimmed average. The resulting Trimmed Grassmann Average (TGA) is particularly appropriate for computer vision because it is robust to pixel outliers. The algorithm has low computational complexity and minimal memory requirements, making it scalable to ""big noisy data."" We demonstrate TGA for background modeling, video restoration, and shadow removal. We show scalability by performing robust PCA on the entire Star Wars IV movie.","1063-6919;10636919","Electronic:978-1-4799-5118-5; POD:978-1-4799-5119-2","10.1109/CVPR.2014.481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6909882","Grassmann manifold;PCA;Robust PCA;computer vision;directional statistics;machine learning;subspace estimation","Complexity theory;Computer vision;Estimation;Motion pictures;Principal component analysis;Robustness;Vectors","Big Data;computer vision;principal component analysis","Gaussian data;RGA;Star Wars IV movie;TGA;big noisy data;big outliers;computational complexity;computer vision;dimensionality reduction;large datasets;memory requirements;pixel outliers;principal component analysis;robust Grassmann average;scalable robust PCA;shadow removal;trimmed Grassmann average;video restoration","","16","","31","","","","23-28 June 2014","","IEEE","IEEE Conferences"
"Machine learning approaches on map reduce for Big Data analytics","J. V. N. Lakshmi; A. Sheshasaayee","Research Department of Computer Science, SCSVMV University, Kanchipuram, Tamil Nadu - India","2015 International Conference on Green Computing and Internet of Things (ICGCIoT)","20160114","2015","","","480","484","To analyze enormous datasets, collection of algorithms, associated systems and perform necessary processing on massive data structures there is obligation for a novel trend, which is framed by Big Data. Architecture of Big Data varies across compound machines and clusters with unique purpose sub systems. The data produced from several sources requires analysis and organization with meager amounts of time. To potentially speed up the processing, a unified way of machine learning is applied on MapReduce frame work. A broadly applicable programming model MapReduce is applied on different learning algorithms belonging to machine learning family for all business decisions. By using ML algorithms with Hadoop for better storage distribution will improve the time and processing speed. This paper presents parallel implementation of various machine learning algorithms implemented on top of MapReduce model for time and processing efficiency.","","Electronic:978-1-4673-7910-6; POD:978-1-4673-7911-3; USB:978-1-4673-7909-0","10.1109/ICGCIoT.2015.7380512","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7380512","Big Data;Hadoop;Machine Learning;Machine Learning Algorithms;MapReduce","Algorithm design and analysis;Big data;Clustering algorithms;Computational modeling;Computer architecture;Data models;Machine learning algorithms","Big Data;data analysis;learning (artificial intelligence);parallel programming;storage management","Big Data analytics;ML algorithms;MapReduce framework;MapReduce model;business decisions;compound machines;machine learning algorithms;machine learning approach;machine learning family;parallel implementation;processing efficiency;programming model;storage distribution","","","","20","","","","8-10 Oct. 2015","","IEEE","IEEE Conferences"
"SAMURAI: A Streaming Multi-tenant Context-Management Architecture for Intelligent and Scalable Internet of Things Applications","D. Preuveneers; Y. Berbers","Dept. of Comput. Sci., KU Leuven, Leuven, Belgium","2014 International Conference on Intelligent Environments","20140929","2014","","","226","233","In the Internet of Things, heterogeneous and distributed streams of sensor events is a driver for context-aware behavior in intelligent environments. However, processing the event data usually cross-cuts the business logic of IoT applications and offering such reusable functionality as a service towards a variety of customers with different needs is often faced with scalability concerns. We present SAMURAI, a multi-tenant streaming context architecture that integrates and exposes well-known components for complex event processing, machine learning, knowledge representation, NoSQL persistence and in-memory data grids. SAMURAI pursues a twofold approach to achieve scalability: (1) distributed deployment with horizontal scalability, (2) shared resources through multi-tenancy. For the scenario used in the experimental evaluation of our architecture, the results show little overhead to support multi-tenancy, with near-linear scalability and flexible elasticity for deployment schemes with data partitioning per tenant.","","Electronic:978-1-4799-2947-4; POD:978-1-4799-2948-1","10.1109/IE.2014.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6910454","classification;complex event processing;context;scalability;semantic enrichment;stream mining","Acceleration;Accelerometers;Computer architecture;Context;Data mining;Scalability;Semantics","Internet of Things;SQL;business data processing;knowledge representation;learning (artificial intelligence);ubiquitous computing","IoT applications;NoSQL persistence;SAMURAI;business logic;complex event processing;data partitioning;distributed deployment;flexible elasticity;horizontal scalability;in-memory data grids;intelligent Internet of things applications;knowledge representation;machine learning;near-linear scalability;scalable Internet of things applications;sensor event streams;streaming multitenant context-management architecture","","2","","17","","","","June 30 2014-July 4 2014","","IEEE","IEEE Conferences"
"Supervised Machine Learning Model for High Dimensional Gene Data in Colon Cancer Detection","H. Chen; H. Zhao; J. Shen; R. Zhou; Q. Zhou","Sch. of Inf. Sci. & Eng., Lanzhou Univ., Lanzhou, China","2015 IEEE International Congress on Big Data","20150820","2015","","","134","141","With well-developed methods in gene level data extraction, there are huge amount of gene expression data, including normal composition and abnormal ones. Therefore, mining gene expression data is currently an urgent research question, for detecting a corresponding pattern, such as cancer species, quickly and accurately. Since gene expression data classification problem has been widely studied accompanying with the development of gene technology, by far numerous methods, mainly neural network related, have been deployed in medical data analysis, which is mainly dealing with the high dimension and small quantity. A lot of research has been conducted on clustering approaches, extreme learning machine and so on. They are usually applied in a shallow neural network model. Recently deep learning has shown its power and good performance on high dimensional datasets. Unlike current popular deep neural network, we will continue to apply shallow neural network but develop an innovative algorithm for shallow neural network. In the supervised model, we demonstrate a shallow neural network model with a batch of parameters, and narrow its computational process into several positive parts, which process smoothly for a better result and finally achieve an optimal goal. It shows a stable and excellent result comparable to deep neural network. An analysis of the algorithm is also presented in this paper.","2379-7703;23797703","Electronic:978-1-4673-7278-7; POD:978-1-4673-7279-4","10.1109/BigDataCongress.2015.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7207212","Monte Carlo;Neural Network;high dimensional data","Cancer;Colon;Cost function;Gene expression;Monte Carlo methods;Neural networks;Support vector machines","cancer;data analysis;data mining;genetics;learning (artificial intelligence);medical information systems;neural nets;pattern classification;pattern clustering","cancer species;clustering approaches;colon cancer detection;deep neural network;extreme learning machine;gene expression data classification problem;gene expression data mining;gene level data extraction;high dimensional gene data;medical data analysis;neural network model;shallow neural network;supervised machine learning model","","2","","24","","","","June 27 2015-July 2 2015","","IEEE","IEEE Conferences"
"Sparse-reduced computation for large-scale spectral clustering","P. Baumann","Department of Business Administration, University of Bern, Schuetzenmattstrasse 14, 3012, Switzerland","2016 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)","20161229","2016","","","1284","1288","Clustering is a fundamental task in machine learning and data analysis. A large number of clustering algorithms has been developed over the past decades. Among these algorithms, the recently developed spectral clustering methods have consistently outperformed traditional clustering algorithms. Spectral clustering algorithms, however, have limited applicability to large-scale problems due to their high computational complexity. We propose a new approach for scaling spectral clustering methods that is based on the idea of replacing the entire data set with a small set of representative data points and performing the spectral clustering on the representatives. The main contribution is a new approach for efficiently identifying the representative data points. First results indicate that the proposed scaling approach achieves high-quality clusterings and is substantially faster than existing scaling approaches.","","Electronic:978-1-5090-3665-3; POD:978-1-5090-3666-0; USB:978-1-5090-3664-6","10.1109/IEEM.2016.7798085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7798085","Big Data;Large-scale Clustering;Sparse-reduced computation;Spectral Clustering;Unsupervised Learning","Algorithm design and analysis;Business;Clustering algorithms;Clustering methods;Laplace equations;Machine learning algorithms;Principal component analysis","computational complexity;data structures;learning (artificial intelligence);pattern clustering","computational complexity;data analysis;data representation;large-scale spectral clustering;machine learning;sparse-reduced computation","","","","","","","","4-7 Dec. 2016","","IEEE","IEEE Conferences"
"A Spatio - Temporal Hedonic House Regression Model","T. Oladunni; S. Sharma; R. Tiwang","Dept. of Comput. Sci., Bowie state Univ., Bowie, MD, USA","2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)","20180118","2017","","","607","612","This work focuses on an algorithmic investigation of the housing market spanning 11 years using the hedonic pricing theory. An improved pricing model will benefit home buyers and sellers, real estate agents and appraisers, government and mortgage lenders. Hedonic pricing theory is an econometric concept that explains the market value of a differentiated commodity using implicit pricing. Exploiting the spatial dependent nature of the housing market, we created new submarkets. A model was built with the new submarket, while another one was built using the existing submarket. Random forest and LASSO were trained with the two models. We argue that our approach has a considerable impact on the dimension of a spatio-temporal hedonic house pricing model without a significant reduction in its performance.","","Electronic:978-1-5386-1418-1; POD:978-1-5386-1419-8","10.1109/ICMLA.2017.00-94","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8260698","K-means clustering;hedonic pricing model;housing prices prediction;least absolute shrinkage and selection operator (LASSO);random forest;time series","Automobiles;Clustering algorithms;Estimation;Loans and mortgages;Pricing;Standards;Urban areas","econometrics;pricing;random processes;real estate data processing;regression analysis","LASSO;appraisers;econometric concept;estate agents;government lenders;hedonic pricing theory;home buyers;home sellers;implicit pricing;market value;mortgage lenders;random forest;spatio - temporal hedonic house regression;spatio-temporal hedonic house pricing model","","","","","","","","18-21 Dec. 2017","","IEEE","IEEE Conferences"
"A Covering Classification Rule Induction Approach for Big Datasets","V. Kolias; I. Anagnostopoulos; E. Kayafas","Sch. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Zografou, Greece","2014 IEEE/ACM International Symposium on Big Data Computing","20151109","2014","","","45","53","With the ever increasing production of data from various heterogeneous sources in modern information societies, the need for scalable data-intensive processing is increasing. MapReduce quickly became the de facto framework for large scale data analysis, due to its simple and abstract programming model and its efficient underlying execution system. However, this simplicity comes with a price: its unidirectional communication model and the lack of support for iterations, makes repeated querying of datasets difficult and imposes limitations in many fields including Machine Learning. In this paper we describe the implementation of a classification rule induction algorithm based on MapReduce, with the aim of building a classification model within as few iterations as possible. After a thorough description of the algorithm, we evaluate its performance from three perspectives: its accuracy, its parallel performance and the communication costs. The evaluations indicate that the approach is scalable and since it produces a comprehensive human-readable model it can be proven valuable for a wide range of applications.","","Electronic:978-1-4799-1897-3; POD:978-1-4799-1898-0; USB:978-1-4799-1896-6","10.1109/BDC.2014.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7321728","big data;classification;machine learning;mapreduce;rule induction","Accuracy;Big data;Clustering algorithms;Machine learning algorithms;Production;Radiation detectors;Training","Big Data;learning (artificial intelligence);parallel processing;pattern classification","MapReduce;big datasets;communication costs;comprehensive human-readable model;covering classification rule induction approach;dataset querying;de-facto framework;execution system;heterogeneous sources;information societies;large scale data analysis;machine learning;parallel performance costs;performance evaluation;programming model;scalable data-intensive processing;unidirectional communication model","","","","28","","","","8-11 Dec. 2014","","IEEE","IEEE Conferences"
"keybin: Key-Based Binning for Distributed Clustering","X. Chen; J. Benson; T. Estrada","","2017 IEEE International Conference on Cluster Computing (CLUSTER)","20170925","2017","","","572","581","Traditional machine learning algorithms often require computations on centralized data, but modern datasets are collected and stored in a distributed way. In addition to the cost of moving data to centralized locations, increasing concerns about privacy and security warrant distributed approaches. We propose keybin, a distributed key-based binning clustering algorithm for high-dimensional spaces. keybin locally generates a spatial key for each data point across all dimensions without needing knowledge of other data. Then, it performs a conceptual Map- Reduce procedure in the index space to form a global clustering assignment. We present an implementation and a case study on the capabilities and limitations of this approach, showing that this algorithm can learn a global clustering structure with limited communication and can scale with the dimensionality and size of data sets.","","Electronic:978-1-5386-2326-8; POD:978-1-5386-2327-5","10.1109/CLUSTER.2017.96","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8048971","Big Data;Distributed Clustering;Map-Reduce;Privacy Preserving","Clustering algorithms;Computational modeling;Data models;Distributed databases;Indexes;Privacy;Scalability","data analysis;data privacy;learning (artificial intelligence);parallel programming;pattern clustering","binning clustering algorithm;centralized data;centralized locations;conceptual Map- Reduce procedure;distributed approaches;distributed key;global clustering assignment;high-dimensional spaces;index space;keybin;privacy;security;spatial key;traditional machine learning algrithm","","","","","","","","5-8 Sept. 2017","","IEEE","IEEE Conferences"
"Radio Resources Dimensioning Using Machine Learning with case study in Live Network","A. Roshdy; A. Gaber; M. Khairy","Radio Access Network, Vodafone Cairo, Egypt","2017 International Conference on Computer and Applications (ICCA)","20171023","2017","","","67","73","The authors propose a new tool to extensively analyze and plan the radio resources based on machine learning technique using available data analytics methods like clustering, correlation and regression. This will be done through; Classifying the cells according to their Priority and required quality of service, detecting resources utilization that cause throughput limitation, efficient dimensioning of the sites' capacity according to their load, advising by the capacity off-loading solutions, and offering potential cost saving by redeployment the resources from low to high utilized cells. Those features will achieve optimum resources utilization across the network. Moreover, detected throughput bottlenecks are relieved and overall system performance is enhanced. This will reflect on the customer satisfaction with a minimum spending cost, and minimum human intervention.","","Electronic:978-1-5386-2752-5; POD:978-1-5386-2753-2; USB:978-1-5386-2751-8","10.1109/COMAPP.2017.8079767","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8079767","Clustering;Correlation;dimensioning;regression;throughput;utilization","Correlation;Multiaccess communication;Predictive models;Spread spectrum communication;Support vector machines;Throughput;Tools","cellular radio;customer satisfaction;data analysis;learning (artificial intelligence);optimisation;pattern classification;quality of service;resource allocation;telecommunication computing;telecommunication network planning","capacity off-loading solutions;cells classification;customer satisfaction;data analytics methods;live network;machine learning technique;quality of service;radio resources analysis;radio resources dimensioning;radio resources planning;resources redeployment;resources utilization;throughput bottlenecks","","","","","","","","6-7 Sept. 2017","","IEEE","IEEE Conferences"
"Leveraging Big Data to Provide a Web Service That Provides the Likelihood of Developing Psychological Conditions after a Concussion","F. Dabek","Dept. of Comput. Sci. & Electr. Eng., Univ. of Maryland, College Park, MD, USA","2016 IEEE International Conference on Mobile Services (MS)","20161219","2016","","","160","165","Preventive care attempts to provide an early diagnosis by seeking to inform the patient and their physician of potential complications and diagnoses to expect. With the recent increase in big data present in the healthcare domain, from medical organizations modernizing their operations through electronic health records (EHRs) and deploying new health information technology (HIT) systems, there exists an opportunity for applying predictive models on this vast amount of EHR data and developing a web service to increase the potential for preventive care to be applied. In this paper we present our web service that uses previous work on predicting psychological conditions in concussion patients to provide doctors and patients with the ability to identify the likelihood of said developing psychological conditions. The existing predictive work that we base our web service on utilizes a neural network model which is very slow and inefficient by nature. Due to the inherent costly computation associated with this neural network model we ensure that our web service is fast, efficient, and scalable to meet the demands of prospective users. We provide users with both an API and a frontend interface for predicting psychological conditions 60, 90, 180, 365, and 730 days post concussion, and we utilize various technologies including Hadoop, Pybrain, and Django to form the architecture of our fast, scalable, and efficient prediction web service.","","Electronic:978-1-5090-2625-8; POD:978-1-5090-2626-5","10.1109/MobServ.2016.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7787069","Hadoop;concussion;machine learning;preventive healthcare;scalability;web service","Big data;Biological neural networks;Medical services;Predictive models;Psychology;Web services","Big Data;Web services;application program interfaces;electronic health records;health care;patient diagnosis;psychology;user interfaces","API;Big Data;EHR;HIT system;Web service;application program interface;concussion patient diagnosis;electronic health record;frontend interface;health care;health information technology;medical organization;psychological condition","","","","","","","","June 27 2016-July 2 2016","","IEEE","IEEE Conferences"
"Astro: A predictive model for anomaly detection and feedback-based scheduling on Hadoop","C. Gupta; M. Bansal; T. C. Chuang; R. Sinha; S. Ben-romdhane","ebay Inc., San Jose, California","2014 IEEE International Conference on Big Data (Big Data)","20150108","2014","","","854","862","The sheer growth in data volume and Hadoop cluster size make it a significant challenge to diagnose and locate problems in a production-level cluster environment efficiently and within a short period of time. Often times, the distributed monitoring systems are not capable of detecting a problem well in advance when a large-scale Hadoop cluster starts to deteriorate i n performance or becomes unavailable. Thus, inc o m i n g workloads, scheduled between the time when cluster starts to deteriorate and the time when the problem is identified, suffer from longer execution times. As a result, both reliability and throughput of the cluster reduce significantly. In this paper, we address this problem by proposing a system called Astro, which consists of a predictive model and an extension to the Hadoop scheduler. The predictive model in Astro takes into account a rich set of cluster behavioral information that are collected by monitoring processes and model them using machine learning algorithms to predict future behavior of the cluster. The Astro predictive model detects anomalies in the cluster and also identifies a ranked set of metrics that have contributed the most towards the problem. The Astro scheduler uses the prediction outcome and the list of metrics to decide whether it needs to move and reduce workloads from the problematic cluster nodes or to prevent additional workload allocations to them, in order to improve both throughput and reliability of the cluster. The results demonstrate that the Astro scheduler improves usage of cluster compute resources significantly by 64.23% compared to traditional Hadoop. Furthermore, the runtime of the benchmark application reduced by 26.68% during the time of anomaly, thus improving the cluster throughput.","","Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8","10.1109/BigData.2014.7004315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004315","Hadoop;anomaly;cluster;distributed systems;machine learning;predictive model;scheduler;self-remediation","Data models;Hidden Markov models;Measurement;Monitoring;Predictive models;Training;Yarn","distributed processing;pattern clustering;scheduling","Astro scheduler;Hadoop;anomaly detection;cluster behavioral information;distributed systems;feedback-based scheduling;predictive model","","1","","33","","","","27-30 Oct. 2014","","IEEE","IEEE Conferences"
"Challenges for MapReduce in Big Data","K. Grolinger; M. Hayes; W. A. Higashino; A. L'Heureux; D. S. Allison; M. A. M. Capretz","Dept. of Electr. & Comput. Eng., Western Univ., London, ON, Canada","2014 IEEE World Congress on Services","20140922","2014","","","182","189","In the Big Data community, MapReduce has been seen as one of the key enabling approaches for meeting continuously increasing demands on computing resources imposed by massive data sets. The reason for this is the high scalability of the MapReduce paradigm which allows for massively parallel and distributed execution over a large number of computing nodes. This paper identifies MapReduce issues and challenges in handling Big Data with the objective of providing an overview of the field, facilitating better planning and management of Big Data projects, and identifying opportunities for future research in this field. The identified challenges are grouped into four main categories corresponding to Big Data tasks types: data storage (relational databases and NoSQL stores), Big Data analytics (machine learning and interactive analytics), online processing, and security and privacy. Moreover, current efforts aimed at improving and extending MapReduce to address identified challenges are presented. Consequently, by identifying issues and challenges MapReduce faces when handling Big Data, this study encourages future Big Data research.","2378-3818;23783818","Electronic:978-1-4799-5069-0; POD:978-1-4799-5070-6","10.1109/SERVICES.2014.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903263","Big Data;Big Data Analytics;Interactive Analytics;Machine Learning;MapReduce;NoSQL;Online Processing;Privacy;Security","Algorithm design and analysis;Big data;Data models;Data visualization;Memory;Scalability;Security","Big Data;SQL;data analysis;data privacy;learning (artificial intelligence);parallel programming;relational databases;security of data;storage management","Big Data analytics;Big Data community;Big Data project management;Big Data project planning;MapReduce paradigm;NoSQL stores;data privacy;data security;data storage;interactive analytics;machine learning;massive data sets;massively distributed execution;massively parallel execution;online processing;relational databases","","27","","51","","","","June 27 2014-July 2 2014","","IEEE","IEEE Conferences"
"[Title page i]","","","2015 IEEE/ACM 2nd International Symposium on Big Data Computing (BDC)","20160215","2015","","","i","i","The following topics are dealt with: Big Data infrastructure; machine learning & data analytics; and process discovery scalability & middleware.","","Electronic:978-0-7695-5696-3","10.1109/BDC.2015.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406317","","","Big Data;data analysis;learning (artificial intelligence);middleware;software reliability","Big Data infrastructure;data analytics;machine learning;middleware;process discovery scalability","","","","","","","","7-10 Dec. 2015","","IEEE","IEEE Conferences"
"23-bit metaknowledge template towards Big Data knowledge discovery and management","N. Bari; R. Vichr; K. Kowsari; S. Berkovich","Department of Computer Science, George Washington University, DC 20052, USA","2014 International Conference on Data Science and Advanced Analytics (DSAA)","20150312","2014","","","519","526","The global influence of Big Data is not only growing but seemingly endless. The trend is leaning towards knowledge that is attained easily and quickly from massive pools of Big Data. Today we are living in the technological world that Dr. Usama Fayyad and his distinguished research fellows discussed in the introductory explanations of Knowledge Discovery in Databases (KDD) [1] predicted nearly two decades ago. Indeed, they were precise in their outlook on Big Data analytics. In fact, the continued improvement of the interoperability of machine learning, statistics, database building and querying fused to create this increasingly popular science-Data Mining and Knowledge Discovery. The next generation computational theories are geared towards helping to extract insightful knowledge from even larger volumes of data at higher rates of speed. As the trend increases in popularity, the need for a highly adaptive solution for knowledge discovery will be necessary. In this research paper, we are introducing the investigation and development of 23 bit-questions for a Metaknowledge template for Big Data Processing and clustering purposes. This research aims to demonstrate the construction of this methodology and proves the validity and the beneficial utilization that brings Knowledge Discovery from Big Data.","","Electronic:978-1-4799-6991-3; POD:978-1-4799-6982-1","10.1109/DSAA.2014.7058121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058121","23-Bit Meta-knowledge template;Big Data Processing and Analytics;Knowledge Discovery;Meta-feature Selection;Metalearning System","Big data;Classification algorithms;Correlation;Data mining;Knowledge discovery;Motion pictures;Vectors","Big Data;data mining;learning (artificial intelligence)","23-bit metaknowledge template;big data analytics;big data knowledge discovery;database building;machine learning;querying;science-data mining","","0","","22","","","","Oct. 30 2014-Nov. 1 2014","","IEEE","IEEE Conferences"
"A Time Series Distance Measure for Efficient Clustering of Input/Output Signals by Their Underlying Dynamics","O. Lauwers; B. De Moor","Department of Electrical Engineering, Leuven, Belgium","IEEE Control Systems Letters","20170623","2017","1","2","286","291","Starting from a dataset with input/output time series generated by multiple deterministic linear dynamical systems, this letter tackles the problem of automatically clustering these time series. We propose an extension to the so-called Martin cepstral distance, that allows to efficiently cluster these time series, and apply it to simulated electrical circuits data. Traditionally, two ways of handling the problem are used. The first class of methods employs a distance measure on time series (e.g., Euclidean, dynamic time warping) and a clustering technique (e.g., k-means, k-medoids, and hierarchical clustering) to find natural groups in the dataset. It is, however, often not clear whether these distance measures effectively take into account the specific temporal correlations in these time series. The second class of methods uses the input/output data to identify a dynamic system using an identification scheme, and then applies a model norm-based distance (e.g., H<sub>2</sub> and H<sub>∞</sub>) to find out which systems are similar. This, however, can be very time consuming for large amounts of long time series data. We show that the new distance measure presented in this letter performs as good as when every input/output pair is modeled explicitly, but remains computationally much less complex. The complexity of calculating this distance between two time series of length N is O(N log N).","","","10.1109/LCSYS.2017.2715399","Belgian Federal Science Policy Office: IUAP P7/19 (DYSCO, Dynamical Systems, Control and Optimization, 2012¿¿¿2017) Flemish Government: IWT: TBM IETA; KIC EIT Health: New MOOC¿¿¿Data Analytics in Health; EIT Health Summer School Innovation on Big Data for Healthy Living imec strategic funding 2017; Ph.D. grants Industrial Research fund (IOF): IOF Fellowship 13-0260 VLK Stichting E. van der Schueren: Rectal Cancer EU H2020-SC1-2016-2017; SB-Grant of the FWO (formerly IWT); 10.13039/501100004040 - MIDAS Meaningful Integration of Data, Analytics and Services KU Leuven Internal Funds; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7948756","Pattern recognition and classification;linear systems;machine learning","Cepstral analysis;Computational modeling;Data models;Euclidean distance;Linear systems;Time measurement;Time series analysis","computational complexity;distance measurement;linear systems;nonlinear dynamical systems;pattern clustering;signal processing;time series","Martin cepstral distance;computational complexity;electrical circuits data;input output signal clustering;model norm-based distance;multiple deterministic linear dynamical systems;time series clustering;time series distance measure","","","","","","","20170614","Oct. 2017","","IEEE","IEEE Journals & Magazines"
"The Role of Data Analysis in the Development of Intelligent Energy Networks","Z. Ma; J. Xie; H. Li; Q. Sun; Z. Si; J. Zhang; J. Guo","Beijing Univ. of Posts & Telecommun., Beijing, China","IEEE Network","20170928","2017","31","5","88","95","Data analysis plays an important role in the development of intelligent energy networks (IENs). This article reviews and discusses the application of data analysis methods for energy big data. The installation of smart energy meters has provided a huge volume of data at different time resolutions, suggesting data analysis is required for clustering, demand forecasting, energy generation optimization, energy pricing, monitoring and diagnostics. The currently adopted data analysis technologies for IENs include pattern recognition, machine learning, data mining, statistics methods, and so on. However, existing methods for data analysis cannot fully meet the requirements for processing the big data produced by IENs, therefore more comprehensive data analysis methods are needed to handle the increasing amount of data and to mine more valuable information.","0890-8044;08908044","","10.1109/MNET.2017.1600319","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8053484","","Big Data;Data analysis;Energy consumption;Energy measurement;Intelligent systems;Smart meters","Big Data;data analysis;data mining;learning (artificial intelligence);load forecasting;power engineering computing;power meters;power system measurement;statistical analysis","IEN;clustering;comprehensive data analysis methods;currently adopted data analysis technologies;data mining;demand forecasting;energy big data;energy generation optimization;energy pricing;intelligent energy networks;smart energy meters;statistics methods","","","","","","","","2017","","IEEE","IEEE Journals & Magazines"
"K*-Means: An Effective and Efficient K-Means Clustering Algorithm","J. Qi; Y. Yu; L. Wang; J. Liu","Sch. of Comput. & Control Eng., Yantai Univ., Yantai, China","2016 IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking (SocialCom), Sustainable Computing and Communications (SustainCom) (BDCloud-SocialCom-SustainCom)","20161031","2016","","","242","249","K-means is a widely used clustering algorithm in field of data mining across different disciplines in the past fifty years. However, k-means heavily depends on the position of initial centers, and the chosen starting centers randomly may lead to poor quality of clustering. Motivated by this, this paper proposes an optimized k-means clustering method along with three optimization principles named k*-means. First, we propose a hierarchical optimization principle initialized by k* cluster centers (k* > k) to reduce the risk of randomly seeds selection, and then utilize proposed top-n method to merge the nearest clusters associated with the shortest n edges in each round until the number of clusters reaches at k. Second, we propose a cluster pruning strategy for improving efficiency of k-means by omitting the farther clusters to shrink the adjustable searching space for each point in each iteration. Third, we implement an optimized update theory to optimize the k-means iteration updating, which leverages moved points updating instead of recalculating mean and SSE of cluster to minimize computation cost. Our comprehensive experimental studies, using 2 synthetic datasets and 4 real world datasets from the UCI Machine Learning Repository, demonstrate that our method outperforms state-of-the-art methods in both effectiveness and efficiency.","","","10.1109/BDCloud-SocialCom-SustainCom.2016.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7723700","Cluster pruning;Clustering;Data mining;Top-n merging;k-means","Acceleration;Clustering algorithms;Data mining;Indexes;Merging;Optimization;Sensitivity","data mining;iterative methods;optimisation;pattern clustering;search problems","cluster pruning;data mining;hierarchical optimization principle;k* cluster centers;k*-means;k-means iteration updating;nearest clusters;optimized k-means clustering algorithm;optimized update theory;searching space;seeds selection;top-n method","","1","","","","","","8-10 Oct. 2016","","IEEE","IEEE Conferences"
"Inverse extreme learning machine for learning with label proportions","L. Cui; J. Zhang; Z. Chen; Y. Shi; P. S. Yu","School of Computer and Control Engineering, University of Chinese Academy of Sciences, Beijing, China","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","576","585","In large-scale learning problem, the scalability of learning algorithms is usually the key factor affecting the algorithm practical performance, which is determined by both the time complexity of the learning algorithms and the amount of supervision information (i.e., labeled data). Learning with label proportions (LLP) is a new kind of machine learning problem which has drawn much attention in recent years. Different from the well-known supervised learning, LLP can estimate a classifier from groups of weakly labeled data, where only the positive/negative class proportions of each group are known. Due to its weak requirements for the input data, LLP presents a variety of real-world applications in almost all the fields involving anonymous data, like computer vision, fraud detection and spam filtering. However, even through the required labeled data is of a very small amount, LLP still suffers from the long execution time a lot due to the high time complexity of the learning algorithm itself. In this paper, we propose a very fast learning method based on inversing output scaling process and extreme learning machine, namely Inverse Extreme Learning Machine (IELM), to address the above issues. IELM can speed up the training process by order of magnitudes for large datasets, while achieving highly competitive classification accuracy with the existing methods at the same time. Extensive experiments demonstrate the significant speedup of the proposed method. We also demonstrate the feasibility of IELM with a case study in real-world setting: modeling image attributes based on ImageNet Object Attributes dataset.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8257973","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8257973","Learning with label proportions;attribute modeling;classifier calibration;extreme learning machine;semi-supervised learning","Calibration;Optimization;Supervised learning;Support vector machines;Training;Visualization","learning (artificial intelligence);pattern classification","algorithm practical performance;anonymous data;input data;inverse extreme learning machine;label proportions;large-scale learning problem;learning algorithm;machine learning problem;required labeled data;supervised learning;weakly labeled data","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"A Parallel K-Medoids Algorithm for Clustering based on MapReduce","M. O. Shafiq; E. Torunski","Sch. of Inf. Technol., Carleton Univ., Ottawa, ON, Canada","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","502","507","One of the most important machine learning techniques include clustering of data into different clusters or categories. There are several decent algorithms and techniques that exist to perform clustering on small to medium scale data. In the era of Big Data and with applications being large-scale and data-intensive in nature, there is a significant increment in volume, variety and velocity of data in the form of log events produced by such applications. This makes the task of clustering of huge amounts of data more challenging and limited. In this paper, we present our approach of a parallel K-Medoids clustering algorithm based on MapReduce paradigm to be able to perform clustering on large-scale of data. We have kept our solution simple and feasible to be used to handle huge volume, variety and velocity of data. Another key uniqueness in our proposed algorithm is that it can achieve parallelism independent of the number of k clusters to be formed, unlike other related approaches. We have tested our algorithm on large amounts of data and on a real-life case-study.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838193","Clustering; K-Medoids; Big Data; MapReduce","Algorithm design and analysis;Big data;Clustering algorithms;Computational modeling;Computers;Monitoring;Program processors","Big Data;learning (artificial intelligence);parallel processing;pattern clustering","Big Data;MapReduce;data variety;data velocity;data volume;large-scale data clustering;machine learning;medium scale data;parallel K-Medoids clustering","","1","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conferences"
"Big data machine learning using apache spark MLlib","M. Assefi; E. Behravesh; G. Liu; A. P. Tafti","Department of Computer Science, University of Georgia, GA 30602, USA","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","3492","3498","Artificial intelligence, and particularly machine learning, has been used in many ways by the research community to turn a variety of diverse and even heterogeneous data sources into high quality facts and knowledge, providing premier capabilities to accurate pattern discovery. However, applying machine learning strategies on big and complex datasets is computationally expensive, and it consumes a very large amount of logical and physical resources, such as data file space, CPU, and memory. A sophisticated platform for efficient big data analytics is becoming more important these days as the data amount generated in a daily basis exceeds over quintillion bytes. Apache Spark MLlib is one of the most prominent platforms for big data analysis which offers a set of excellent functionalities for different machine learning tasks ranging from regression, classification, and dimension reduction to clustering and rule extraction. In this contribution, we explore, from the computational perspective, the expanding body of the Apache Spark MLlib 2.0 as an open-source, distributed, scalable, and platform independent machine learning library. Specifically, we perform several real world machine learning experiments to examine the qualitative and quantitative attributes of the platform. Furthermore, we highlight current trends in big data machine learning research and provide insights for future work.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258338","Apache Spark MLlib;Big Data Analytics;Big Data Machine Learning;Machine Learning","Big Data;Computer architecture;Data models;Libraries;Machine learning algorithms;Sparks","Big Data;data analysis;data mining;learning (artificial intelligence)","Apache Spark MLlib 2;artificial intelligence;big data analysis;big data analytics;big data machine learning research;big datasets;complex datasets;computational perspective;diverse data sources;heterogeneous data sources;logical resources;machine learning strategies;pattern discovery;physical resources;platform independent machine learning library;premier capabilities;qualitative attribute;quantitative attribute;research community","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"A Recommendation System Combining LDA and Collaborative Filtering Method for Scenic Spot","S. Xie; Y. Feng","Hangzhou On Honest Tech. Co., Ltd., Hangzhou, China","2015 2nd International Conference on Information Science and Control Engineering","20150611","2015","","","67","71","Researchers have long sought to find an effective and straightforward method to bridge the gap between us and big data. Especially during the big data era, how to find the needed information with rapid speed and exact result has become the central concerns of the internet users. This paper focuses on exploring the valuable data in UGC (User Generated Content), and recommending useful information to specified users. To achieve this goal, we model the social network, and then the LDA (Linear Discriminant Analysis), PCA (Principal Component Analysis) and KNN (K-Nearest Neighbour) algorithms are adopted to calculate the recommendation items. Our algorithm avoids the disadvantages of the common collaborative filtering algorithm that only behaviors is considered but without considering the behaviour results, thus our method effectively improves the accuracy of the recommendation system. Experimental results show that our algorithm improves the accuracy comparing with the CF algorithms.","","Electronic:978-1-4673-6850-6; POD:978-1-4673-6851-3","10.1109/ICISCE.2015.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7120564","K-Nearest Neighbour;Recommendation System for Scenic Spot;SVD;Topic Model","Algorithm design and analysis;Classification algorithms;Clustering algorithms;Collaboration;Filtering;Machine learning algorithms;Matrix decomposition","collaborative filtering;learning (artificial intelligence);principal component analysis;recommender systems;social networking (online)","CF algorithms;K-nearest neighbor algorithms;KNN;LDA;PCA;Scenic Spot;UGC;collaborative filtering method;linear discriminant analysis;principal component analysis;recommendation system;social network;user generated content","","0","","20","","","","24-26 April 2015","","IEEE","IEEE Conferences"
"Eagle: User profile-based anomaly detection for securing Hadoop clusters","C. Gupta; R. Sinha; Y. Zhang","eBay Inc. San Jose, CA, USA","2015 IEEE International Conference on Big Data (Big Data)","20151228","2015","","","1336","1343","Existing Big data analytics platforms, such as Hadoop, lack support for user activity monitoring. Several diagnostic tools such as Ganglia, Ambari, and Cloudera Manager are available to monitor health of a cluster, however, they do not provide algorithms to detect security threats or perform user activity monitoring. Hence, there is a need to develop a scalable system that can detect malicious user activities, especially in real-time, so that appropriate actions can be taken against the user. At eBay, we developed such a system named Eagle, which collects audit logs from Hadoop clusters and applications running on them, analyzes users behavior, generates profiles per user of the system, and predicts anomalous user activities based on their prior profiles. Eagle is a highly scalable system, capable of monitoring multiple eBay clusters in real-time. It includes machine-learning algorithms that create user profiles based on the user's history of activities. As far as we know, this is the first activity monitoring system on the Hadoop-ecosystem for the detection of intrusion-related activities using behavior-based profiles of users. When a user performs any operation in the cluster, Eagle matches current user action against his prior activity pattern and raises alarm if it suspects anomalous action. We investigate two machine-learning algorithms: density estimation, and principal component analysis (PCA). In this paper, we introduce the Eagle system, discuss the algorithms in detail, and show performance results. We demonstrate that the sensitivity of the density estimation algorithm is 93%, however the sensitivity of our system increases by 4.94% (on average) to 98% (approximately) by using an ensemble of the two algorithms during anomaly detection.","","Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9","10.1109/BigData.2015.7363892","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363892","Hadoop;anomaly detection;cluster;machine learning;user activity monitoring;user profiles","Algorithm design and analysis;Clustering algorithms;Estimation;Monitoring;Principal component analysis;Real-time systems;Training","Big Data;learning (artificial intelligence);principal component analysis;security of data","Big data analytics;Eagle system;Hadoop cluster;Hadoop-ecosystem;PCA;density estimation;eBay;intrusion-related activity;machine-learning algorithm;principal component analysis;security threat;user profile-based anomaly detection","","","","30","","","","Oct. 29 2015-Nov. 1 2015","","IEEE","IEEE Conferences"
"Multi-dimensional and customizable open-source labware for promoting big data analytical skills in STEM education","Ying Xie; Kai Qian; Jing He","Department of Computer Science, Kennesaw State University, Georgia, USA","2016 IEEE Frontiers in Education Conference (FIE)","20161201","2016","","","1","5","In order to remove resource barriers and smooth the learning curve for education on big data analytics in STEM disciplines, we develop an portable open source labware that is called STEM-BD for promoting education on big data analytics. STEM-BD integrates the following four critical components, big data platform, big data sets, data analytics algorithms and hands-on lab exercises in a multi-dimensional and customizable way. In this paper, we provide a detailed description of the design goal of STEM-BD, its prototype, preliminary evaluation results, and future development.","","Electronic:978-1-5090-1790-4; POD:978-1-5090-1791-1","10.1109/FIE.2016.7757700","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7757700","STEM education;big data;component;labware","Algorithm design and analysis;Big data;Classification algorithms;Clustering algorithms;Education;Machine learning algorithms;Sparks","Big Data;STEM;computer aided instruction;computer science education;laboratories;public domain software","Big Data analytical skills;Big Data sets;STEM education;STEM-BD;data analytics algorithms;hands-on lab exercises;multidimensional-customizable-portable open-source labware","","","","","","","","12-15 Oct. 2016","","IEEE","IEEE Conferences"
"Towards a virtual environment for interactive analysis of cluster-based flow pattern abstraction","S. Roy; Y. Hu; R. J. Martinuzzi; C. Morton","Dept. of Electrical and Computer Engineering, University of Calgary, Calgary, AB, Canada","2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20171130","2017","","","3547","3552","Recent research efforts show the benefits of using machine learning and interactive visualizations in data analytics. However, there is a void in the implementation of these techniques for the analysis of large and complex 4-dimentional (4D) unsteady flows. Hence, this paper presents an initial development of a virtual environment (VE) to fill this void. The VE has a two-layer architecture with different technologies running in the back and fore grounds. Using machine learning, the background layer implements clustering algorithms for abstracting spatiotemporal patterns of the flows like flow features, spatial regions and temporal phases. The outputs of the clustering algorithms are fed to the foreground layer for interactive selection, sorting and filtering of these patterns. The operations in the foreground make up our designed techniques of flow pattern analysis that aims to provide greater comprehension of 4D flow data. Running in the foreground, the virtual reality (VR) technologies of stereoscopic rendering and haptic feedback further enhance these analysis techniques. Thus, our work introduces a novel environment for the interactive analysis of cluster-based flow pattern abstractions.","","Electronic:978-1-5386-1645-1; POD:978-1-5386-1646-8; USB:978-1-5386-1644-4","10.1109/SMC.2017.8123181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8123181","3D visualization;Virtual reality;clustering;flow features;interactive visual analytics","Clustering algorithms;Data visualization;Haptic interfaces;Image color analysis;Rendering (computer graphics);Three-dimensional displays;Virtual environments","data analysis;data visualisation;haptic interfaces;learning (artificial intelligence);pattern clustering;rendering (computer graphics);virtual reality","4-dimentional unsteady flows;4D flow data;abstracting spatiotemporal patterns;analysis techniques;background layer;cluster-based flow pattern abstraction;clustering algorithms;data analytics;designed techniques;flow features;flow pattern analysis;foreground layer;haptic feedback;interactive analysis;interactive selection;interactive visualizations;machine learning;recent research efforts;two-layer architecture;virtual environment;virtual reality technologies","","","","","","","","5-8 Oct. 2017","","IEEE","IEEE Conferences"
"Designing reconfigurable large-scale deep learning systems using stochastic computing","A. Ren; Z. Li; Y. Wang; Q. Qiu; B. Yuan","Dept. of Electrical Engineering & Computer Science, Syracuse University, NY 13244, USA","2016 IEEE International Conference on Rebooting Computing (ICRC)","20161110","2016","","","1","7","Deep Learning, as an important branch of machine learning and neural network, is playing an increasingly important role in a number of fields like computer vision, natural language processing, etc. However, large-scale deep learning systems mainly operate in high-performance server clusters, thus restricting the application extensions to personal or mobile devices. The solution proposed in this paper is taking advantage of the fantastic features of stochastic computing methods. Stochastic computing is a type of data representation and processing technique, which uses a binary bit stream to represent a probability number (by counting the number of ones in this bit stream). In the stochastic computing area, some key arithmetic operations such as additions or multiplications can be implemented with very simple components like AND gates or multiplexers, respectively. Thus it provides an immense design space for integrating a large amount of neurons and enabling fully parallel and scalable hardware implementations of large-scale deep learning systems. In this paper, we present a reconfigurable large-scale deep learning system based on stochastic computing technologies, including the design of the neuron, the convolution function, the back-propagation function and some other basic operations. And the network-on-chip technique is also proposed in this paper to achieve the goal of implementing a large-scale hardware system. Our experiments validate the functionality of reconfigurable deep learning systems using stochastic computing, and demonstrate that when the bit streams are set to be 8192 bits, classification of MNIST digits by stochastic computing can perform as low error rate as that by normal arithmetic operations.","","Electronic:978-1-5090-1370-8; POD:978-1-5090-1371-5","10.1109/ICRC.2016.7738685","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7738685","Stochastic computing;deep learning;large-scale;neuron;reconfigurable","Biological neural networks;Encoding;Hardware;Machine learning;Neurons;Stochastic processes","backpropagation;data structures;learning (artificial intelligence);network-on-chip;neural nets;parallel architectures;probability;reconfigurable architectures","AND gates;MNIST digit classification;backpropagation function;data processing;data representation;high-performance server clusters;key arithmetic operations;large-scale hardware system;machine learning;mobile devices;multiplexers;network-on-chip;neural network;normal arithmetic operations;parallel scalable hardware implementations;personal devices;probability number;reconfigurable large-scale deep learning systems;stochastic computing","","7","","","","","","17-19 Oct. 2016","","IEEE","IEEE Conferences"
"Data-Driven Network Optimization in Ultra-Dense Radio Access Networks","S. Huang; Q. Liu; T. Han; N. Ansari","Dept. of Electr. & Comput. Eng., Univ. of North Carolina at Charlotte, Charlotte, NC, USA","GLOBECOM 2017 - 2017 IEEE Global Communications Conference","20180115","2017","","","1","6","The complexity of networking mechanisms will increase significantly because of the dense deployment of radio base stations in ultra-dense mobile networks. As a result, the existing networking mechanisms may be unable to efficiently manage ultra-dense mobile networks. To solve this problem, we propose a data driven network optimization framework which integrates the big data analysis methods with networking mechanisms. In the proposed framework, we adopt big data analysis methods to divide densely deployed base stations into groups. Then, each group of base stations are managed with networking mechanisms independently. In this way, the complexity of the networking mechanisms is reduced. The key challenge in designing the framework is to optimally group base stations into clusters in real time. Addressing this challenge, the proposed framework consists of an offline machine learning module and an online base station clustering and network optimization module. The offline machine learning module predicts the optimal number of base station groups in the next time interval based on the historical data. The online base station clustering and network optimization module clusters base stations and optimize the network in real time. The performance of the proposed data-driven network management framework is validated through network simulations with real network data traces.","","Electronic:978-1-5090-5019-2; POD:978-1-5090-5020-8","10.1109/GLOCOM.2017.8255003","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8255003","","Base stations;Clustering algorithms;Complexity theory;Load management;Mobile communication;Mobile computing;Optimization","Big Data;data analysis;learning (artificial intelligence);mobile radio;neural nets;optimisation;pattern clustering;radio access networks;telecommunication computing;telecommunication network management","base station groups;big data analysis methods;data driven network optimization framework;data-driven network optimization;densely deployed base stations;network data traces;network management framework;networking mechanisms;offline machine learning module;online base station clustering;optimally group base stations;radio base stations;ultradense mobile networks;ultradense radio access networks","","","","","","","","4-8 Dec. 2017","","IEEE","IEEE Conferences"
"Flux: Groupon's automated, scalable, extensible machine learning platform","D. C. Spell; X. H. T. Zeng; J. Y. Chung; B. Nooraei; R. T. Shomer; L. Y. Wang; J. C. Gibson; D. Kirsche","Groupon, Inc.","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","1554","1559","As machine learning becomes the driving force of the daily operation of companies within the information technology sector, infrastructure that enables automated, scalable machine learning is a core component of the systems of many large companies. Various systems and products are being built, offered, and open sourced. As an e-commerce company, numerous aspects of Groupon's business is driven by machine learning. To solve the scalability issue and provide a seamless collaboration between data scientists and engineers, we built Flux, a system that expedites the deployment, execution, and monitoring of machine learning models. Flux focuses on enabling data scientists to build model prototypes with languages and tools they are most proficient in, and integrating the models into the enterprise production system. It manages the life cycle of deployed models, and executes them in distributed batch mode, or exposes them as micro-services for real-time use cases. Its design focuses on automation and easy management, scalability, and extensibility. Flux is the central system for supervised machine learning tasks at Groupon and has been supporting multiple teams across the company.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258089","machine learning automation;machine learning infrastructure;machine learning system","Computational modeling;Data models;Engines;Libraries;Predictive models;Real-time systems;Tools","electronic commerce;learning (artificial intelligence)","Groupon's business;data scientists;e-commerce company;enterprise production system;information technology sector;scalable machine learning;supervised machine learning tasks","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Manifold regularized robust unsupervised feature selection for image clustering","Y. Shi; S. Du","School of Electrical Engineering, Northwest Minzu University, Lanzhou 730030, China","2017 36th Chinese Control Conference (CCC)","20170911","2017","","","11161","11165","Dimensionality reduction is a challenging task for high dimensional data processing in machine learning and data mining. As an effective dimension reduction technique, unsupervised feature selection aims at finding a subset of features to retain the most relevant information. In this paper, we propose a novel unsupervised feature selection method, called Manifold Regularized Robust Unsupervised Feature Selection (MRUFS) for image clustering. MRUFS performs robust discriminative feature selection and robust clustering simultaneously under ℓ_<sub>2</sub>,<sub>1</sub>-norm while preserves the local manifold structures of original data. Compared with several unsupervised feature selection methods, the proposed algorithm comes with better clustering performance for two datasets: FERET and COIL20 which we have experimented with here.","","Electronic:978-988-15639-3-4; POD:978-1-5386-2918-5","10.23919/ChiCC.2017.8029138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029138","dimensionality reduction;manifold regularization;matrix factorization;unsupervised feature selection","Databases;Feature extraction;Manifolds;Matrix decomposition;Optimization;Robustness;Sparse matrices","data mining;feature selection;image processing;learning (artificial intelligence);pattern clustering","MRUFS;data mining;dimensionality reduction;high dimensional data processing;image clustering;machine learning;manifold regularized robust unsupervised feature selection;robust clustering;robust discriminative feature selection","","","","","","","","26-28 July 2017","","IEEE","IEEE Conferences"
"Affordable supercomputing using open source software","D. Trejo; I. Obeid; J. Picone","The Neural Engineering Data Consortium, Temple University, USA","2015 IEEE Signal Processing in Medicine and Biology Symposium (SPMB)","20160215","2015","","","1","2","Big data and machine learning require powerful centralized computing systems. Small research groups cannot afford to support large, expensive computing infrastructure. Cloud computing options, such as renting cycles from Amazon AWS, can often end up costing more than hosting hardware locally, and pose challenges when attempting to move big data resources across the network (or staging them remotely on the server). Open source projects are enabling the development of low cost scalable clusters and are significantly lowering the barrier for administrating and maintaining these clusters. In this poster, we explore the tradeoffs a small research group faces in constructing a cost-effective cluster. We present an affordable approach to cluster computing that uses commodity processors and open source software. Though the overall system is not novel, we believe the lessons learned in this project can be a valuable guide for small research groups interested in building such clusters.","","Electronic:978-1-5090-1350-0; POD:978-1-5090-1351-7","10.1109/SPMB.2015.7405431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7405431","","Big data;Hardware;Investment;Monitoring;Parallel processing;Program processors;Servers","Big Data;cloud computing;learning (artificial intelligence);multiprocessing systems;public domain software","Big Data;centralized computing system;cloud computing option;cluster computing approach;commodity processors;machine learning;open source software;supercomputing","","","","","","","","12-12 Dec. 2015","","IEEE","IEEE Conferences"
"Distributed Kernel Matrix Approximation and Implementation Using Message Passing Interface","T. A. Dameh; W. Abd-Almageed; M. Hefeeda","Sch. of Comput. Sci., Simon Fraser Univ., Burnaby, BC, Canada","2013 12th International Conference on Machine Learning and Applications","20140410","2013","1","","52","57","We propose a distributed method to compute similarity (also known as kernel and Gram) matrices used in various kernel-based machine learning algorithms. Current methods for computing similarity matrices have quadratic time and space complexities, which make them not scalable to large-scale data sets. To reduce these quadratic complexities, the proposed method first partitions the data into smaller subsets using various families of locality sensitive hashing, including random project and spectral hashing. Then, the method computes the similarity values among points in the smaller subsets to result in approximated similarity matrices. We analytically show that the time and space complexities of the proposed method are sub quadratic. We implemented the proposed method using the Message Passing Interface (MPI) framework and ran it on a cluster. Our results with real large-scale data sets show that the proposed method does not significantly impact the accuracy of the computed similarity matrices and it achieves substantial savings in running time and memory requirements.","","CD-ROM:978-1-4799-4154-4; Electronic:978-0-7695-5144-9; POD:978-1-4799-4155-1","10.1109/ICMLA.2013.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784587","Large-scale data processing;big data;distributed clustering;kernel matrix approximation;kernel-based algorithms","Accuracy;Algorithm design and analysis;Approximation algorithms;Approximation methods;Clustering algorithms;Complexity theory;Kernel","computational complexity;learning (artificial intelligence);matrix algebra;message passing","MPI framework;approximated similarity matrices;distributed kernel matrix approximation;gram matrix;kernel-based machine learning algorithm;large-scale data sets;locality sensitive hashing;message passing interface;quadratic time and space complexity;random project;similarity values;spectral hashing","","0","","23","","","","4-7 Dec. 2013","","IEEE","IEEE Conferences"
"Automatic detection of subsurface defects in composite materials using thermography and unsupervised machine learning","R. Marani; D. Palumbo; U. Galietti; E. Stella; T. D'Orazio","Institute of Intelligent Systems for Automation, Italian National Research Council, via Amendola 122 D/O, Bari, Italy","2016 IEEE 8th International Conference on Intelligent Systems (IS)","20161110","2016","","","516","521","This paper presents a complete framework aimed to nondestructive inspection of composite materials. Starting from the acquisition, performed with lock-in thermography, the method flows through a set of consecutive blocks of data processing: input enhancement, feature extraction, classification and defect detection. Experimental results prove the capability of the presented methodology to detect the presence of defects underneath the surface of a calibrated specimen made of Glass Fiber Reinforced Polymer (GFRP). Results are also compared with those obtained by other techniques, based on different features and unsupervised learning methods. The comparison further proves that the proposed methodology is able to reduce the number of false positives, while ensuring the exact detection of subsurface defects.","","Electronic:978-1-5090-1354-8; POD:978-1-5090-1355-5; USB:978-1-5090-1353-1","10.1109/IS.2016.7737471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7737471","Composite materials;Hierarchical clustering;Nondestructive inspection;SURF;Thermography;Unsupervised machine learning","Composite materials;Data processing;Feature extraction;Heat transfer;Histograms;Inspection;Robustness","fault diagnosis;feature extraction;glass fibre reinforced composites;image classification;image enhancement;infrared imaging;inspection;materials science computing;polymers;unsupervised learning","GFRP;composite material;feature classification;feature extraction;glass fiber reinforced polymer;input enhancement;nondestructive inspection;subsurface defect detection;thermography;unsupervised machine learning","","2","","","","","","4-6 Sept. 2016","","IEEE","IEEE Conferences"
"On the role of feature space granulation in feature selection processes","M. Grzegorowski; A. Janusz; D. Ślezak; M. Szczuka","Institute of Informatics, University of Warsaw, Banacha 2, 02-097 Warsaw, Poland","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","1806","1815","Information granulation plays an important role in the process of scaling up modern machine learning and knowledge discovery algorithms. By employing compact descriptions of granules - whereby granules are defined as collections of original data elements gathered together by means of their similarity, proximity or functionality - one can drastically accelerate computations and, moreover, make the results of those computations more meaningful for domain experts. In this paper, we summarize some of the feature space granulation approaches introduced by now. We discuss the meaning of similarity, proximity and functionality while considering the granules of physically existing or potentially derivable attributes. We also show several examples of utilization of the granulation structures defined over the feature spaces in the feature selection algorithms. As a case study, we consider the algorithms developed within the theory of rough sets, aimed at finding irreducible subsets of attributes that are sufficient to distinguish between the cases belonging to different target decision classes.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258124","Attribute Clustering;Attribute Hierarchies;Feature Selection;Information Granulation;Rough Sets","Big Data;Clustering algorithms;Complexity theory;Feature extraction;Rough sets;Sensor phenomena and characterization","approximation theory;data handling;data mining;feature selection;fuzzy set theory;learning (artificial intelligence);rough set theory","compact descriptions;feature selection algorithms;feature selection processes;feature space granulation approaches;functionality;granulation structures;granules;information granulation;knowledge discovery algorithms;modern machine learning;original data elements;physically existing attributes;potentially derivable attributes;proximity;rough set theory;similarity;target decision classes","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Superpixel Clustering and Planar Fit Segmentation of 3D LIDAR Point Clouds","H. Mahmoudabadi; T. Shoaf; M. J. Olsen","Sch. of Civil & Constr. Eng., Oregon State Univ., Corvallis, OR, USA","2013 Fourth International Conference on Computing for Geospatial Research and Application","20130919","2013","","","1","7","Terrestrial laser scanning (TLS, also called ground based Light Detection and Ranging, LIDAR) is an effective data acquisition method capable of high precision, detailed 3D models for surveying natural environments. However, despite the high density, and quality, of the data itself, the data acquired contains no direct intelligence necessary for further modeling and analysis - merely the 3D geometry (XYZ), 3-component color (RGB), and laser return signal strength (I) for each point. One common task for LIDAR data processing is the selection of an appropriate methodology for the extraction of geometric features from the irregularly distributed point clouds. Such recognition schemes must accomplish both segmentation and classification. Planar (or other geometrically primitive) feature extraction is a common method for point cloud segmentation, however, current algorithms are computationally expensive and often do not utilize color or intensity information. In this paper we present an efficient algorithm, that takes advantage of both colorimetric and geometric data as input and consists of three principal steps to accomplish a more flexible form of feature extraction. First, we employ a Simple Linear Iterative Clustering (SLIC) super pixel algorithm for clustering and dividing the colorimetric data. Second, we use a plane-fitting technique on each significantly smaller cluster to produce a set of normal vectors corresponding to each super pixel. Last, we utilize a Least Squares Multi-class Support Vector Machine (LSMSVM) to classify each cluster as either ""ground"", ""wall"", or ""natural feature"". Despite the challenging problems presented by the occlusion of features during data acquisition, our method effectively generates accurate (>85%) segmentation results by utilizing the color space information, in addition to the standard geometry, during segmentation.","","Electronic:978-0-7695-5012-1; POD:978-1-4799-0301-6","10.1109/COMGEO.2013.2","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6602033","Clustering;LIDAR;Laser Point Cloud;Machine Learning;SVM;Superpixel","Clustering algorithms;Feature extraction;Image color analysis;Laser modes;Support vector machines;Three-dimensional displays;Vectors","feature extraction;image colour analysis;iterative methods;least squares approximations;optical radar;radar computing;support vector machines","3-component color;3D LIDAR point clouds;3D geometry;LIDAR data processing;SLIC super pixel algorithm;classification;color space information;colorimetric data;data acquisition method;feature occlusion;geometric data;geometric feature extraction;ground-based light detection and ranging;high-precision detailed 3D model;irregularly-distributed point clouds;laser return signal strength;least square multiclass support vector machine;planar feature extraction;planar fit segmentation;point cloud segmentation;simple-linear iterative clustering super pixel algorithm;terrestrial laser scanning","","1","","17","","","","22-24 July 2013","","IEEE","IEEE Conferences"
"GPU-Accelerated Parallel Hierarchical Extreme Learning Machine on Flink for Big Data","C. Chen; K. Li; A. Ouyang; Z. Tang; K. Li","College of Information Science and Engineering, Hunan University, Changsha, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","20170914","2017","47","10","2740","2753","The extreme learning machine (ELM) has become one of the most important and popular algorithms of machine learning, because of its extremely fast training speed, good generalization, and universal approximation/classification capability. The proposal of hierarchical ELM (H-ELM) extends ELM from single hidden layer feedforward networks to multilayer perceptron, greatly strengthening the applicability of ELM. Generally speaking, during training H-ELM, large-scale datasets (DSTs) are needed. Therefore, how to make use of H-ELM framework in processing big data is worth further exploration. This paper proposes a parallel H-ELM algorithm based on Flink, which is one of the in-memory cluster computing platforms, and graphics processing units (GPUs). Several optimizations are adopted to improve the performance, such as cache-based scheme, reasonable partitioning strategy, memory mapping scheme for mapping specific Java virtual machine objects to buffers. Most importantly, our proposed framework for utilizing GPUs to accelerate Flink for big data is general. This framework can be utilized to accelerate many other variants of ELM and other machine learning algorithms. To the best of our knowledge, it is the first kind of library, which combines in-memory cluster computing with GPUs to parallelize H-ELM. The experimental results have demonstrated that our proposed GPU-accelerated parallel H-ELM named as GPH-ELM can efficiently process large-scale DSTs with good performance of speedup and scalability, leveraging the computing power of both CPUs and GPUs in the cluster.","2168-2216;21682216","","10.1109/TSMC.2017.2690673","International Science and Technology Cooperation Program of China; Key Technology Research and Development Programs of Guangdong Province; National High-Tech Research and Development Program of China; 10.13039/501100001809 - International (Regional) Cooperation and Exchange Program of National Natural Science Foundation of China; 10.13039/501100001809 - Key Program of National Natural Science Foundation of China; 10.13039/501100001809 - National Natural Science Foundation of China; 10.13039/501100001809 - National Outstanding Youth Science Program of National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7908958","Big data;GPGPU;deep learning (DL);flink;hierarchical extreme learning machine (H-ELM);parallel","Acceleration;Approximation algorithms;Big Data;Clustering algorithms;Libraries;Machine learning algorithms;Training","Big Data;graphics processing units;learning (artificial intelligence);parallel algorithms","Big Data;Flink;GPU-accelerated parallel hierarchical extreme learning machine;Java virtual machine;graphics processing units;in-memory cluster computing platforms;multilayer perceptron;parallel H-ELM algorithm;single hidden layer feedforward networks","","","","","","","20170424","Oct. 2017","","IEEE","IEEE Journals & Magazines"
"A novel approach to optimization of iterative machine learning algorithms: Over heap structure","H. Kurban; M. M. Dalkilic","Department of Computer Science, Indiana University, Bloomington, IN 47405, United States","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","102","109","Iterative machine learning algorithms, i.e., k-means (KM), expectation maximization (EM), become overwhelmed with big data since all data points are being continually and indiscriminately visited while a cost is being minimized. In this work, we demonstrate (1) an optimization approach to reduce training run-time complexity of iterative machine learning algorithms and (2) implementation of this framework over KM algorithm. We call this extended KM algorithm, KM*. The experimental results show that KM* outperforms KM over big real world and synthetic data sets. Lastly, we demonstrate the theoretical elements of our work.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8257917","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8257917","big data;clustering;data mining;heap;k-means;machine learning;optimization","Big Data;Clustering algorithms;Convergence;Iterative algorithms;Machine learning algorithms;Optimization","computational complexity;expectation-maximisation algorithm;iterative methods;learning (artificial intelligence);optimisation","big data;data points;expectation maximization;extended KM algorithm;heap structure;iterative machine learning algorithm optimization;k-means;optimization approach;run-time complexity training;synthetic data sets","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Distributed data augmented support vector machine on Spark","T. D. Nguyen; V. Nguyen; T. Le; D. Phung","Center for Pattern Recognition and Data Analytics, Deakin University, Australia","2016 23rd International Conference on Pattern Recognition (ICPR)","20170424","2016","","","498","503","Support vector machines (SVMs) are widely-used for classification in machine learning and data mining tasks. However, they traditionally have been applied to small to medium datasets. Recent need to scale up with data size has attracted research attention to develop new methods and implementation for SVM to perform tasks at scale. Distributed SVMs are relatively new and studied recently, but the distributed implementation for SVM with data augmentation has not been developed. This paper introduces a distributed data augmentation implementation for SVM on Apache Spark, a recent advanced and popular platform for distributed computing that has been employed widely in research as well as in industry. We term our implementation sparkling vector machine (SkVM) which supports both classification and regression tasks by scanning through the data exactly once. In addition, we further develop a framework to handle the data with new classes arriving under an online classification setting where new data points can have labels that have not previously seen - a problem we term label-drift classification. We demonstrate the scalability of our proposed method on large-scale datasets with more than one hundred million data points. The experimental results show that the predictive performances of our method are comparable or better than those of baselines whilst the execution time is much faster at an order of magnitude.","","Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9","10.1109/ICPR.2016.7899683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899683","Apache Spark;big data;distributed computing;large-scale classification;support vector machine","Data models;Distributed databases;Estimation;Industries;Scalability;Sparks;Support vector machines","data mining;image classification;learning (artificial intelligence);support vector machines","Apache Spark;SVM;SkVM;data mining;distributed computing;distributed data augmented support vector machine;image classification;machine learning;regression tasks;sparkling vector machine","","2","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conferences"
"Data Clustering Using Group Search Optimization with Alternative Fitness Functions","L. D. S. Pacifico; T. B. Ludermir","Dept. de Estatistica e Inf., Univ. Fed. Rural de Pernambuco, Recife, Brazil","2016 5th Brazilian Conference on Intelligent Systems (BRACIS)","20170202","2016","","","301","306","Data clustering is an important tool for statistical data analysis and exploration, and it has been successfully applied in many fields like image understanding, bioinformatics, big data mining, and so on. From the past few decades, Evolutionary Algorithms (EAs) have been introduced to deal with clustering task, given their global search capabilities and their mechanisms to escape from local minima points. EAs execution is driven in an attempt to optimize a criterion function, also known as fitness function. In this work, we evaluate the influence of the fitness function on Group Search Optimization (GSO) meta-heuristic when applied to data clustering. Three different fitness function are proposed to GSO. Experiments are performed on twelve benchmark data sets obtained from UCI Machine Learning Repository to evaluate the performance of all alternative GSO models in comparison to other well-known partitional clustering methods from literature.","","Electronic:978-1-5090-3566-3; POD:978-1-5090-3567-0","10.1109/BRACIS.2016.062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839603","data clustering;evolutionary algorithms;group search optimization","Clustering algorithms;Distance measurement;Optimization;Partitioning algorithms;Silicon;Sociology;Statistics","data analysis;evolutionary computation;learning (artificial intelligence);pattern clustering;search problems;statistical analysis","EA;GSO models;UCI machine learning repository;alternative fitness functions;big data mining;bioinformatics;criterion function;data clustering;evolutionary algorithms;group search optimization;image understanding;partitional clustering methods;statistical data analysis","","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conferences"
"A Non-parametric Hidden Markov Clustering Model with Applications to Time Varying User Activity Analysis","W. Wei; C. Liu; M. Y. Zhu; S. A. Matei","Dept. of Stat., Purdue Univ., West Lafayette, IN, USA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","549","554","Activity data of individual users on social media are easily accessible in this big data era. However, proper modeling strategies for user profiles have not been well developed in the literature. Existing methods or models usually have two limitations. The first limitation is that most methods target the population rather than individual users, and the second is that they cannot model non-stationary time-varying patterns. Different users in general demonstrate different activity modes on social media. Therefore, one population model may fail to characterize activities of individual users. Furthermore, online social media are dynamic and ever evolving, so are users' activities. Dynamic models are needed to properly model users' activities. In this paper, we introduce a non-parametric hidden Markov model to characterize the time-varying activities of social media users. An EM algorithm has been developed to estimate the parameters of the proposed model. In addition, based on the proposed model, we develop a clustering method to group users with similar activity patterns.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.200","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424373","B-spline;Clustering;EM Algorithm;Hidden Markov Model;Nonparametric;User Profile Modeling","Analytical models;Clustering algorithms;Data models;Hidden Markov models;Mathematical model;Media;Splines (mathematics)","expectation-maximisation algorithm;hidden Markov models;pattern clustering;social networking (online)","Big Data;EM algorithm;activity modes;activity patterns;dynamic models;expectation-maximization algorithm;nonparametric hidden Markov clustering model;online social media;parameters estimation;population model;time varying user activity analysis","","","","27","","","","9-11 Dec. 2015","","IEEE","IEEE Conferences"
"A data mining model of knowledge discovery based on the deep learning","Y. Ma; Y. Tan; C. Zhang; Y. Mao","Application Management office of SINOPEC IT management Department, Beijing, 100728, China","2015 IEEE 10th Conference on Industrial Electronics and Applications (ICIEA)","20151123","2015","","","1212","1216","With the development of the database technology and the spread of the internet, the amount of the data in databases increases at an exponential speed, which yields the difficult problems of “excess data” and “information explosion”, etc. The traditional database technology is restricted in reading and writing, querying and basic statics operations, but can't acquire the deep data attributes or implicit information. Facing with the huge database in all kinds of fields, it is more and more difficult to cope with the big data only by using conventional technology. New technique to deal with these data at a high level is eagerly demanded. Therefore, the KDD (Knowledge Discovery in Database) technology arises at the historic moment. KDD is an integrated process, which includes data input, iterative solving, user interface and many other custom requirements and design decisions, where the data mining (DM) is a key and specific step in KDD. This paper deeply analyzes state of the art technology of DM, and points out the challenge and technological bottleneck of DM. Moreover, a data mining model architecture of knowledge discovery based on deep learning is proposed.","","Electronic:978-1-4799-8389-6; POD:978-1-4799-8467-1; USB:978-1-4673-7317-3","10.1109/ICIEA.2015.7334292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7334292","KDD;data mining model;deep leaning","Algorithm design and analysis;Classification algorithms;Clustering algorithms;Data mining;Data models;Databases;Machine learning","Internet;data mining;database management systems;iterative methods;learning (artificial intelligence);user interfaces","DM technology;Internet;KDD technology;custom requirements;data input;data mining model architecture;database technology development;deep learning;design decisions;integrated process;iterative solving;knowledge discovery;user interface","","","","26","","","","15-17 June 2015","","IEEE","IEEE Conferences"
"Predicting Transportation Carbon Emission with Urban Big Data","X. Lu; K. Ota; M. Dong; C. Yu; H. Jin","Services Computing Technology and System Lab, Big Data Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Sustainable Computing","20171208","2017","2","4","333","344","Transportation carbon emission is a significant contributor to the increase of greenhouse gases, which directly threatens the change of climate and human health. Under the pressure of the environment, it is very important to master the information of transportation carbon emission in real time. In the traditional way, we get the information of the transportation carbon emission by calculating the combustion of fossil fuel in the transportation sector. However, it is very difficult to obtain the real-time and accurate fossil fuel combustion in the transportation field. In this paper, we predict the real-time and fine-grained transportation carbon emission information in the whole city, based on the spatio-temporal datasets we observed in the city, that is taxi GPS data, transportation carbon emission data, road networks, points of interests (POIs), and meteorological data. We propose a three-layer perceptron neural network (3-layerPNN) to learn the characteristics of collected data and infer the transportation carbon emission. We evaluate our method with extensive experiments based on five real data sources obtained in Zhuhai, China. The results show that our method has advantages over the well-known three machine learning methods (Gaussian Naive Bayes, Linear Regression, and Logistic Regression) and two deep learning methods (Stacked Denoising Autoencoder and Deep Belief Networks).","","","10.1109/TSUSC.2017.2728805","Central Universities; Fundamental Research Funds; KDDI Foundation; National 863 Hi-Tech Research; 10.13039/501100000646 - JSPS KAKENHI; 10.13039/501100001809 - NSFC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7984819","Transportation carbon emission;multilayer perceptron neural network;real-time prediction;urban big data","Carbon dioxide;Fuels;Neural networks;Real-time systems;Smart cities;Urban areas","Bayes methods;Big Data;air pollution;belief networks;combustion;environmental science computing;fossil fuels;learning (artificial intelligence);multilayer perceptrons;regression analysis;transportation","3-layer PNN;China;Gaussian Naive Bayes;POI;Urban Big Data;Zhuhai;climate change;deep belief networks;deep learning methods;fine-grained transportation carbon emission information;fossil fuel combustion;greenhouse gases;human health;linear regression;logistic regression;machine learning methods;meteorological data;points of interests;road networks;spatio-temporal datasets;stacked denoising autoencoder;taxi GPS data;three-layer perceptron neural network;transportation carbon emission data;transportation carbon emission prediction;transportation field;transportation sector","","","","","","","20170719","Oct.-Dec. 1 2017","","IEEE","IEEE Journals & Magazines"
"Predictive medication and use of Big Data","A. Goswami","Cognizant Technology Solutions, California, USA","2017 IEEE Great Lakes Biomedical Conference (GLBC)","20170518","2017","","","1","1","In today's digital age, availability of health related information are not only limited to traditional sources like Clinical data, Claims Data, NIS (National Inpatient Sample) and EMRs but they are also being made available from digital sources like Smart Watch, Health Trackers, Glucose Meters, Blood Pressure Monitors and many other newer personal electronics devices. Patients are also often sharing their medical treatment related information and reaction to certain medicines in Social Media pages like Twitter or Facebook which in turn making the medical data available in the social media pages of renowned pharmaceutical companies. As a result, these digital sources are continuously generating huge amount of health related information and effective use of all these valuable information can help create statistical models that can predict certain medical condition for a patient or provide recommended lifestyle changes or prescription medicines for certain individuals.","","Electronic:978-1-5090-6358-1; POD:978-1-5090-6359-8","10.1109/GLBC.2017.7928885","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7928885","","Classification algorithms;Clustering algorithms;Data models;Facebook;Machine learning algorithms;Medical conditions;Predictive models","Big Data;health care;medical information systems;statistical analysis","Big Data;clinical data;healthcare;predictive medication;statistical models","","","","","","","","6-7 April 2017","","IEEE","IEEE Conferences"
"Table of contents","","","2014 IEEE 26th International Conference on Tools with Artificial Intelligence","20141215","2014","","","v","xvii","The following topics are dealt with: constraint satisfaction problem; machine learning; planning; ontologies; knowledge representation; reasoning; intelligent tutoring systems; clustering; social networks; multi-agent systems; emotion analysis; artificial intelligence; document processing; natural language processing; robotics gesture cognition; parallel SAT solving; big data; data mining; databases; and transportation.","1082-3409;10823409","Electronic:978-1-4799-6572-4; POD:978-1-4799-6573-1","10.1109/ICTAI.2014.4","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6984360","","","Big Data;computability;constraint satisfaction problems;data mining;database management systems;document handling;gesture recognition;intelligent tutoring systems;learning (artificial intelligence);multi-agent systems;natural language processing;network theory (graphs);ontologies (artificial intelligence);pattern clustering;planning (artificial intelligence);robots;transportation","artificial intelligence;big data;clustering;constraint satisfaction problem;data mining;databases;document processing;emotion analysis;intelligent tutoring systems;knowledge representation;machine learning;multiagent systems;natural language processing;ontologies;parallel SAT solving;planning;reasoning;robotics gesture cognition;social networks;transportation","","0","","","","","","10-12 Nov. 2014","","IEEE","IEEE Conferences"
"Understanding and optimizing the performance of distributed machine learning applications on apache spark","C. Dünner; T. Parnell; K. Atasu; M. Sifalakis; H. Pozidis","IBM Research, Z&#x00FC;rich, Switzerland","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","331","338","In this paper we explore the performance limits of Apache Spark for machine learning applications. We begin by analyzing the characteristics of a state-of-the-art distributed machine learning algorithm implemented in Spark and compare it to an equivalent reference implementation using the high performance computing framework MPI. We identify critical bottlenecks of the Spark framework and carefully study their implications on the performance of the algorithm. In order to improve Spark performance we then propose a number of practical techniques to alleviate some of its overheads. However, optimizing computational efficiency and framework related overheads is not the only key to performance - we demonstrate that in order to get the best performance out of any implementation it is necessary to carefully tune the algorithm to the respective trade-off between computation time and communication latency. The optimal trade-off depends on both the properties of the distributed algorithm as well as infrastructure and framework-related characteristics. Finally, we apply these technical and algorithmic optimizations to three different distributed linear machine learning algorithms that have been implemented in Spark. We present results using five large datasets and demonstrate that by using the proposed optimizations, we can achieve a reduction in the performance difference between Spark and MPI from 20x to 2x.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8257942","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8257942","","Algorithm design and analysis;Computational modeling;Machine learning algorithms;Optimization;Partitioning algorithms;Programming;Sparks","cluster computing;computational complexity;distributed algorithms;learning (artificial intelligence);message passing","MPI;Spark framework;Spark performance;algorithmic optimizations;apache spark;computational efficiency;distributed algorithm;distributed linear machine learning;distributed machine;framework-related characteristics;high performance computing;machine learning applications;performance difference;technical optimizations","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Implementing Randomized Matrix Algorithms in Parallel and Distributed Environments","J. Yang; X. Meng; M. W. Mahoney","Inst. for Comput. & Math. Eng., Stanford Univ., Stanford, CA, USA","Proceedings of the IEEE","20151218","2016","104","1","58","92","In this era of large-scale data, distributed systems built on top of clusters of commodity hardware provide cheap and reliable storage and scalable processing of massive data. With cheap storage, instead of storing only currently relevant data, it is common to store as much data as possible, hoping that its value can be extracted later. In this way, exabytes (1018 bytes) of data are being created on a daily basis. Extracting value from these data, however, requires scalable implementations of advanced analytical algorithms beyond simple data processing, e.g., statistical regression methods, linear algebra, and optimization algorithms. Most such traditional methods are designed to minimize floating-point operations, which is the dominant cost of in-memory computation on a single machine. In parallel and distributed environments, however, load balancing and communication, including disk and network input/output (I/O), can easily dominate computation. These factors greatly increase the complexity of algorithm design and challenge traditional ways of thinking about the design of parallel and distributed algorithms. Here, we review recent work on developing and implementing randomized matrix algorithms in large-scale parallel and distributed environments. Randomized algorithms for matrix problems have received a great deal of attention in recent years, thus far typically either in theory or in machine learning applications or with implementations on a single machine.","0018-9219;00189219","","10.1109/JPROC.2015.2494219","U.S. Army Research Office; 10.13039/100000015 - U.S. Department of Energy; 10.13039/100000185 - Defense Advanced Research Projects Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7355313","Big data;distributed matrix algorithms;least absolute deviation;least squares;preconditioning;randomized linear algebra;subspace embedding","Algorithm design and analysis;Approximation algorithms;Approximation methods;Big data;Distributed databases;Machine learning algorithms;Matrix decomposition","iterative methods;matrix algebra;parallel algorithms;randomised algorithms;regression analysis","analytical algorithms;commodity hardware clusters;communication;data extraction value;data processing;data storage;distributed environment;floating-point operation minimization;in-memory computation;input-sparsity time;iterative algorithms;l<sub>1</sub>-regression problem;l<sub>2</sub>-regression problem;large-scale data;linear algebra;load balancing;machine learning;network I/O;network input/output;optimization algorithms;over-constrained problems;parallel environment;preconditioned problem;probability;random projection algorithms;random sampling algorithms;randomized matrix algorithm;relative-error approximate solutions;statistical regression methods","","5","","71","","","20151217","Jan. 2016","","IEEE","IEEE Journals & Magazines"
"An Online-Offline Combined Big Data Mining Platform","W. Zhang; H. Lv; L. Xu; Y. Liu; X. Liu; Q. Lu; Z. Li; J. Zhou","","2017 IEEE 15th Intl Conf on Dependable, Autonomic and Secure Computing, 15th Intl Conf on Pervasive Intelligence and Computing, 3rd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)","20180402","2017","","","1220","1225","Machine learning libraries are integral to a big data mining platform. There are three limitations on adopting current machine learning libraries in such a platform. First, these algorithms are not implemented for handling both online and offline big data analysis. Second, libraries exist in a variety of frameworks using different programming languages, which make it difficult in integrating several algorithms. Third, most machine learning libraries provides APIs for programming only, thus not user-friendly for those do not have a sufficient understanding of algorithms and those lack of programming skills. In this paper, we implement a comprehensive machine learning library including common algorithms and deep learning algorithms. We integrate this library at a platform level that allows both online and offline data analysis using this library. We further design a user-friendly portal that enables quick and agile data analysis practices. All of these form an Online-Offline Combined Big Data Mining Platform (OOBDP). We present a demonstration of big oil data analysis using this platform. We observe the that OOBDP can easily accommodate industrial requirement for adaptable data mining process, with personalized usage scenarios, and easy to use experiences.","","Electronic:978-1-5386-1956-8; POD:978-1-5386-1957-5; USB:978-1-5386-1955-1","10.1109/DASC-PICom-DataCom-CyberSciTec.2017.195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8328539","Big Data Mining;Machine Learning;Online-Offline processing","Big Data;Clustering algorithms;Data mining;Libraries;Machine learning;Machine learning algorithms;Task analysis","Big Data;application program interfaces;data analysis;data mining;learning (artificial intelligence);portals;software libraries","API;OOBDP;adaptable data mining process;agile data analysis;big oil data analysis;deep learning algorithms;machine learning libraries;offline big data analysis;online data analysis;online-offline combined Big Data mining platform;platform level;programming languages;programming skills;user-friendly portal","","","","","","","","6-10 Nov. 2017","","IEEE","IEEE Conferences"
"Use of machine learning in big data analytics for insider threat detection","M. Mayhew; M. Atighetchi; A. Adler; R. Greenstadt","United States Air Force Research Laboratory, Rome, NY, USA","MILCOM 2015 - 2015 IEEE Military Communications Conference","20151217","2015","","","915","922","In current enterprise environments, information is becoming more readily accessible across a wide range of interconnected systems. However, trustworthiness of documents and actors is not explicitly measured, leaving actors unaware of how latest security events may have impacted the trustworthiness of the information being used and the actors involved. This leads to situations where information producers give documents to consumers they should not trust and consumers use information from non-reputable documents or producers. The concepts and technologies developed as part of the Behavior-Based Access Control (BBAC) effort strive to overcome these limitations by means of performing accurate calculations of trustworthiness of actors, e.g., behavior and usage patterns, as well as documents, e.g., provenance and workflow data dependencies. BBAC analyses a wide range of observables for mal-behavior, including network connections, HTTP requests, English text exchanges through emails or chat messages, and edit sequences to documents. The current prototype service strategically combines big data batch processing to train classifiers and real-time stream processing to classifier observed behaviors at multiple layers. To scale up to enterprise regimes, BBAC combines clustering analysis with statistical classification in a way that maintains an adjustable number of classifiers.","","Electronic:978-1-5090-0073-9","10.1109/MILCOM.2015.7357562","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7357562","HTTP;TCP;big data;chat;documents;email;insider threat;machine learning;support vector machine;trust;usage patterns","Access control;Big data;Computer security;Electronic mail;Feature extraction;Monitoring","Big Data;authorisation;data analysis;document handling;learning (artificial intelligence);pattern classification;pattern clustering;trusted computing","BBAC;English text exchanges;HTTP requests;actor trustworthiness;behavior-based access control;big data analytics;big data batch processing;chat messages;classifier training;clustering analysis;document trustworthiness;emails;enterprise environments;information trustworthiness;insider threat detection;interconnected systems;machine learning;mal-behavior;network connections;real-time stream processing;security events;statistical classification","","3","","37","","","","26-28 Oct. 2015","","IEEE","IEEE Conferences"
"A syllabus on data mining and machine learning with applications to cybersecurity","A. Epishkina; S. Zapechnikov","National Research Nuclear University MEPhI (Moscow Engineering Physics Institute), Russia","2016 Third International Conference on Digital Information Processing, Data Mining, and Wireless Communications (DIPDMWC)","20160804","2016","","","194","199","Big data analytics are very fruitful for solving problems in cybersecurity. We have analyzed modern trends in intelligent security systems research and practice and worked out a syllabus for a new university course in the area of data mining and machine learning with applications to cybersecurity. The course is for undergraduate and graduate students studying the cybersecurity. The main objective of the course is to provide students with fundamental concepts in data mining (in particular, mining frequent patterns, associations and correlations, classification, cluster analysis, outlier detection), machine learning (including neural networks, support vector machines etc.) and related issues, e.g. the basics of multidimensional statistics. Contrary to the traditional data mining and machine learning courses we illustrate course topics by cases from the area of cybersecurity including botnet detection, intrusion detection, deep packet inspection, fraud monitoring, malware detection, phishing detection, active authentication. We note that our course has great potential for development.","","CD-ROM:978-1-4673-9378-2; Electronic:978-1-4673-9379-9; POD:978-1-4673-9380-5","10.1109/DIPDMWC.2016.7529388","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7529388","associative rules;classification;clustering;data mining;educational and methodical maintenance;information security;information technology;outlier detection;specialists training","Big data;Computer security;Correlation;Data analysis;Data mining;Information security","Big Data;computer science education;data mining;educational courses;further education;learning (artificial intelligence);security of data","active authentication;big data analytics;botnet detection;course syllabus;cybersecurity;data mining;deep packet inspection;fraud monitoring;graduate students;intelligent security systems;intrusion detection;machine learning;malware detection;multidimensional statistics;phishing detection;undergraduate students;university course","","","","","","","","6-8 July 2016","","IEEE","IEEE Conferences"
"Towards a Fully Automated Diagnostic System for Orthodontic Treatment in Dentistry","S. Murata; C. Lee; C. Tanikawa; S. Date","","2017 IEEE 13th International Conference on e-Science (e-Science)","20171116","2017","","","1","8","A deep learning technique has emerged as a successful approach for diagnostic imaging. Along with the increasing demands for dental healthcare, the automation of diagnostic imaging is increasingly desired in the field of orthodontics for many reasons (e.g., remote assessment, cost reduction, etc.). However, orthodontic diagnoses generally require dental and medical scientists to diagnose a patient from a comprehensive perspective, by looking at the mouth and face from different angles and assessing various features. This assessment process takes a great deal of time even for a single patient, and tends to generate variation in the diagnosis among dental and medical scientists. In this paper, the authors propose a deep learning model to automate diagnostic imaging, which provides an objective morphological assessment of facial features for orthodontic treatment. The automated diagnostic imaging system dramatically reduces the time needed for the assessment process. It also helps provide objective diagnosis that is important for dental and medical scientists as well as their patients because the diagnosis directly affects to the treatment plan, treatment priorities, and even insurance coverage. The proposed deep learning model outperforms a conventional convolutional neural network model in its assessment accuracy. Additionally, the authors present a work-in-progress development of a data science platform with a secure data staging mechanism, which supports computation for training our proposed deep learning model. The platform is expected to allow users (e.g., dental and medical scientists) to securely share data and flexibly conduct their data analytics by running advanced machine learning algorithms (e.g., deep learning) on high performance computing resources (e.g., a GPU cluster).","","Electronic:978-1-5386-2686-3; POD:978-1-5386-2687-0","10.1109/eScience.2017.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8109117","","Computational modeling;Dentistry;Feature extraction;Machine learning;Medical diagnostic imaging","dentistry;face recognition;health care;learning (artificial intelligence);medical image processing;patient treatment","automated diagnostic imaging system;deep learning;dental healthcare;dentistry;facial features;fully automated diagnostic system;insurance coverage;machine learning;objective morphological assessment;orthodontic diagnoses;orthodontic treatment;orthodontics;treatment plan;treatment priorities","","","","","","","","24-27 Oct. 2017","","IEEE","IEEE Conferences"
"SARGS method for distributed actionable pattern mining using spark","A. Bagavathi; P. Mummoju; K. Tarnowska; A. A. Tzacheva; Z. W. Ras","Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC, 28223, USA","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","4272","4281","Actionability is a mode of revealing actionable knowledge in the form of Action Rules from large datasets. Action rule imparts in the form of recommendations as how a data object can change from one value to another more desirable value. The towering production of data in the recent years, due to increased usage of web, social media and IoT, has led to the age of big data. Also, abundant usage of cloud storages and cloud based services causes the data to be spread around the globe. This requires more time and space for a single computer to cope with such widespread data. Ecosystems like Hadoop MapReduce, Spark have been introduced to store, process and retrieve back the data efficiently in a distributed fashion. Data mining finds substantial improvements over such distributed frameworks to process huge volume of data and acquire knowledge from them in a short span of time. In this paper, we present an approach SARGS: Specific Action Rule discovery based on Grabbing Strategy, to build more specific Action Rules using Apache Spark framework and evaluate the results with our previous Hadoop MapReduce system (MR-Random Forest Algorithm for Distributed Action Rules Discovery). Also, we propose a novel approach to distribute data in a distributed environment to get more optimal Action Rules and upgraded ARoGS algorithm to get more specific Action Rules.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258454","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258454","Action Rules;actionability;actionable patterns;data mining;spark","Clustering algorithms;Data mining;Distributed databases;Fault tolerance;Fault tolerant systems;Machine learning algorithms;Sparks","Big Data;cloud computing;cluster computing;data handling;data mining;parallel processing","Action rule imparts;Apache Spark framework;Distributed Action Rules Discovery;Hadoop MapReduce system;SARGS;SARGS method;Specific Action Rule discovery;actionable knowledge;big data;cloud based services;cloud storages;data mining;data object;distributed actionable pattern mining;distributed environment;distributed fashion;distributed frameworks;optimal Action Rules;specific Action Rules","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Performance Evaluation of Mahout Clustering Algorithms Using a Twitter Streaming Dataset","F. Xhafa; A. Bogza; S. Caballé","Univ. Politec. de Catalunya, Barcelona, Spain","2017 IEEE 31st International Conference on Advanced Information Networking and Applications (AINA)","20170508","2017","","","1019","1026","Big Data has become commonplace in most Internet-based applications, which by delivering services to planetary scale numbers of users generate very large data sets. Such data sets are considered as a valuable source of analytics information and knowledge for many purposes and domains. It is claimed each time more that Big Data and machine learning, especially data mining, are the basis for developing advanced analytics platforms for turning data into valuable assets, gaining competitive advantage and make better decisions. At the same time, however, Big Data applications are showing to be killer applications for the state of the art machine learning and data mining algorithms. Indeed, traditional data mining frameworks such as WEKA, R, etc. and those from big companies such as IBM SPSS Modeler, SAS Enterprise Miner, Oracle Data Mining, etc. are facing the challenges of 1) coping with mining large data sets within short times and 2) under high rates of data generation. The way envisaged ahead to effectively deal with such challenges is to move to Cloudbased versions of such frameworks and development of new frameworks implemented using Cloud platforms. In either case, data mining and machine learning algorithms are being fully implemented in Cloud platforms under new requirements of Big Data for efficiency and performance. In the group of newly developed frameworks there is Apache Mahout, whose goal is “to build an environmentfor quickly creating scalable performant machine learning applications"". In this paper we analyse the performance of some clustering algorithms of Apache Mahout using a Twitter streaming dataset under a Hadoop MapReduce cluster infrastructure according to various evaluation criteria.","1550-445X;1550445X","Electronic:978-1-5090-6029-0; POD:978-1-5090-6030-6","10.1109/AINA.2017.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7921018","Apache Mahout;Big Data;Data Mining;Hadoop Cluster;Machine Learning;Performance","Algorithm design and analysis;Big data;Clustering algorithms;Heuristic algorithms;Text mining;Twitter","Big Data;cloud computing;data mining;decision making;learning (artificial intelligence);parallel processing;pattern clustering;performance evaluation;social networking (online)","Apache Mahout frameworks;Big Data applications;Hadoop MapReduce cluster infrastructure;Internet-based applications;Mahout clustering algorithms;Twitter streaming dataset;advanced analytics platforms;analytics information;cloud-based versions;data generation;data mining;decision making;machine learning;performance evaluation;very large data sets","","","","","","","","27-29 March 2017","","IEEE","IEEE Conferences"
"Survey of machine learning methods for big data applications","A. Vinothini; S. B. Priya","Department of Information Technology, Rajalakshmi Engineering College, Chennai, India","2017 International Conference on Computational Intelligence in Data Science(ICCIDS)","20180201","2017","","","1","5","The most unwaveringly heard fanciful word in today's digital world is “Big Data”. It is arriving from multiple sources at an alarming velocity, volume and variety. Innovative analytics and skills are vital to excerpt knowledge from big data. This knowledge will drive the further innovation with new trade and services. Big data features, applications, and challenges related to each application are conversed. Antique machine learning techniques that can be applied proficiently on big data platforms are discoursed. Algorithms in classification, clustering and association that can be scalable to mine knowledge from big data are presented. This survey will be helpful for the practitioners and researchers to select an appropriate method for a given application.","","Electronic:978-1-5090-5595-1; POD:978-1-5090-5596-8","10.1109/ICCIDS.2017.8272638","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8272638","Big data;association;big data applications;classification;clustering","Algorithm design and analysis;Big Data;Classification algorithms;Industries;Medical services;Sensors;Support vector machines","Big Data;data mining;learning (artificial intelligence);pattern classification;pattern clustering","big data applications;big data platforms;classification;clustering;machine learning methods","","","","","","","","2-3 June 2017","","IEEE","IEEE Conferences"
"Scalable Data Stream Clustering with k Estimation","P. L. Cândido; M. C. Naldi; J. A. Silva; E. R. Faria","Dept. de Inf., Univ. Fed. de Vicosa, Vicosa, Brazil","2017 Brazilian Conference on Intelligent Systems (BRACIS)","20180108","2017","","","336","341","The constant increasing of the generated data in real time has been creating new challenges for machine learning tasks and for one of their main branches: data clustering. The scenario of big data stream (high-speed data stream) has become reality. In order to deal with this scenario, new approaches are required. In this work, we present new techniques based on three clustering fields: Data Stream, MapReduce and automatic estimation of k from data. The goal is to cluster a high-speed data stream with varying number of clusters. Two scalable algorithms are proposed, based on centralized data stream algorithms. The first is based on the StreamKM++ and the second based on the F-EAC, an evolutionary algorithm used for batch clustering purpose. Results achieved the same high quality as the original centralized versions of the algorithms. The proposed techniques can be used instead the centralized ones when the velocity/volume is so high to fit in a centralized system.","","Electronic:978-1-5386-2407-4; POD:978-1-5386-2408-1","10.1109/BRACIS.2017.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8247076","MapReduce;data stream clustering;estimation of k","Adaptation models;Algorithm design and analysis;Big Data;Clustering algorithms;Data models;Electronic mail;Estimation","Big Data;data handling;evolutionary computation;learning (artificial intelligence);pattern clustering;real-time systems","F-EAC;MapReduce;StreamKM++;batch clustering purpose;big data stream;centralized data stream algorithms;evolutionary algorithm;high-speed data stream;machine learning tasks;real time;scalable data stream clustering","","","","","","","","2-5 Oct. 2017","","IEEE","IEEE Conferences"
"Infra: SLO Aware Elastic Auto-scaling in the Cloud for Cost Reduction","S. Sidhanta; S. Mukhopadhyay","Louisiana State Univ., Baton Rouge, LA, USA","2016 IEEE International Congress on Big Data (BigData Congress)","20161006","2016","","","141","148","Enterprises often host applications and services on clusters of virtual machine instances provided by cloud service providers, like Amazon, Rackspace, Microsoft, etc. Users pay a cloud usage cost on the basis of the hourly usage [1] of virtual machine instances composing the cluster. A cluster composition refers to the number of virtual machine instances of each type (from a predefined list of types) comprising a cluster. We present Infra, a cloud provisioning framework that can predict an (ϵ, δ)-minimum cluster composition required to run a given application workload on a cloud under an SLO (i.e., Service Level Objective) deadline. This paper does not present a new approximation algorithm, instead we provide a tool that applies existing machine learning techniques to predict an (ϵ, δ)-minimum cluster composition. An (ϵ, δ)-minimum cluster composition specifies a cluster composition whose cost approximates that of the minimum cluster composition (i.e., the cluster composition that incurs the minimum cloud usage cost that must be incurred in executing a given application under an SLO deadline); the approximation bounds the error to a predefined threshold ϵ with a degree of confidence 100 * (1 - δ)%. The degree of confidence 100 * (1 - δ)% specifies that the probability of failure in achieving the error threshold ϵ for the above approximation is at most δ. For ϵ = 0.1 and δ = 0.02, we experimentally demonstrate that an (ϵ, δ)-minimum cluster composition predicted by Infra successfully approximates the minimum cluster composition, i.e., the accuracy of prediction of minimum cluster composition ranges from 93.1% to 97.99% (the error is bound by the error threshold of 0.1) with - 98% degree of confidence, since 100* (1 - δ) = 98%. Auto scaling refers to the process of automatically adding cloud instances to a cluster to adapt to an increase in application workload (increased request rate), and deleting instances from a cluster when there is a decrease in workload (reduced request rate). However, state-of-the-art auto scaling techniques have the following disadvantages: A) they require explicit policy definition for changing the cluster configuration and therefore lack the ability to automatically adapt a cluster with respect to changing workload, B) they do not compute the appropriate size of resources required, and therefore do not result in an “optimal” cluster composition. Infra provides an auto scaler that automatically adapts a cloud infrastructure to changing application workload, scaling the cluster up/down based on predictions from the Infra provisioning tool.","","","10.1109/BigDataCongress.2016.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7584931","cost optimal cluster composition;elastic auto scaling;provisioning","Algorithm design and analysis;Cloud computing;Clustering algorithms;Machine learning algorithms;Mathematical model;Training;Virtual machining","cloud computing;cost reduction;learning (artificial intelligence);pattern clustering;virtual machines","(ϵ, δ)-minimum cluster composition prediction;Infra provisioning tool;SLO aware elastic autoscaling;cloud infrastructure;cloud provisioning framework;cloud service providers;cloud usage cost;cost reduction;error threshold;machine learning techniques;probability of failure;service level objective;virtual machine instances","","","","","","","","June 27 2016-July 2 2016","","IEEE","IEEE Conferences"
"Reflex-Tree: A Biologically Inspired Parallel Architecture for Future Smart Cities","J. Kane; B. Tang; Z. Chen; J. Yan; T. Wei; H. He; Q. Yang","Dept. of Electr., Comput., & Biomed. Eng., Univ. of Rhode Island, Kingston, RI, USA","2015 44th International Conference on Parallel Processing","20151210","2015","","","360","369","We introduce a new parallel computing and communication architecture, Reflex-Tree, with massive sensing, data processing, and control functions suitable for future smart cities. The central feature of the proposed Reflex-Tree architecture is inspired by a fundamental element of the human nervous system: reflex arcs, the neuromuscular reactions and instinctive motions of a part of the body in response to urgent situations. At the bottom level of the Reflex-Tree (layer 4), novel sensing devices are proposed that are controlled by low power processing elements. These ""leaf"" nodes are then connected to new classification engines based on machine learning techniques, including support vector machines (SVM), to form the third layer. The next layer up consists of servers that provide accurate control decisions via multi-layer adaptive learning and spatial-temporal association, before they are connected to the top level cloud where complex system behavior analysis is performed. Our multi-layered architecture mimics human neural circuits to achieve the high levels of parallelization and scalability required for efficient city-wide monitoring and feedback. To demonstrate the utility of our architecture, we present the design, implementation, and experimental evaluation of a prototype Reflex-Tree. City power supply network and gas pipeline management scenarios are used to drive our prototype as case studies. We show the effectiveness for several levels of the architecture and discuss the feasibility of implementation.","0190-3918;01903918","Electronic:978-1-4673-7587-0; POD:978-1-4673-7588-7","10.1109/ICPP.2015.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7349591","Smart cities;fiber optic sensing networks;parallel architectures;pattern recognition","Cities and towns;Computer architecture;Parallel processing;Real-time systems;Sensors;Smart cities;Wireless sensor networks","learning (artificial intelligence);pattern classification;smart cities;support vector machines;tree data structures","SVM;biologically-inspired parallel architecture;city power supply network scenario;classification engines;communication architecture;complex system behavior analysis;control decisions;gas pipeline management scenario;human nervous system;instinctive motions;leaf nodes;low-power processing elements;machine learning techniques;multilayer adaptive learning;multilayered architecture;neuromuscular reactions;parallel computing;reflex arcs;reflex-tree architecture;sensing devices;smart cities;spatial-temporal association;support vector machines;top level cloud","","3","","60","","","","1-4 Sept. 2015","","IEEE","IEEE Conferences"
"Big data parallelism: Challenges in different computational paradigms","K. Mondal; P. Dutta","Computer Centre Indian Institute of Technology Indore, India","Proceedings of the 2015 Third International Conference on Computer, Communication, Control and Information Technology (C3IT)","20150316","2015","","","1","5","Developers are engaged themselves in processing big data for different computational environments especially in different information systems, biological expression preparations and visual and graphical modelling. Digital Elevation Models (DEMs) in Geographic Information Systems (GIS) is one such information systems where in memory computation faces a lot challenges to manipulate and visualize the data. Scalable distributed framework broadly exhibit two design characteristics: (i) they are using memory scalability in such a manner that the amount of memory required by each process decreases as the number of processes used to solve a given problem instance increases, and (ii) they exploit coarse grain parallelism in the sense that they structure their computations into a sequence of local computation followed by communication phases in which the local computations take a non-trivial amount of time and often involve a non-trivial subset of the process' memory. In this paper we will discuss about big data, data science, different models available in the parallel paradigms, the pros and cons and the probable way out to work with high dimensional data.","","DVD:978-1-4799-4447-7; Electronic:978-1-4799-4445-3; POD:978-1-4799-4444-6","10.1109/C3IT.2015.7060186","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7060186","Big Data;Data Science;High Dimensional Data;Machine Learning;Parallel Computing;Scalable framework;Semi-stochastic","Big data;Computational modeling;Data models;Mathematical model;Memory management;Parallel processing;Stochastic processes","Big Data;data visualisation;learning (artificial intelligence);parallel processing","Big Data parallelism;computational paradigms;data visualization;machine learning;scalable distributed framework","","0","","20","","","","7-8 Feb. 2015","","IEEE","IEEE Conferences"
"Elective Recommendation Support through K-Means Clustering Using R-Tool","Agnivesh; R. Pandey","Amity Inst. of Inf. Technol., Amity Univ., Lucknow, India","2015 International Conference on Computational Intelligence and Communication Networks (CICN)","20160818","2015","","","851","856","The data generated from both men and machines are exponentially multiplying the size and the structural definition of the data. Such a voluminous, dynamic and unstructured data termed as Big Data is analyzed and maintained and can be used for various purposes and applications. Big Data is generated from sources like social media, cyber physical system and business entities. This enormous data generation leads to problems of data storage and analysis. The Big Data with its diverse features calls for various tools, technologies and algorithms to make an inference which shall render strategically advantage to any entity. A typical data analytical scenario is a multidimensional problem and data clustering can lead to multi spatial analysis. Cluster can be a result of various algorithms. In this paper k means clustering is applied to generate clusters using R statistical tool and recommend elective on the basis of student's performance.","","Electronic:978-1-5090-0076-0; POD:978-1-5090-0077-7","10.1109/CICN.2015.324","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7546216","Big Data;Clustering;Elective Recommendation;K-Means;R tool","Algorithm design and analysis;Big data;Classification algorithms;Clustering algorithms;Inference algorithms;Machine learning algorithms;Software algorithms","Big Data;commerce;cyber-physical systems;data analysis;pattern clustering;recommender systems;social networking (online);statistical analysis;storage management","Big Data;K-means clustering;R statistical tool;R-tool;business entities;cyber physical system;data analysis;data generation;data storage;elective recommendation;social media;structural definition","","1","","","","","","12-14 Dec. 2015","","IEEE","IEEE Conferences"
"Dela — Sharing Large Datasets between Hadoop Clusters","A. A. Ormenişan; J. Downling","Dept. of Software & Comput. Syst., KTH R. Inst. of Technol., Stockholm, Sweden","2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS)","20170717","2017","","","2533","2536","Big data has, in recent years, revolutionised an ever-growing number of fields, from machine learning to climate science to genomics. The current state-of-the-art for storing large datasets is either object stores or distributed filesystems, with Hadoop being the dominant open-source platform for managing `Big Data'. Existing large-scale storage platforms, however, lack support for the efficient sharing of large datasets over the Internet. Those systems that are widely used for the dissemination of large files, like BitTorrent, need to be adapted to handle challenges such as network links with both high latency and high bandwidth, and scalable storage backends that are optimised for streaming and not random access. In this paper, we introduce Dela, a peer-to-peer data-sharing service integrated into the Hops Hadoop platform that provides an end-to-end solution for dataset sharing. Dela is designed for large-scale storage backends and data transfers that are both non-intrusive to existing TCP network traffic and provide higher network throughput than TCP on high latency, high bandwidth network links, such as transatlantic network links. Dela provides a pluggable storage layer, implementing two alternative ways for clients to access shared data: stream processing of data as it arrives with Kafka, and traditional offline access to data using the Hadoop Distributed Filesystem. Dela is the first step for the Hadoop platform towards creating an open dataset ecosystem that supports user-friendly publishing, searching, and downloading of large datasets.","1063-6927;10636927","Electronic:978-1-5386-1792-2; POD:978-1-5386-1793-9","10.1109/ICDCS.2017.199","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7980225","Big Data;BitTorrent;Hadoop;dataset sharing;peer-to-peer","Bandwidth;Bioinformatics;Distributed databases;Genomics;Peer-to-peer computing;Protocols;Throughput","Big Data;Internet;parallel processing;peer-to-peer computing;public domain software;storage management","Big Data;BitTorrent;Dela;Hadoop clusters;Hadoop distributed filesystem;Internet;TCP network traffic;data transfers;end-to-end solution;large datasets sharing;large datasets storing;large-scale storage backends;open-source platform;peer-to-peer data sharing;pluggable storage layer","","","","","","","","5-8 June 2017","","IEEE","IEEE Conferences"
"PALLADIO: A Parallel Framework for Robust Variable Selection in High-Dimensional Data","M. Barbieri; S. Fiorini; F. Tomasi; A. Barla","Dept. of Inf., Bioeng., Robot. & Syst. Eng., Univ. degli Studi di Genova, Genoa, Italy","2016 6th Workshop on Python for High-Performance and Scientific Computing (PyHPC)","20170202","2016","","","19","26","The main goal of supervised data analytics is to model a target phenomenon given a limited amount of samples, each represented by an arbitrarily large number of variables. Especially when the number of variables is much larger than the number of available samples, variable selection is a key step as it allows to identify a possibly reduced subset of relevant variables describing the observed phenomenon. Obtaining interpretable and reliable results, in this highly indeterminate scenario, is often a non-trivial task. In this work we present PALLADIO, a framework designed for HPC cluster architectures, that is able to provide robust variable selection in high-dimensional problems. PALLADIO is developed in Python and it integrates CUDA kernels to decrease the computational time needed for several independent element-wise operations. The scalability of the proposed framework is assessed on synthetic data of different sizes, which represent realistic scenarios.","","Electronic:978-1-5090-5220-2; POD:978-1-5090-5221-9","10.1109/PyHPC.2016.007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836840","machine learning;parallel algorithms;predictive models;variable selection","Data analysis;Graphics processing units;Input variables;Mathematical model;Measurement;Predictive models;Robustness","data analysis;learning (artificial intelligence);parallel algorithms;parallel architectures","CUDA kernels;HPC cluster architectures;PALLADIO framework;Python;computational time reduction;element-wise operations;high-dimensional data;parallel algorithms;reduced subset identification;robust variable selection;supervised data analytics","","","","","","","","14-14 Nov. 2016","","IEEE","IEEE Conferences"
"Self-organizing maps for anomaly detection in fuel consumption. Case study: Illegal fuel storage in Bolivia","V. G. Aquize; E. Emery; F. B. de Lima Neto","University of Pernambuco, Recife 50720-001 Pernambuco, Brazil","2017 IEEE Latin American Conference on Computational Intelligence (LA-CCI)","20180208","2017","","","1","6","Currently, Bolivia is a country that suffers problems due to fuel smuggling caused by the subsidy. To address this problem, the government records the fuel supply of each vehicle through a Radio Frequency Identification (RFID) technology as a control action. However, the massive volumes of stored records does not have any intelligent engine to support tasks of detecting anomalies during the monitoring the consumption of each vehicle that could be possible incidents of illegal fuel storage. Thus, the present work proposes an algorithm to identify anomalies behaviors that may be considered fraud cases. We use the unsupervised machine learning technique, Self-Organizational Maps (SOM), to extract patterns of consumption of vehicles and identify anomalies scores based on its own and group history behavior. According to our results, the proposal detects anomalies with 80% certainty.","","Electronic:978-1-5386-3734-0; POD:978-1-5386-3735-7; USB:978-1-5386-3733-3","10.1109/LA-CCI.2017.8285697","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8285697","Anomaly Detection;Clustering;Fuel Smuggling;Self Organizing Maps","Algorithm design and analysis;Anomaly detection;Clustering algorithms;Fuels;History;Neurons;Self-organizing feature maps","fraud;fuel storage;government data processing;radiofrequency identification;security of data;self-organising feature maps;unsupervised learning","Bolivia;Radio Frequency Identification technology;Self-Organizational Maps;anomalies scores;anomaly detection;case study;fraud cases;fuel consumption;illegal fuel storage;stored records","","","","","","","","8-10 Nov. 2017","","IEEE","IEEE Conferences"
"Iterative sparse matrix-vector multiplication on in-memory cluster computing accelerated by GPUs for big data","J. Peng; Z. Xiao; C. Chen; W. Yang","College of Information Science and Engineering, Hunan University, Changsha, Hunan 410082, China","2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)","20161024","2016","","","1454","1460","Iterative SpMV (ISpMV) is a key operation in many graph-based data mining algorithms and machine learning algorithms. Along with the development of big data, the matrices can be so large, perhaps billion-scale, that the SpMV can not be implemented in a single computer. Therefore, it is a challenging issue to implement and optimize SpMV for large-scale data sets. In this paper, we used an in-memory heterogeneous CPU-GPU cluster computing platforms (IMHCPs) to efficiently solve billion-scale SpMV problem. A dedicated and efficient hierarchy partitioning strategy for sparse matrices and the vector is proposed. The partitioning strategy contains partitioning sparse matrices among workers in the cluster and among GPUs in one worker. More, the performance of the IMHCPs-based SpMV is evaluated from the aspects of computation efficiency and scalability.","","","10.1109/FSKD.2016.7603391","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7603391","BigData;Flink;GPU;In-memory Computing;Iterative SpMV","Acceleration;Clustering algorithms;Computers;Data mining;Graphics processing units;Machine learning algorithms;Sparse matrices","Big Data;data analysis;graphics processing units;iterative methods;matrix multiplication;microprocessor chips;pattern clustering;sparse matrices;vectors","IMHCP;ISpMV;big data;computation efficiency;computation scalability;graph-based data mining algorithms;hierarchy partitioning strategy;in-memory heterogeneous CPU-GPU cluster computing platforms;iterative sparse matrix-vector multiplication;large-scale data sets;machine learning algorithms","","","","","","","","13-15 Aug. 2016","","IEEE","IEEE Conferences"
"Homogenizing social networking with smart education by means of machine learning and Hadoop: A case study","A. Jagtap; B. Bodkhe; B. Gaikwad; S. Kalyana","Department of Computer Engineering, MES College of Engineering, Pune, India","2016 International Conference on Internet of Things and Applications (IOTA)","20160908","2016","","","85","90","In today's age of ever increasing use of internet, there are around 74% active internet users out of which 60% users contribute to social networking and most of them are students from the age group 16-30 [1]. If this young generation is targeted specifically towards educational activities keeping the same social networking environment in the background would create interest in students for educational activities and also yield productive results. Using Big Data analytics, machine learning and recommender system on the student data and activity would provide them with useful information and suggestions which would help them gain knowledge and make proper decisions to make their future in right direction. This can be implemented by creating a social-cum-educational portal with recommender systems, also data can be generated and displayed on the same place after analysis through recommenders. There is large amount of social, educational information generated on a rapid basis on the web which can be analysed and used for the betterment of the students and also the analysed information can be provided to the students based on their interests. Specific information to specific student can be provided. Use of such technology can reduce the gap between students and the information which can lead to their inherent development and success! However, most of the existing Social Recommender systems do not have good scalabilities which are unable to process huge volumes of data. Aiming to this problem we can design a social recommender system based on Hadoop and its parallel computing platform.","","","10.1109/IOTA.2016.7562700","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7562700","Education recommender system;Education visualization;Hadoop;Machine learning;Smart student visualization;Social network;Social recommender system","Big data;Data analysis;Education;Portals;Recommender systems;Relational databases;Social network services","Big Data;computer aided instruction;data analysis;learning (artificial intelligence);parallel processing;portals;recommender systems;social networking (online)","Big Data analytics;Hadoop;Internet;educational activities;machine learning;parallel computing;smart education;social networking;social recommender systems;social-cum-educational portal;student activity;student data","","","","","","","","22-24 Jan. 2016","","IEEE","IEEE Conferences"
"A scalable implementation of information theoretic feature selection for high dimensional data","A. Kleerekoper; M. Pappas; A. Pocock; G. Brown; M. Lujan","School of Computing, Mathematics and Digital Technologies, Manchester Metropolitan University, UK","2015 IEEE International Conference on Big Data (Big Data)","20151228","2015","","","339","346","With the growth of high dimensional data, feature selection is a vital component of machine learning as well as an important stand alone data analytics tool. Without it, the computation cost of big data analytics can become unmanageable and spurious correlations and noise can reduce the accuracy of any results. Feature selection removes irrelevant and redundant information leading to faster, more reliable data analysis. Feature selection techniques based on information theory are among the fastest known and the Manchester AnalyticS Toolkit (MAST) provides an efficient, parallel and scalable implementation of these methods. This paper considers a number of data structures for storing the frequency counters that underpin MAST. We show that preprocessing the data to reduce the number of zero-valued counters in an array structure results in an order of magnitude reduction in both memory usage and execution time compared to state of the art structures that use explicit mappings to avoid zero-valued counters. We also describe a number of parallel processing techniques that enable MAST to scale linearly with the number of processors even on NUMA architectures. MAST targets scale-up servers rather than scale-out clusters and we show that it performs orders of magnitude faster than existing tools. Moreover, we show that MAST is 3.5 times faster than a scale-out solution built for Spark running on the same server. As an example of the performance of MAST, we were able to process a dataset of 100 million examples and 100,000 features in under 10 minutes on a four socket server which each socket containing an 8-core Intel Xeon E5-4620 processor.","","Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9","10.1109/BigData.2015.7363774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363774","Big Data;Data Analytics;Data Structures;Feature Selection;Information Theory;Mutual Information;Parallel Processing","Arrays;Big data;Machine learning algorithms;Mutual information;Radiation detectors;Servers","Big Data;data analysis;data structures;feature selection;information theory;parallel processing","8-core Intel Xeon E5-4620 processor;Big Data analytics;MAST;Manchester analytics toolkit;NUMA architectures;Spark;data analytics tool;data preprocessing;data structures;frequency counters;high dimensional data;information theoretic feature selection;machine learning;magnitude reduction;memory execution time;memory usage;noise reduction;parallel processing techniques;scale-out clusters;socket server;zero-valued counters","","","","22","","","","Oct. 29 2015-Nov. 1 2015","","IEEE","IEEE Conferences"
"Recommending environmental knowledge as linked open data cloud using semantic machine learning","A. Morshed; R. Dutta; J. Aryal","Intell. Sensing & Syst. Lab., CSIRO, Hobart, TAS, Australia","2013 IEEE 29th International Conference on Data Engineering Workshops (ICDEW)","20130627","2013","","","27","28","Large scale environmental knowledge integration and development of a knowledge recommendation system for the Linked Open Data Cloud using semantic machine learning approach was the main mission of this research. This study considered five different environmental big data sources including SILO, AWAP, ASRIS, MODIS and CosmOz complementary for knowledge integration. Unsupervised clustering techniques based on principal component analysis (PCA) and Fuzzy-C-Means (FCM) and Self-organizing map (SOM) clustering was used to learn the extracted features and to create a 2D map based dynamic knowledge recommendation system. Knowledge was stored in a triplestore using triples format (subject, predicate, and object) along with the complete meta-data provenance information. The Resource Description Framework (RDF) representation made i-EKbase very flexible to integrate with the Linked Open Data (LOD) cloud. The developed Intelligent Environmental Knowledgebase (i-EKbase) could be used for any environmental decision support application.","","Electronic:978-1-4673-5304-5; POD:978-1-4673-5303-8","10.1109/ICDEW.2013.6547421","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6547421","FCM;Linked Open Data cloud;PCA;RDF;g-SOM based visual selection;i-EKbase;triplestore","Artificial intelligence;Databases;MODIS;Principal component analysis;Resource description framework;Semantics;Visualization","cloud computing;decision support systems;environmental science computing;fuzzy set theory;learning (artificial intelligence);meta data;open systems;pattern clustering;principal component analysis;recommender systems;self-organising feature maps","2D map-based dynamic environmental knowledge recommendation system;ASRIS;AWAP;CosmOz;FCM;LOD cloud;MODIS;PCA;RDF representation;Resource Description Framework representation;SILO;SOM clustering;environmental big-data sources;environmental decision support application;environmental knowledge development;environmental knowledge integration;feature extraction;fuzzy-c-means;i-EKbase could;intelligent environmental knowledgebase could;linked open data cloud;meta-data provenance information;principal component analysis;self-organizing map clustering;semantic machine learning;subject-predicate-object triples format;triplestore;unsupervised clustering techniques","","5","","6","","","","8-12 April 2013","","IEEE","IEEE Conferences"
"PANDA: Extreme Scale Parallel K-Nearest Neighbor on Distributed Architectures","M. M. A. Patwary; N. R. Satish; N. Sundaram; J. Liu; P. Sadowski; E. Racah; S. Byna; C. Tull; W. Bhimji; Prabhat; P. Dubey","","2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","20160721","2016","","","494","503","Computing k-Nearest Neighbors (KNN) is one of the core kernels used in many machine learning, data mining and scientific computing applications. Although kd-tree based O(log n) algorithms have been proposed for computing KNN, due to its inherent sequentiality, linear algorithms are being used in practice. This limits the applicability of such methods to millions of data points, with limited scalability for Big Data analytics challenges in the scientific domain. In this paper, we present parallel and highly optimized kd*tree based KNN algorithms (both construction and querying) suitable for distributed architectures. Our algorithm includes novel approaches for pruning search space and improving load balancing and partitioning among nodes and threads. Using TB-sized datasets from three science applications: astrophysics, plasma physics, and particle physics, we show that our implementation can construct kd-tree of 189 billion particles in 48 seconds on utilizing ~50,000 cores. We also demonstrate computation of KNN of 19 billion queries in 12 seconds. We demonstrate almost linear speedup both for shared and distributed memory computers. Our algorithms outperforms earlier implementations by more than order of magnitude, thereby radically improving the applicability of our implementation to state-of-the-art Big Data analytics problems.","1530-2075;15302075","Electronic:978-1-5090-2140-6; POD:978-1-5090-2141-3","10.1109/IPDPS.2016.57","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7516046","Big Data Analytics;Classification;KNN;Parallel Algorithms;and Load Balancing;kd-tree","Complexity theory;Computational modeling;Distributed databases;Machine learning algorithms;Partitioning algorithms;Physics;Plasmas","Big Data;computational complexity;data analysis;data mining;learning (artificial intelligence);pattern classification;physics computing;resource allocation","Big Data analytics;KNN algorithms;O(log n) algorithms;PANDA;astrophysics;data mining;distributed architectures;distributed memory computers;extreme scale parallel K-nearest neighbor;kd-tree;linear algorithms;load balancing;machine learning;particle physics;plasma physics;pruning search space;scientific computing applications","","1","","","","","","23-27 May 2016","","IEEE","IEEE Conferences"
"Short-term load forecasting in smart grid: A combined CNN and K-means clustering approach","Xishuang Dong; Lijun Qian; Lei Huang","Center of Excellence in Research and Education for Big Military Data Intelligence, Prairie View A&M University, Texas A&M University System, 77446, USA","2017 IEEE International Conference on Big Data and Smart Computing (BigComp)","20170320","2017","","","119","125","Although many methods are available to forecast short-term electricity load based on small scale data sets, they may not be able to accommodate large data sets as electricity load data becomes bigger and more complex in recent years. In this paper, a novel machine learning model combining convolutional neural network with K-means clustering is proposed for short-term load forecasting with improved scalability. The large data set is clustered into subsets using K-means algorithm, then the obtained subsets are used to train the convolutional neural network. A real-world power industry data set containing more than 1.4 million of load records is used in this study and the experimental results demonstrate the effectiveness of the proposed method.","","Electronic:978-1-5090-3015-6; POD:978-1-5090-3016-3; USB:978-1-5090-3014-9","10.1109/BIGCOMP.2017.7881726","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881726","Big Data Analytics;Convolutional Neural Network;Machine Learning;Short-term Load Forecasting","Data models;Forecasting;Load forecasting;Load modeling;Predictive models;Testing;Training","learning (artificial intelligence);load forecasting;neural nets;pattern clustering;power engineering computing;smart power grids","CNN;K-means algorithm;K-means clustering approach;convolutional neural network;machine learning model;short-term load forecasting;smart grid","","","","","","","","13-16 Feb. 2017","","IEEE","IEEE Conferences"
"Hard Drive Failure Prediction Using Big Data","W. Yang; D. Hu; Y. Liu; S. Wang; T. Jiang","Baidu, Inc., Beijing, China","2015 IEEE 34th Symposium on Reliable Distributed Systems Workshop (SRDSW)","20160107","2015","","","13","18","We design a general framework named Hdoctor for hard drive failure prediction. Hdoctor leverages the power of big data to achieve a significant improvement comparing to all previous researches that used sophisticated machine learning algorithms. Hdoctor exhibits a series of engineering innovations: (1) constructing time dependent features to characterize the Self-Monitoring, Analysis and Reporting Technology (SMART) value transitions during disk failures, (2) combining features to enable the model to learn the correlation among different SMART attributes, (3) regarding circumstance data such as cluster workload, temperature, humidity, location as related features. Meanwhile, Hdoctor collects/labels samples and updates model automatically, and works well for all kinds of disk failure prediction in our intelligent data center. In this work, we use Hdoctor to collect 74,477,717 training records from our clusters involving 220,022 disks. By training a simple and scalable model, our system achieves a detection rate of 97.82%, with a false alarm rate (FAR) of 0.3%, which hugely outperforms all previous algorithms. In addition, Hdoctor is an excellent indicator for how to predict different hardware failures efficiently under various circumstances.","","Electronic:978-1-5090-0092-0; POD:978-1-5090-0093-7","10.1109/SRDSW.2015.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371435","","Algorithm design and analysis;Data models;Drives;Hidden Markov models;Prediction algorithms;Training;Training data","Big Data;disc drives;failure analysis;hard discs","Big Data;FAR;Hdoctor framework;false alarm rate;hard drive failure prediction;self-monitoring analysis and reporting technology SMART","","2","","12","","","","Sept. 28 2015-Oct. 1 2015","","IEEE","IEEE Conferences"
"Calculating feature importance in data streams with concept drift using Online Random Forest","A. P. Cassidy; F. A. Deviney","Commonwealth Computer Research Inc. (CCRi) Charlottesville, USA","2014 IEEE International Conference on Big Data (Big Data)","20150108","2014","","","23","28","Large volume data streams with concept drift have garnered a great deal of attention in the machine learning community. Numerous researchers have proposed online learning algorithms that train iteratively from new observations, and provide continuously relevant predictions. Compared to previous offline, or sliding window approaches, these algorithms have shown better predictive performance, rapid detection of, and adaptation to, concept drift, and increased scalability to high volume or high velocity data. Online Random Forest (ORF) is one such approach to streaming classification problems. We adapted the feature importance metrics of Mean Decrease in Accuracy (MDA) and Mean Decrease in Gini Impurity (MDG), both originally designed for offline Random Forest, to Online Random Forest so that they evolve with time and concept drift. Our work is novel in that previous streaming models have not provided any measures of feature importance. We experimentally tested our Online Random Forest versions of feature importance against their offline counterparts, and concluded that our approach to tracking the underlying drifting concepts in a simulated data stream is valid.","","Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8","10.1109/BigData.2014.7004352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004352","Concept Drift;Data Streams;Feature Importance;Online Random Forest","Accuracy;Adaptation models;Impurities;Measurement;Prediction algorithms;Training;Vegetation","Big Data;learning (artificial intelligence);statistical analysis;very large databases","MDA;MDG;ORF;concept drift;feature importance metrics;high velocity data;high volume data;large volume data streams;machine learning community;mean decrease in Gini impurity;mean decrease in accuracy;offline random forest;online learning algorithms;online random forest;streaming models","","1","","16","","","","27-30 Oct. 2014","","IEEE","IEEE Conferences"
"Auto-Tagging for Massive Online Selection Tests: Machine Learning to the Rescue","S. Krithivasan; S. Gupta; S. Shandilya; K. Arya; K. Lala","Dept. of Comput. Sci. & Eng., Indian Inst. of Technol. Bombay, Mumbai, India","2016 IEEE Eighth International Conference on Technology for Education (T4E)","20170116","2016","","","204","207","Difficulty Level of a question is relative to that of other questions in a test and also to the test takers, hence manually assigning Difficulty Level tags may not be accurate. There is a need to infer them from historical data pertaining to the performance of students in a test. e-Yantra Robotics Competition (eYRC) is an annual competition having around 5000 teams (20,000 students) registering in the latest edition of the competition, eYRC-2015. All four team members take a test simultaneously and each individual gets questions which are different but have a similar Difficulty Level. A Question Bank containing 1800 unique questions from 3 subjects - Aptitude, Electronics, and C-Programming - is used to generate question sets each having 30 questions. It is a challenge to ensure that each set contains questions of similar Difficulty Levels tagged manually as Easy, Medium or Hard. In this paper, we discuss a learning algorithm called Weighted Clustering that can automatically tag questions by analyzing the performance of students. We used this algorithm to analyze the performance data in eYRC-2014 for 614 questions from the Question Bank, we found that Manual Tagging accuracy was 44%. We retagged questions with Suggested Tags resulting from our analysis and used them again in eYRC-2015. When we applied the algorithm to the performance data in eYRC-2015, we found that the accuracy of tagging had significantly improved to 67%.","","Electronic:978-1-5090-6115-0; POD:978-1-5090-6116-7","10.1109/T4E.2016.050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814825","Machine Learning;Online Testing Environment;Robotics Competition;Selection Test;Weighted Clustering;e-Yantra.","Clustering algorithms;Manuals;Measurement;Partitioning algorithms;Robots;Semantics;Tagging","data analysis;educational administrative data processing;learning (artificial intelligence);pattern clustering","Aptitude subject;C-Programming subject;Electronics subject;auto-tagging;difficulty level;e-Yantra Robotics Competition;eYRC-2015;easy level;hard level;machine learning;manual tagging;massive online selection tests;medium level;performance data analysis;question bank;weighted clustering","","","","","","","","2-4 Dec. 2016","","IEEE","IEEE Conferences"
"Machine Learning based criminal short listing using Modus Operandi features","M. Munasinghe; H. Perera; S. Udeshini; R. Weerasinghe","University of Colombo School of Computing, 07, Sri Lanka","2015 Fifteenth International Conference on Advances in ICT for Emerging Regions (ICTer)","20160111","2015","","","69","76","One of the most challenging problems faced by crime analysts is identifying sets of crimes committed by the same individual or group. Amount of criminal records piling up daily has made it cumbersome to manually process connections between crimes. These Crime series' possess certain attributes that are characteristic of the criminal(s) involved in them, which are useful in defining their modus operandi (MO). After a careful study in the grave crime category of House breaking and Theft in Sri Lanka, we have identified certain MO attributes which we have used to collect from past crime scene data from police records. Then we have explored whether it is possible to group suspects who have similar MO patterns through a machine learning approach and give a short list for a new crime from the existing data. The evaluation of the research presented an accuracy above 75% which proved that Machine Learning is capable of short listing criminals based on their Modus Operandi features.","","CD-ROM:978-1-4673-9439-0; Electronic:978-1-4673-9441-3; POD:978-1-4673-9442-0","10.1109/ICTER.2015.7377669","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7377669","Criminal Profiling;Feature Extraction;Hierarchical Clustering;Machine Learning;Modus Operandi","Encoding;Feature extraction;Unsupervised learning;Weapons","criminal law;learning (artificial intelligence);police data processing","Sri Lanka;crime analysis;criminal record amount;grave crime category;machine learning;modus operandi feature","","","","16","","","","24-26 Aug. 2015","","IEEE","IEEE Conferences"
"Big Data, Big Challenges","W. Wang","Dept. of Comput. Sci., UCLA, Los Angeles, CA, USA","2014 IEEE International Conference on Semantic Computing","20140825","2014","","","6","6","Summary form only given. Big data analytics is the process of examining large amounts of data of a variety of types (big data) to uncover hidden patterns, unknown correlations and other useful information. Its revolutionary potential is now universally recognized. Data complexity, heterogeneity, scale, and timeliness make data analysis a clear bottleneck in many biomedical applications, due to the complexity of the patterns and lack of scalability of the underlying algorithms. Advanced machine learning and data mining algorithms are being developed to address one or more challenges listed above. It is typical that the complexity of potential patterns may grow exponentially with respect to the data complexity, and so is the size of the pattern space. To avoid an exhaustive search through the pattern space, machine learning and data mining algorithms usually employ a greedy approach to search for a local optimum in the solution space, or use a branch-and-bound approach to seek optimal solutions, and consequently, are often implemented as iterative or recursive procedures. To improve efficiency, these algorithms often exploit the dependencies between potential patterns to maximize in-memory computation and/or leverage special hardware (such as GPU and FPGA) for acceleration. These lead to strong data dependency, operation dependency, and hardware dependency, and sometimes ad hoc solutions that cannot be generalized to a broader scope. In this talk, I will present some open challenges faced by data scientist in biomedical fields and the current approaches taken to tackle these challenges.","","Electronic:978-1-4799-4003-5; POD:978-1-4799-4004-2","10.1109/ICSC.2014.65","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6881994","BD2K;big dataanalytics;data mining;machine learning","Algorithm design and analysis;Big data;Complexity theory;Conferences;Data mining;Hardware;Machine learning algorithms","Big Data;data analysis;data mining;learning (artificial intelligence);medical computing;pattern classification","Big Data analytics;FPGA;GPU;biomedical applications;branch-and-bound approach;data complexity;data dependency;data examination;data heterogeneity;data mining algorithms;data scale;data scientist;data timeliness;field programmable gate array;graphics processing unit;greedy approach;hardware dependency;iterative procedure;machine learning;operation dependency;pattern space;recursive procedure","","0","","","","","","16-18 June 2014","","IEEE","IEEE Conferences"
"Integrated STEM learning within health science, mathematics and computer science","M. Droppa; W. Lu; S. Bemis; L. Ocker; M. Miller","Keene State College","2015 IEEE Integrated STEM Education Conference","20150611","2015","","","242","245","In this integrated STEM learning module we developed a data collection tool and used innovative analysis methods to investigate the relationship between academic achievement and risky wellness behaviors among college students. Exploratory factor analysis (EFA) was performed using data from college students (n = 1,499) at a large north-central university. Advanced machine learning analysis techniques found a strong connection between student wellness behavior and academic achievement and that this relationship can be predicted using wellness behavior data. The real world research project in this study integrated educational activities among Mathematics, Computer Science, and Health Science creating an interdisciplinary learning experience within Science, Technology, Engineering and Mathematics (STEM).","","Electronic:978-1-4799-1829-4; POD:978-1-4799-1830-0","10.1109/ISECon.2015.7119932","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7119932","STEM;academic achievement;machine learning techniques;risky wellness behavior","Big data;Clustering algorithms;Computer science;Drugs;Education","behavioural sciences computing;data analysis;educational administrative data processing;educational institutions;further education;health care;learning (artificial intelligence);mathematics","Mathematics;Science-Technology-Engineering-and-Mathematics;academic achievement;advanced machine learning analysis techniques;computer science;data collection tool;exploratory factor analysis;health science;innovative analysis methods;integrated STEM learning module;integrated educational activities;interdisciplinary learning experience;north-central university;risky wellness behaviors","","0","","17","","","","7-7 March 2015","","IEEE","IEEE Conferences"
"Estimating skill fungibility and forecasting services labor demand","B. Johnston; B. Zweig; M. Peran; C. Wang; R. Rosenfeld","IBM Chief Analytics Office, IBM","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","3583","3585","We present an approach for forecasting labor demand in a services business. We introduce an arrangement of machine learning techniques, each constructed by necessity to overcome issues with data veracity and high dimensionality.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258351","Forecasting;Labor Economics;Machine Learning;Natural Language Processing;Workforce","Business;Clustering algorithms;Conferences;Forecasting;Predictive models;Semantics;Taxonomy","customer services;labour resources;learning (artificial intelligence);multiskilling;organisational aspects","data veracity;forecasting labor demand;machine learning techniques;services business;skill fungibility","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Toward big data in QSAR/QSPR","A. Duprat; J. L. Ploix; F. Dioury; G. Dreyfus","SIGnal processing and MAchine learning (SIGMA) lab, ESPCI ParisTech, France","2014 IEEE International Workshop on Machine Learning for Signal Processing (MLSP)","20141120","2014","","","1","6","We investigate a prospective path to processing “big data” in the field of computer-aided drug design, motivated by the expected increase of the size of available databases. We argue that graph machines, which exempt the designer of a predictive model from handcrafting, selecting and computing ad hoc molecular descriptors, may open a way toward efficient model design procedures. We recall the principle of graph machines, which perform predictions directly from the molecular structure described as a graph, without resorting to descriptors. We discuss scalability issues in the present implementation of graph machines, and we describe an application to the prediction of an important thermodynamic property of contrast agents for MRI imaging.","1551-2541;15512541","Electronic:978-1-4799-3694-6; POD:978-1-4799-3695-3","10.1109/MLSP.2014.6958884","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958884","QSAR/QSPR;chelate;graph machine;scale;stability","Computational modeling;Databases;Drugs;Magnetic resonance imaging;Neural networks;Standards;Training","Big Data;drugs;graph theory;learning (artificial intelligence);pharmaceutical technology","Big Data;MRI imaging;QSAR;QSPR;ad hoc molecular descriptors;computer-aided drug design;contrast agents;graph machines;handcrafting;molecular structure;predictive model;quantitative structure activity relationship;quantitative structure property relationship;scalability issues;thermodynamic property","","1","","11","","","","21-24 Sept. 2014","","IEEE","IEEE Conferences"
"Big Data Analytics in Industrial IoT Using a Concentric Computing Model","M. H. u. Rehman; E. Ahmed; I. Yaqoob; I. A. T. Hashem; M. Imran; S. Ahmad","","IEEE Communications Magazine","20180213","2018","56","2","37","43","The unprecedented proliferation of miniaturized sensors and intelligent communication, computing, and control technologies have paved the way for the development of the Industrial Internet of Things. The IIoT incorporates machine learning and massively parallel distributed systems such as clouds, clusters, and grids for big data storage, processing, and analytics. In IIoT, end devices continuously generate and transmit data streams, resulting in increased network traffic between device-cloud communication. Moreover, it increases in-network data transmissions. requiring additional efforts for big data processing, management, and analytics. To cope with these engendered issues, this article first introduces a novel concentric computing model (CCM) paradigm composed of sensing systems, outer and inner gateway processors, and central processors (outer and inner) for the deployment of big data analytics applications in IIoT. Second, we investigate, highlight, and report recent research efforts directed at the IIoT paradigm with respect to big data analytics. Third, we identify and discuss indispensable challenges that remain to be addressed for employing CCM in the IIoT paradigm. Lastly, we provide several future research directions (e.g., real-time data analytics, data integration, transmission of meaningful data, edge analytics, real-time fusion of streaming data, and security and privacy).","0163-6804;01636804","","10.1109/MCOM.2018.1700632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8291112","","Big Data;Cloud computing;Data analytics;Logic gates;Production facilities;Program processors;Robot sensing systems;Servers","Big Data;Internet of Things;data analysis;learning (artificial intelligence)","Big Data analytics applications;IIoT paradigm;concentric computing model;data integration;data privacy;data security;data streams;device-cloud communication;edge analytics;in-network data transmissions;industrial IoT;inner gateway processors;outer gateway processors;real-time data analytics;streaming data","","","","","","","","Feb. 2018","","IEEE","IEEE Journals & Magazines"
"The technical hashtag in Twitter data: A hadoop experience","I. Moise","ETH Zurich, Switzerland","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3519","3528","The continuously growing wealth of data has radically changed the data science landscape. At the same time, Big Data tools have known important progress in terms of optimising performance and scalability. However, applying them into practical deployment settings is still a challenging task that is highly dependent on the particularities of the data. In this paper, we present our experiences with implementing a Big Data analytics pipeline with the purpose of extracting value from Twitter data. We acquire and process nearly 60 million tweets that capture the recent outbreaks of the Ebola and Zika viruses. Our processing pipeline first extracts useful information from tweets and then applies a topic modelling technique, provided by Mahout, a Hadoop-based machine learning library. We further extend our Twitter analysis with the study of temporal evolution of daily sentiment toward an important topic, as expressed through the social platform. We highlight at each level, the technical challenges originating from the specific nature of Twitter data and the lessons drawn from our work.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7841015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841015","Big Data;Hadoop;Mahout;Twitter data analysis;optimisation;performance;sentiment analysis;topic modelling","Big data;Data acquisition;Data analysis;Data mining;Data models;Pipelines;Twitter","Big Data;information retrieval;learning (artificial intelligence);parallel processing;sentiment analysis;social networking (online)","Big Data analytics pipeline;Big Data tools;Ebola viruses;Hadoop;Hadoop-based machine learning library;Mahout library;Twitter analysis;Twitter data;Zika viruses;daily sentiment evolution;data science;information extraction;performance optimisation;scalability optimisation;social platform;technical hashtag;temporal evolution;topic modelling","","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conferences"
"A study of a video analysis framework using Kafka and spark streaming","A. Ichinose; A. Takefusa; H. Nakada; M. Oguchi","Ochanomizu University 2-1-1 Otsuka, Bunkyo-ku, Tokyo, 112-8610, Japan","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","2396","2401","As the use of various sensors and cloud computing technologies has spread, many life-log analysis applications for safety services for the elderly and children have been developed. However, it is difficult to perform real-time large data processing in clouds due to the computational complexity of the analysis because efficient deployment schemes of streaming computing components over cloud resources have not been well-investigated. In this study, we propose a video analysis framework that collects videos from multiple cameras and analyzes them using Apache Kafka and Apache Spark Streaming. We first investigate the data transfer performance of Apache Kafka and examine efficient cluster configuration and parameter settings. We then apply this configuration to the proposed framework and measure the data analysis throughput. The experimental results show that the overall throughput varies depending on the number of broker nodes that store data, the number of topic partitions of data, and the number of nodes that conduct analysis processing. In addition, it is confirmed that the number of cores is needed to consider for the efficient cluster configuration, and that the network bandwidth between the nodes becomes a bottleneck as the amount of data and the number of components increase.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258195","","Cloud computing;Data transfer;Distributed databases;Machine learning;Sparks;Streaming media;Throughput","Big Data;cloud computing;data analysis;fault tolerant computing;pattern clustering;video signal processing","Apache Kafka;Apache Spark Streaming;analysis applications;cloud computing technologies;cloud resources;cluster configuration;computational complexity;computing components;data analysis throughput;data processing;data storage;data transfer performance;deployment schemes;elderly children;safety services;video analysis framework","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Knowledge acquisition using parallel rough set and mapreduce from big data","S. Jadhav; S. Suryawanshi","Department of Computer Engineering, Savitribai Phule Pune University, City-Pune, India","2015 International Conference on Information Processing (ICIP)","20160613","2015","","","16","20","These days, the volume of information is developing at an uncommon rate, enormous information mining, and learning revelation have turned into another test in the time of information mining and machine learning. Large set hypothesis for learning procurement has been effectively connected in information mining. The Map Reduce strategy got more consideration from academic group and also industry for its pertinence in unstructured huge information examination. Clusters are viably utilized for parallel handling application and obtained information speak to into different groups. In this venture we have introduced working and execution stream of the Map Reduce programming ideal model with map and reduce capacity and unpleasant set hypothesis. In this work we have design distributed system. Likewise quickly talk about diverse issues and difficulties that are confronted by Map Reduce while taking care of the huge data. Also, finally we have introduced a few focal points of the Map reduce Programming model.","","Electronic:978-1-4673-7758-4; POD:978-1-4673-7759-1","10.1109/INFOP.2015.7489343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489343","Big Data;Big Data Analytic;Knowledge Acquisition;Map Reduce;Rough Sets","Adaptation models;Google;Knowledge acquisition;Programming profession;Rough sets;Yarn","Big Data;data mining;learning (artificial intelligence);rough set theory","Big Data;Map Reduce programming;Map Reduce strategy;information mining;knowledge acquisition;machine learning;parallel handling application;parallel rough set","","","","15","","","","16-19 Dec. 2015","","IEEE","IEEE Conferences"
"Classifying text documents using unconventional representation","B. S. Harish; S. V. Aruna Kumar; S. Manjunath","Dept. of Inf. Sci. & Eng., S.J. Coll. of Eng., Mysore, India","2014 International Conference on Big Data and Smart Computing (BIGCOMP)","20140217","2014","","","210","216","Classification of text documents is one of the most common themes in the field of machine learning. Although a text document expresses a wide range of information, but it lacks the imposed structure of tradition database. Thus, unstructured data, particularly free running text data has to be transferred into a structured data. Hence, in this paper we represent the text document unconventionally by making use of symbolic data analysis concepts. We propose a new method of representing documents based on clustering of term frequency vectors. Term frequency vectors of each cluster are used to form a symbolic representation by the use of Mean and Standard Deviation. Further, term frequency vectors are used in the form a interval valued features. To cluster the term frequency vectors, we make use of Single Linkage, Complete Linkage, Average Linkage, K-Means and Fuzzy C-Means clustering algorithms. To corroborate the efficacy of the proposed model we conducted extensive experimentations on standard datasets like 20 Newsgroup Large, 20 Mini Newsgroup, Vehicles Wikipedia datasets and our own created datasets like Google Newsgroup and Research Article Abstracts. Experimental results reveal that the proposed model gives better results when compared to the state of the art techniques. In addition, as the method is based on a simple matching scheme, it requires a negligible time.","2375-933X;2375933X","Electronic:978-1-4799-3919-0; POD:978-1-4799-3920-6","10.1109/BIGCOMP.2014.6741438","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6741438","Classification;Clustering Algorithms;Representation;Text Documents","Accuracy;Classification algorithms;Clustering algorithms;Couplings;Text categorization;Training;Vectors","data analysis;data structures;fuzzy set theory;learning (artificial intelligence);pattern classification;pattern clustering;pattern matching;text analysis","K-means clustering algorithms;average linkage;complete linkage;free running text data;fuzzy C-means clustering algorithms;interval valued features;machine learning;matching scheme;mean;single linkage;standard datasets;standard deviation;symbolic data analysis;symbolic representation;term frequency vector clustering;text document classification;unconventional text document representation;unstructured data","","1","","26","","","","15-17 Jan. 2014","","IEEE","IEEE Conferences"
"A streaming clustering approach using a heterogeneous system for big data analysis","D. Lee; A. Althoff; D. Richmond; R. Kastner","The Department of Electrical and Computer Engineering, University of California, San Diego, La Jolla, CA, USA","2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)","20171214","2017","","","699","706","Data clustering is a fundamental challenge in data analytics. It is the main task in exploratory data mining and a core technique in machine learning. As the volume, variety, velocity, and variability of data grows, we need more efficient data analysis methods that can scale towards increasingly large and high dimensional data sets. We develop a streaming clustering algorithm that is highly amenable to hardware acceleration. Our algorithm eliminates the need to store the data objects, which removes limits on the size of the data that we can analyze. Our algorithm is highly parameterizable, which allows it to fit to the characteristics of the data set, and scale towards the available hardware resources. Our streaming hardware core can handle more than 40 Msamples/s when processing 3-dimensional streaming data and up to 1.78 Msamples/s for 70-dimensional data. To validate the accuracy and performance of our algorithms we compare it with several common clustering techniques on several different applications. The experimental result shows that it outperforms other prior hardware accelerated clustering systems.","","Electronic:978-1-5386-3093-8; POD:978-1-5386-3094-5","10.1109/ICCAD.2017.8203845","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8203845","FPGA;Online clustering;hardware acceleration;hardware-software codesign;streaming architecture;vector quantization","Acceleration;Algorithm design and analysis;Approximation algorithms;Clustering algorithms;Field programmable gate arrays;Hardware;Partitioning algorithms","Big Data;data analysis;data mining;learning (artificial intelligence);pattern clustering","3D streaming data;big data analysis;common clustering techniques;core technique;data analytics;data clustering;data objects;dimensional data;efficient data analysis methods;exploratory data mining;hardware accelerated clustering systems;hardware acceleration;hardware resources;heterogeneous system;high dimensional data sets;increasingly large data sets;machine learning;streaming clustering algorithm;streaming clustering approach;streaming hardware core","","","","","","","","13-16 Nov. 2017","","IEEE","IEEE Conferences"
"Learning Stable Multilevel Dictionaries for Sparse Representations","J. J. Thiagarajan; K. Natesan Ramamurthy; A. Spanias","Sensor Signal and Information Processing Center, School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA","IEEE Transactions on Neural Networks and Learning Systems","20150817","2015","26","9","1913","1926","Sparse representations using learned dictionaries are being increasingly used with success in several data processing and machine learning applications. The increasing need for learning sparse models in large-scale applications motivates the development of efficient, robust, and provably good dictionary learning algorithms. Algorithmic stability and generalizability are desirable characteristics for dictionary learning algorithms that aim to build global dictionaries, which can efficiently model any test data similar to the training samples. In this paper, we propose an algorithm to learn dictionaries for sparse representations from large scale data, and prove that the proposed learning algorithm is stable and generalizable asymptotically. The algorithm employs a 1-D subspace clustering procedure, the K-hyperline clustering, to learn a hierarchical dictionary with multiple levels. We also propose an information-theoretic scheme to estimate the number of atoms needed in each level of learning and develop an ensemble approach to learn robust dictionaries. Using the proposed dictionaries, the sparse code for novel test data can be computed using a low-complexity pursuit procedure. We demonstrate the stability and generalization characteristics of the proposed algorithm using simulations. We also evaluate the utility of the multilevel dictionaries in compressed recovery and subspace learning applications.","2162-237X;2162237X","","10.1109/TNNLS.2014.2361052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6926841","Compressed sensing;dictionary learning;generalization;sparse representations;stability;stability.","Algorithm design and analysis;Asymptotic stability;Clustering algorithms;Dictionaries;Stability analysis;Training;Vectors","learning (artificial intelligence);pattern clustering;signal representation","1D subspace clustering procedure;algorithmic stability;compressed recovery applications;data processing applications;dictionary learning algorithms;ensemble approach;generalizability;global dictionaries;hierarchical dictionary;information-theoretic scheme;k-hyperline clustering;large scale data;low-complexity pursuit procedure;machine learning applications;multilevel dictionaries;sparse code;sparse representations;subspace learning applications;test data","0","4","","54","","","20141016","Sept. 2015","","IEEE","IEEE Journals & Magazines"
"An effective and efficient grid-based data clustering algorithm using intuitive neighbor relationship for data mining","C. F. Tsai; S. C. Huang","Department of Management Information Systems, National Pingtung University of Science and Technology, Pingtung 91201, Taiwan","2015 International Conference on Machine Learning and Cybernetics (ICMLC)","20151203","2015","2","","478","483","This paper presents a new data clustering technique. It is a new grid-based clustering scheme by intuitive neighbor relationship for enhancing data clustering performance. Compared to other algorithms, this improved grid-based clustering algorithm substantially decreases repetitive clustering checks of neighboring grids and greatly improve the efficiency of data processing. Our simulations demonstrate that the proposed data clustering technique delivers better performance, in terms of clustering correctness rate and noise filtering rate, than perform other well-known existing algorithms, GOD-CS, CLIQUE and TING. To our best knowledge, the proposed data clustering technique may be the rapid method in the world currently.","","CD-ROM:978-1-4673-7220-6; Electronic:978-1-4673-7221-3; POD:978-1-4673-7222-0","10.1109/ICMLC.2015.7340603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7340603","Data mining;Dataclustering;Density-based clustering","Algorithm design and analysis;Clustering algorithms;Clustering methods;Cybernetics;Data mining;Databases;Machine learning algorithms","data mining;filtering theory;grid computing;pattern clustering","CLIQUE;GOD-CS;TING;clustering correctness rate;data clustering performance;data clustering technique;data mining;data processing;grid-based clustering algorithm;grid-based clustering scheme;grid-based data clustering algorithm;intuitive neighbor relationship;noise filtering rate;repetitive clustering check","","","","17","","","","12-15 July 2015","","IEEE","IEEE Conferences"
"A new algorithm for money laundering detection based on structural similarity","R. Soltani; U. T. Nguyen; Y. Yang; M. Faghani; A. Yagoub; A. An","Department of Electrical Engineering and Computer Science, York University, Toronto, Canada","2016 IEEE 7th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)","20161212","2016","","","1","7","Money Laundering (ML) is the process of cleaning “dirty” money, thereby making the source of funds no longer identifiable. Detecting money laundering activities is a challenging task due to huge volumes of financial transactions being made in a global market on a daily basis. This paper proposes a novel approach for detecting money laundering transactions among large volumes of financial data in an efficient and accurate manner. We propose a framework that applies case reduction methods to progressively reduce the input data set to a significantly smaller size. The framework then scans the reduced data to find pairs of transactions with common attributes and behaviours that are potentially involved in ML activities. It then applies a clustering method to detect potential ML groups. We present preliminary experimental results that demonstrate the effectiveness of the proposed framework.","","Electronic:978-1-5090-1496-5; POD:978-1-5090-1497-2","10.1109/UEMCON.2016.7777919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7777919","Money laundering;graph theory;money laundering detection;structural similarity","Clustering algorithms;Clustering methods;Government;Network topology;Receivers;Topology","financial data processing;globalisation","ML activities;ML groups;financial data;financial transactions;global market;money laundering detection;money laundering transactions;structural similarity","","","","","","","","20-22 Oct. 2016","","IEEE","IEEE Conferences"
"RHadoop-based fuzzy data mining: Architecture, design and system implementation","P. Sun; L. Xu; H. Fan","School of Software Engineering, Tongji University, Shanghai, China","2016 IEEE International Conference on Big Data Analysis (ICBDA)","20160714","2016","","","1","5","Data mining is a challenge for end-users, which requires knowledge and skills on business domains, data mining algorithms and software development. In response to the challenge, we have proposed, designed and implemented a novel data mining system named RFDM (RHadoop-based Fuzzy Data Mining), which supports fuzzy data mining process and experience with user convenience and reduced cost. The system is capable of supporting fully-automated data mining life-cycle activities, with limited user interactions in dataset uploading and data mining configuration. In addition, a RHadoop-based framework has been integrated, which meets the requirements of large-scale datasets in data mining. Experiments have indicated that the RFDM system achieves enhanced performance while supporting fuzzy data mining.","","CD-ROM:978-1-4673-9589-2; Electronic:978-1-4673-9591-5; POD:978-1-4673-9592-2","10.1109/ICBDA.2016.7509796","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7509796","MapReduce;RHadoop;data mining;fuzzy;large-scale","Algorithm design and analysis;Classification algorithms;Clustering algorithms;Data mining;Data models;Machine learning algorithms;Training","business data processing;data handling;data mining;fuzzy set theory;parallel processing","RFDM;RHadoop-based fuzzy data mining;business domains;data mining algorithms;fully-automated data mining life-cycle activities;fuzzy data mining process;software development;user convenience","","","","","","","","12-14 March 2016","","IEEE","IEEE Conferences"
"Table of contents","","","2016 6th International Conference on IT Convergence and Security (ICITCS)","20161110","2016","","","1","5","Presents the table of contents/splash page of the proceedings record.","","Electronic:978-1-5090-3765-0; POD:978-1-5090-3766-7","10.1109/ICITCS.2016.7740300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7740300","","","Big Data;Internet of Things;Markov processes;SQL;XML;application program interfaces;aspect-oriented programming;astronomy computing;audio watermarking;augmented reality;biometrics (access control);body area networks;channel capacity;cloud computing;computer based training;cooperative communication;cryptographic protocols;data integration;data structures;decision support systems;discrete wavelet transforms;driver information systems;electrocardiography;electronic messaging;energy conservation;error correction codes;feature extraction;femtocellular radio;formal specification;genetic algorithms;gesture recognition;image filtering;image segmentation;information systems;invasive software;learning (artificial intelligence);medical image processing;meta data;neural nets;object detection;ontologies (artificial intelligence);optical radar;parallel programming;power aware computing;problem solving;program testing;query processing;radiofrequency identification;resource allocation;sentiment analysis;signal classification;signal detection;speech enhancement;sport;stereo image processing;storage management;strategic planning;ubiquitous computing;ultra wideband communication;user interfaces;video surveillance;vocabulary;wireless LAN;wireless sensor networks","3D optical microscope;AOP-based approach;AR navigation;ARM64bit Server;Angular 2 framework;BLE;Bgslibrary algorithms;Big Data security analysis;CUDA-based acceleration techniques;EPS-based motion recognition systems;ES information;FARIS;HARN algorithm;IT demand governance;Internet of Things environment;JIT compilation-based unified SQL query optimization system;JSON data;LiDAR data;MapReduce;Markov decision process based routing algorithm;SMS-based mobile botnet detection module;SQL-on-Hadoop engines;SSH attacks;SSL/TLS nonintrusive proxy;SmartDriver;TH-UWB;UHF RFID;WBAN;WSN;WeChat text messages service flow traffic classification;Web user interface technology;Wi-Fi based broadcasting system;XML data;active eavesdropping;aspect-oriented user interfaces design integration;astronaut virtual training system;augmented reality;automated ECG beat classification system;automatic weather system;automotive control Software;business goal structuring notation;classifier performance;cloud computing;common vocabulary set;complex security requirements patterns;context-aware user interface field classification;convolutional neural networks;cooperative spectrum sensing scheme;data integration;data presentations;decision support system;distributed micro data center;distributed multi-platform context-aware user interface;domain specific machine;e-navigation services;energy impact;error correcting code biometric template protection;example-based retrieval system;exploratory testing;fast and memory-efficient URL filter;femtocell network;flexible authentication protocol;full-duplex jamming attack;genetic algorithm;harvest-then-transmit protocol;head-up display;hierarchical cluster network;holistic service orchestration;holistic-based feature extraction;human gesture recognition;human motion data;image filtering;image-based ship detection;interactive event recognition;kinematic constraint method;layout familiarization training;lexical substitution;light-weight API-call safety checking;linguistic based steganography;load balancing;machine learning technique;maritime equipment;max-min energy-efficiency optimization;medical image segmentation;memory allocation techniques;metadata extension;mobile devices;model-based testing;multilevel DWT;network based IMSI catcher detection;neural stegoclassifier;ontology modeling;packet length covert channel capacity estimation;parallel prime number labeling;partial materialization;people detection;phase-error based filters;polymorphic malware detection;power-aware data structure;proactive flood control;project management software;real-time road surface condition determination algorithm;repulsion-propulsion firefly algorithm;resource allocation;secure agent based architecture;secure distance bounding protocol;semantic video understanding;sensitive adaptive thresholding;sentence based mathematical problem solving approach;sentiment analysis;software requirement specification;speech enhancement;stereo image correction;stereo-based tag association;strategic information systems planning;synchronized blind audio watermarking;syntactical transformation;traffic surveillance video;unequal loss protection;vehicle information;video quality;wavelet transform;weighted dynamic time warping;wellness sports industry;windowed vector modulation;wireless powered communication network;z-transform based encryption algorithm","","","","","","","","26-26 Sept. 2016","","IEEE","IEEE Conferences"
"Using Word2Vec to process big text data","L. Ma; Y. Zhang","Computer Science Department, Georgia State University, Atlanta, Georgia","2015 IEEE International Conference on Big Data (Big Data)","20151228","2015","","","2895","2897","Big data is a broad data set that has been used in many fields. To process huge data set is a time consuming work, not only due to its big volume of data size, but also because data type and structure can be different and complex. Currently, many data mining and machine learning technique are being applied to deal with big data problem; some of them can construct a good learning algorithm in terms of lots of training example. However, considering the data dimension, it will be more efficient if learning algorithm is capable of selecting useful features or decreasing the feature dimension. Word2Vec, proposed and supported by Google, is not an individual algorithm, but it consists of two learning models, Continuous Bag of Words (CBOW) and Skip-gram. By feeding text data into one of learning models, Word2Vec outputs word vectors that can be represented as a large piece of text or even the entire article. In our work, we first training the data via Word2Vec model and evaluated the word similarity. In addition, we clustering the similar words together and use the generated clusters to fit into a new data dimension so that the data dimension is decreased.","","Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9","10.1109/BigData.2015.7364114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7364114","Word2Vec;big data;clustering;feature reduction","Algorithm design and analysis;Big data;Clustering algorithms;Data models;Machine learning algorithms;Training;Training data","Big Data;data mining;learning (artificial intelligence);text analysis","CBOW;Word2Vec model;big text data processing;continuous bag-of-words;data dimension;data mining;data structure;data type;machine learning technique;skip-gram","","3","","12","","","","Oct. 29 2015-Nov. 1 2015","","IEEE","IEEE Conferences"
"MapReduce Based Classification for Fault Detection in Big Data Applications","M. O. Shafiq; M. Fekri; R. Ibrahim","Sch. of Inf. Technol., Carleton Univ., Ottawa, ON, Canada","2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)","20180118","2017","","","637","642","Recently emerging software applications are large, complex, distributed and data-intensive, i.e., big data applications. That makes the monitoring of such applications a challenging task due to lack of standards and techniques for modeling and analysis of execution data (i.e., logs) produced by such applications. Another challenge imposed by big data applications is that the execution data produced by such applications also has high volume, velocity, variety, and require high veracity, value. In this paper, we present our monitoring solution that performs real-time fault detection in big data applications. Our solution is two-fold. First, we prescribe a standard model for structuring execution logs. Second, we prescribe a Bayesian classification based analysis solution that is MapReduce compliant, distributed, parallel, single pass and incremental. That makes it possible for our proposed solution to be deployed and executed on cloud computing platforms to process logs produced by big data applications. We have carried out complexity, scalability, and usability analysis of our proposed solution that how efficiently and effectively it can perform fault detection in big data applications.","","Electronic:978-1-5386-1418-1; POD:978-1-5386-1419-8","10.1109/ICMLA.2017.00-89","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8260703","Applications;Bayesian Classification;Big Data;Fault detection;MapReduce","Algorithm design and analysis;Bayes methods;Big Data applications;Classification algorithms;Data mining;Program processors","Bayes methods;Big Data;cloud computing;data mining;fault diagnosis;parallel processing;pattern classification","Bayesian classification based analysis solution;MapReduce based classification;big data applications;cloud computing platforms;execution data;execution logs;monitoring solution;real-time fault detection;software applications;standard model","","","","","","","","18-21 Dec. 2017","","IEEE","IEEE Conferences"
"Big data analytics: Solution to healthcare","M. Singh; V. Bhatia; R. Bhatia","Bachelor of Technology (CSE), GGSIPU, New Delhi, India","2017 International Conference on Intelligent Communication and Computational Techniques (ICCT)","20180326","2017","","","239","241","An enormous amount of data is generated daily by the medical organizations, which collectively consists patients, healthcare centers, medical specialists and of course, the diseases. The data is huge and provides an insight into future predictions, which might definitely prevent maximum medical cases from happening. But without big data analytics techniques and the Hadoop cluster, this data remains useless. Through this paper, we will explain how realtime data m ay be useful to analyze and predict severe emergency cases pretty earlier.","","Electronic:978-1-5386-3030-3; POD:978-1-5386-1868-4","10.1109/INTELCCT.2017.8324052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8324052","Analytics;Big Data;Deep Learning;HealthCare;Recommendation Systems","Atmospheric measurements;Big Data;Influenza;Machine learning;Proteins","Big Data;data analysis;diseases;health care;medical information systems;patient care","Big Data analytics;diseases;healthcare centers;maximum medical cases;medical organizations;medical specialists;realtime data","","","","","","","","22-23 Dec. 2017","","IEEE","IEEE Conferences"
"Mapreduce-Apriori algorithm under cloud computing environment","X. Z. Chang","Department of Information Engineering, Shijiazhuang Institute of Railway Technology, Shijiazhuang 050000, China","2015 International Conference on Machine Learning and Cybernetics (ICMLC)","20151203","2015","2","","637","641","Apriori is a classical data mining algorithm. The traditional Apriori algorithm can be optimized to apply to MapReduce model. The MapReduce programming model with the optimized Apriori is built to realize the parallel computing under cloud computing environment. Experimental results show that improved parallel MR-Apriori algorithm greatly shortens the time consumed, and with its strong scalability, it can be better applied to large-scale data analysis, processing and mining. Moreover, the rate of the algorithm is linearly increased with the increase of the node numbers in the mining frequent item sets.","","CD-ROM:978-1-4673-7220-6; Electronic:978-1-4673-7221-3; POD:978-1-4673-7222-0","10.1109/ICMLC.2015.7340629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7340629","Apriori Algorithm;Association Rules;Cloud Computing;MapReduce Model","","cloud computing;data analysis;data mining;parallel programming","MapReduce programming model;MapReduce-apriori algorithm;cloud computing environment;data mining algorithm;data processing;frequent item sets mining;large-scale data analysis;optimized apriori;parallel MR-apriori algorithm;parallel computing","","","","6","","","","12-15 July 2015","","IEEE","IEEE Conferences"
"Distributed forests for MapReduce-based machine learning","R. Wakayama; R. Murata; A. Kimura; T. Yamashita; Y. Yamauchi; H. Fujiyoshi","Chubu University, Japan","2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)","20160609","2015","","","276","280","This paper proposes a novel method for training random forests with big data on MapReduce clusters. Random forests are well suited for parallel distributed systems, since they are composed of multiple decision trees and every decision tree can be independently trained by ensemble learning methods. However, naive implementation of random forests on distributed systems easily overfits the training data, yielding poor classification performances. This is because each cluster node can have access to only a small fraction of the training data. The proposed method tackles this problem by introducing the following three steps. (1) ""Shared forests"" are built in advance on the master node and shared with all the cluster nodes. (2) With the help of transfer learning, the shared forests are adapted to the training data placed on each cluster node. (3) The adapted forests on every cluster node are returned to the master node, and irrelevant trees yielding poor classification performances are removed to form the final forests. Experimental results show that our proposed method for MapReduce clusters can quickly learn random forests without any sacrifice of classification performance.","","Electronic:978-1-4799-6100-9; POD:978-1-4799-6101-6; USB:978-1-4799-6099-6","10.1109/ACPR.2015.7486509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486509","","Computer architecture;Decision trees;Distributed databases;Learning systems;Training;Training data;Vegetation","Big Data;decision trees;learning (artificial intelligence);parallel programming","Big Data;MapReduce clusters;MapReduce-based machine learning;cluster nodes;decision trees;distributed forests;ensemble learning methods;master node;parallel distributed systems","","1","","9","","","","3-6 Nov. 2015","","IEEE","IEEE Conferences"
"Optimizing routine collection efficiency in IoT based garbage collection monitoring systems","S. Ray; S. Tapadar; S. K. Chatterjee; R. Karlose; S. Saha; H. N. Saha","Department of Computer Science and Engineering Institute of Engineering and Management, Kolkata, India","2018 IEEE 8th Annual Computing and Communication Workshop and Conference (CCWC)","20180226","2018","","","84","90","Ubiquitous objects are getting “smarter” and more “connected”, every day. With this ever-growing Internet of Things, every object can now be uniquely identified and made to communicate with each other. This approach has been applied to dustbins too, to monitor garbage collection, throwing light on numerous valuable insights. Our project too employs a similar approach, to not only monitor garbage collection but also optimize it, using machine learning. The method of unsupervised learning we utilize is K Means Clustering, widely used in data mining and analytics. Our physical device uses an ultrasonic sensor to be aware of a dustbin's current content level. If the level reaches or exceeds a threshold percentage of the total capacity of the dustbin, it informs our servers, via an online application programming interface (API) developed for this purpose. The API also stores related data - fill time, cleanup time, and location, to name a few. This dynamic dataset generated is analyzed by our algorithm, to determine the times of the day, when a regular cleanup should be performed, such that the dustbins are clean, for the maximum possible portion of the day. The algorithm also shows the locations, where another dustbin should be installed, for further optimization. This is found out by inspecting each cluster individually and scanning out - items which are the furthest away from its closest centroid; and multiple items related to the same dustbin. In either case, a new dustbin installation is advised at such locations. Data henceforth generated revealed that the installation has had a positive effect on the optimization.","","Electronic:978-1-5386-4649-6; POD:978-1-5386-4650-2","10.1109/CCWC.2018.8301629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8301629","Clustering;Data Mining;Internet of Things;Waste management","Acoustics;Computer science;Hardware;Internet of Things;Microcontrollers;Monitoring;Wireless fidelity","Internet of Things;application program interfaces;data analysis;data mining;environmental science computing;learning (artificial intelligence);pattern clustering;refuse disposal;storage management;ubiquitous computing;unsupervised learning","API;Internet of Things;IoT based garbage collection monitoring systems;cleanup time;data analytics;data mining;dustbin installation;fill time;k means clustering;machine learning;online application programming interface;routine collection efficiency;ubiquitous objects;unsupervised learning","","","","","","","","8-10 Jan. 2018","","IEEE","IEEE Conferences"
"Analysing and Predicting the Runtime of Social Graphs","R. Maher; D. Malone","Hamilton Inst., Maynooth Univ., Maynooth, Ireland","2016 IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking (SocialCom), Sustainable Computing and Communications (SustainCom) (BDCloud-SocialCom-SustainCom)","20161031","2016","","","370","376","The explosion of Social Network Analysis (SNA) in many different areas and the growing need for powerful data analysis has emphasized the importance of in-memory big data processing in computer systems. Particularly, large-scale graphs are gaining much more attention due to their wide range of application. This rise, accompanied by a massive number of vertices and edges, led computations to become increasingly expensive and time consuming. That is why there is a move towards distributed systems or Big Data cluster(s) to provide the required computational power and memory to handle such demand of huge graphs. Thus, figuring out whether a new social graph dataset can be processed successfully on a personal machine or there is a need for a distributed system or big-memory machine is still a remaining open question. In this paper, we try to address this question by providing a comparative analysis for the performance of two of the most well known SNA tools for performing commonly used graph algorithms such as counting Triads, calculating Degree Distribution and finding Clusters which can give an indication of the possibility of carrying out the work on a personal machine. Based on these measurements, we train different supervised machine learning models for predicting the execution time of these algorithms. We compare the accuracy of the different machine learning models and provided the details of the most accurate model that can be exploited by end users to better estimate the execution time expected for processing new social graphs on a personal machine.","","","10.1109/BDCloud-SocialCom-SustainCom.2016.62","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7723716","Graph Algorithms;Graph Analytics;Performance;Predictive Modeling;Social Graphs;Social Network Analysis","Algorithm design and analysis;Clustering algorithms;Database languages;Facebook;Machine learning algorithms;Prediction algorithms","Big Data;data analysis;learning (artificial intelligence);social networking (online)","SNA;data analysis;graph edges;graph vertices;in-memory big data processing;large-scale graph;social graph dataset;social network analysis;supervised machine learning models","","","","","","","","8-10 Oct. 2016","","IEEE","IEEE Conferences"
"Meta-learning for large scale machine learning with MapReduce","X. Liu; X. Wang; S. Matwin; N. Japkowicz","Sch. of EECS, Univ. of Ottawa, Ottawa, ON, Canada","2013 IEEE International Conference on Big Data","20131223","2013","","","105","110","We have entered the big data age. Knowledge extraction from massive data is becoming more and more rewarding and urgent. MapReduce has provided a feasible framework for programming machine learning algorithms in Map and Reduce functions. The relatively simple programming interface has helped to solve machine learning algorithms' scalability problems. However, this framework suffers from an obvious weakness: it does not support iterations. This makes those algorithms requiring iterations difficult to fully explore the efficiency of MapReduce. In this paper, we propose to apply Meta-learning programmed with MapReduce to avoid parallelizing machine learning algorithms while also improving their scalability to big datasets. The experiments conducted on Hadoop fully distributed mode on Amazon EC2 demonstrate that our algorithm PML reduces the training computational complexity significantly when the number of computing nodes increases while gaining smaller error rates than those on one single node. The comparison of PML with the contemporary parallelized AdaBoost algorithm: AdaBoost.PL shows that PML has lower error rates.","","Electronic:978-1-4799-1293-3; POD:978-1-4799-1294-0","10.1109/BigData.2013.6691741","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691741","Adaboost;MapReduce;big data;meta-learning;parallel computing","Algorithm design and analysis;Classification algorithms;Computational modeling;Error analysis;Machine learning algorithms;Training;Training data","Big Data;application program interfaces;knowledge acquisition;learning (artificial intelligence);parallel programming","Amazon EC2;Map function;MapReduce;PML;Reduce function;big dataset scalability improvement;computational complexity;computing nodes;error rates;fully-distributed Hadoop mode;knowledge extraction;large-scale machine learning algorithm programming;machine learning algorithm scalability problems;massive data;meta-learning;programming interface","","3","","25","","","","6-9 Oct. 2013","","IEEE","IEEE Conferences"
"GPU in-Memory Processing Using Spark for Iterative Computation","S. Hong; W. Choi; W. K. Jeong","Sch. of Electr. & Comput. Eng., Ulsan Nat. Inst. of Sci. & Technol., Ulsan, South Korea","2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)","20170713","2017","","","31","41","Due to its simplicity and scalability, MapReduce has become a de facto standard computing model for big data processing. Since the original MapReduce model was only appropriate for embarrassingly parallel batch processing, many follow-up studies have focused on improving the efficiency and performance of the model. Spark follows one of these recent trends by providing in-memory processing capability to reduce slow disk I/O for iterative computing tasks. However, the acceleration of Spark's in-memory processing using graphics processing units (GPUs) is challenging due to its deep memory hierarchy and host-to-GPU communication overhead. In this paper, we introduce a novel GPU-accelerated MapReduce framework that extends Spark's in-memory processing so that iterative computing is performed only in the GPU memory. Having discovered that the main bottleneck in the current Spark system for GPU computing is data communication on a Java virtual machine, we propose a modification of the current Spark implementation to bypass expensive data management for iterative task offloading to GPUs. We also propose a novel GPU in-memory processing and caching framework that minimizes host-to-GPU communication via lazy evaluation and reuses GPU memory over multiple mapper executions. The proposed system employs message-passing interface (MPI)-based data synchronization for inter-worker communication so that more complicated iterative computing tasks, such as iterative numerical solvers, can be efficiently handled. We demonstrate the performance of our system in terms of several iterative computing tasks in big data processing applications, including machine learning and scientific computing. We achieved up to 50 times speed up over conventional Spark and about 10 times speed up over GPU-accelerated Spark.","","Electronic:978-1-5090-6611-7; POD:978-1-5090-5980-5","10.1109/CCGRID.2017.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7973686","GPU;In-memory Computing;MapReduce;Spark","Big Data;Computational modeling;Data models;Distributed databases;Graphics processing units;Market research;Sparks","cache storage;graphics processing units;iterative methods;message passing;parallel processing;synchronisation","GPU computing;GPU in-memory processing;GPU-accelerated MapReduce;Java virtual machine;MPI;Spark;caching;data communication;data synchronization;disk I/O;graphics processing units;host-to-GPU communication overhead;inter-worker communication;iterative computation;iterative computing;iterative numerical solvers;memory hierarchy;message-passing interface","","","","","","","","14-17 May 2017","","IEEE","IEEE Conferences"
"SPynq: Acceleration of machine learning applications over Spark on Pynq","C. Kachris; E. Koromilas; I. Stamelos; D. Soudris","Institute of Communication and Computer Systems (ICCS/NTUA), Athens, Greece","2017 International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS)","20180423","2017","","","70","77","Spark is one of the most widely used frameworks for data analytics that offers fast development of applications like machine learning and graph computations in distributed systems. In this paper, we present SPynq: A framework for the efficient utilization of hardware accelerators over the Spark framework on heterogeneous MPSoC FPGAs, such as Zynq. Spark has been mapped to the Pynq platform and the proposed framework allows the seamlessly utilization of the programmable logic for the hardware acceleration of computational intensive Spark kernels. We have also developed the required libraries in Spark that hides the accelerator's details to minimize the design effort to utilize the accelerators. A cluster of 4 nodes (workers) based on the all-programmable MPSoCs has been implemented and the proposed platform is evaluated in a typical machine learning application based on logistic regression. The logistic regression kernel has been developed as an accelerator and incorporated to the Spark. The developed system is compared to a high-performance Xeon cluster that is typically used in cloud computing. The performance evaluation shows that the heterogeneous accelerator-based MpSoC can achieve up to 2.3x system speedup compared with a Xeon system (with 90% accuracy) and 20x better energy-efficiency. For embedded application, the proposed system can achieve up to 40x speedup compared to the software only implementation on low-power embedded processors and 30x lower energy consumption.","","Electronic:978-1-5386-3437-0; POD:978-1-5386-3438-7","10.1109/SAMOS.2017.8344613","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8344613","","Cloud computing;Field programmable gate arrays;Hardware;Libraries;Machine learning;Program processors;Sparks","","","","","","","","","","17-20 July 2017","","IEEE","IEEE Conferences"
"A theoretical and experimental analysis on K nearest neighbour joins in big data","S. S. Shree; M. Ahmed","Dept of Information Science and Engineering, Dayananda Sagar College of Engineering, Karnataka, India","2017 International Conference on Intelligent Computing and Control Systems (ICICCS)","20180111","2017","","","915","918","kNN is frequently used as a clustering method in machine learning or data mining. The main application of a kNN join is k-nearest neighbour classification. Some data points are given for training and new unlabelled data is given for testing. The focus is to find the class label for the new points. For each unlabelled data in kNN query on training set will be performed to estimate its class membership. This process can be considered as kNN join for new data to test with the training set. The different existing approaches for computing knn on MapReduce are compared, first theoretically and then performing an extensive experimental evaluation. To compare these solutions, there are three generic steps for kNN computation on MapReduce: data pre-processing, data partitioning and computation. Theoretically each step is analysed for load balancing, accuracy and complexity aspects.","","CD:978-1-5386-2744-0; Electronic:978-1-5386-2745-7; POD:978-1-5386-3901-6","10.1109/ICCONS.2017.8250598","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8250598","Euclidean distance;MapReduce;kNN;load balancing;partitioning","Big Data;Control systems;Data mining;Feature extraction;Load management;Training","Big Data;nearest neighbour methods;pattern classification;pattern clustering","MapReduce;big data;class label;class membership;clustering method;data partitioning;data points;data pre-processing;k-nearest neighbour classification;kNN computation;kNN query;machine learning;unlabelled data","","","","","","","","15-16 June 2017","","IEEE","IEEE Conferences"
"Vessel route anomaly detection with Hadoop MapReduce","X. Wang; X. Liu; B. Liu; E. N. de Souza; S. Matwin","Faculty of Computer Science, Dalhousie University, Canada","2014 IEEE International Conference on Big Data (Big Data)","20150108","2014","","","25","30","We present a two-level approach to detect abnormal activities for vessels' routes. The data is obtained from the Automatic Identification System (AIS) which is required to be installed on vessels over specific gross tonnage. In the first level, we develope a Clustering algorithm: Density-based Spatial Clustering of Applications with Noise considering Speed and Direction (DBSCAN_SD). This algorithm is applied to pre-cluster the data points. Using domain knowledge in maritime, experts adjust the results produced by DBSCAN_SD with extra features. In this way, we get the optimal labeling result about whether a data point is normal or abnormal. In the second level, we use the labeled data generated in the first level to train the Parallel Meta-Learning (PML) algorithm on Hadoop. The results show that both accuracy and time complexity results are improved when we increase the number of nodes in a cluster.","","Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8","10.1109/BigData.2014.7004464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004464","AIS data;Anomaly detection;Hadoop;MapReduce","Classification algorithms;Clustering algorithms;Computational modeling;Machine learning algorithms;Noise;Training;Trajectory","data handling;learning (artificial intelligence);marine engineering;marine vehicles;parallel processing;pattern clustering;security of data","AIS;DBSCAN_SD;Hadoop MapReduce;PML algorithm;abnormal activities detection;automatic identification system;clustering algorithm;data points preclustering;density-based spatial clustering of applications with noise;domain knowledge;optimal labeling;parallel meta-learning;speed and direction;two-level approach;vessel route anomaly detection","","2","","15","","","","27-30 Oct. 2014","","IEEE","IEEE Conferences"
"Scaling Hadoop clusters with virtualized volunteer computing environment","E. Kijsipongse; S. U-ruekolan","Nat. Electron. & Comput. Technol. Center, Large-Scale Simulation Res. Lab., Pathumthani, Thailand","2014 11th International Joint Conference on Computer Science and Software Engineering (JCSSE)","20140626","2014","","","146","151","MapReduce framework has commonly been used to perform large-scale data processing, such as social network analysis, data mining as well as machine learning, on cluster computers. However, building a large dedicated cluster for MapReduce is not cost effective if the system is underutilized. To speedup the MapReduce computation with low cost, the computing resources donated from idle desktop/notebook computers in an organization become true potential. The MapReduce framework is then implemented into Volunteer Computing environment to allow such data processing tasks to be carried out on the unused computers. Virtualization technology is deployed to resolve the security and heterogeneity problem in Volunteer Computing so that the MapReduce jobs can always run under a unified runtime and isolated environment. This paper presents a Hadoop cluster that can be scaled into virtualized Volunteer Computing environment. The system consists of a small fixed set of dedicate nodes plus a variable number of volatile volunteer nodes which give additional computing power to the cluster. To this end, we consolidate Apache Hadoop, the most popular MapReduce implementation, with the virtualized BOINC platform. We evaluate the proposed system on our testbed with MapReduce benchmark that represents different workload patterns. The performance of the Hadoop cluster is measured when its computing capability is expanded with volunteer nodes. The results show that the system can be scaled preferably for CPU-intensive jobs, as opposed to data-intensive jobs which their scalability is more restricted.","","CD-ROM:978-1-4799-5821-4; Electronic:978-1-4799-5822-1; POD:978-1-4799-5823-8","10.1109/JCSSE.2014.6841858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6841858","MapReduce;Virtualization;Volunteer Computing","","security of data;virtualisation;volunteer computing","Apache Hadoop;CPU-intensive jobs;Hadoop cluster performance;Hadoop clusters scaling;MapReduce computation;MapReduce framework;MapReduce jobs;computing power;computing resources;data processing tasks;data-intensive jobs;heterogeneity problem;large-scale data processing;security;virtualization technology;virtualized BOINC platform;virtualized volunteer computing environment;volunteer nodes;workload patterns","","0","","19","","","","14-16 May 2014","","IEEE","IEEE Conferences"
"Towards Machine Learning-Based Auto-tuning of MapReduce","N. Yigitbasi; T. L. Willke; G. Liao; D. Epema","Intel Labs., Hillsboro, OR, USA","2013 IEEE 21st International Symposium on Modelling, Analysis and Simulation of Computer and Telecommunication Systems","20140203","2013","","","11","20","MapReduce, which is the de facto programming model for large-scale distributed data processing, and its most popular implementation Hadoop have enjoyed widespread adoption in industry during the past few years. Unfortunately, from a performance point of view getting the most out of Hadoop is still a big challenge due to the large number of configuration parameters. Currently these parameters are tuned manually by trial and error, which is ineffective due to the large parameter space and the complex interactions among the parameters. Even worse, the parameters have to be re-tuned for different MapReduce applications and clusters. To make the parameter tuning process more effective, in this paper we explore machine learning-based performance models that we use to auto-tune the configuration parameters. To this end, we first evaluate several machine learning models with diverse MapReduce applications and cluster configurations, and we show that support vector regression model (SVR) has good accuracy and is also computationally efficient. We further assess our auto-tuning approach, which uses the SVR performance model, against the Starfish auto tuner, which uses a cost-based performance model. Our findings reveal that our auto-tuning approach can provide comparable or in some cases better performance improvements than Starfish with a smaller number of parameters. Finally, we propose and discuss a complete and practical end-to-end auto-tuning flow that combines our machine learning-based performance models with smart search algorithms for the effective training of the models and the effective exploration of the parameter space.","1526-7539;15267539","Electronic:978-0-7695-5102-9; POD:978-1-4799-1209-4","10.1109/MASCOTS.2013.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6730744","big data;distributed systems;hadoop;performance modeling","Accuracy;Benchmark testing;Computational modeling;Data models;Training;Training data;Tuning","distributed programming;learning (artificial intelligence);public domain software;regression analysis;search problems;support vector machines","Hadoop;MapReduce;SVR performance model;cluster configurations;configuration parameters;cost-based performance model;de facto programming model;end-to-end autotuning flow;large-scale distributed data processing;machine learning-based autotuning approach;machine learning-based performance models;parameter tuning process;smart search algorithms;starfish autotuner;support vector regression model","","15","","22","","","","14-16 Aug. 2013","","IEEE","IEEE Conferences"
"A Lightweight MapReduce Framework for Secure Processing with SGX","R. Pires; D. Gavril; P. Felber; E. Onica; M. Pasin","Univ. of Neuchatel, Neucha&#x0301;tel, Switzerland","2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)","20170713","2017","","","1100","1107","MapReduce is a programming model used extensively for parallel data processing in distributed environments. A wide range of algorithms were implemented using MapReduce, from simple tasks like sorting and searching up to complex clustering and machine learning operations. Many of these implementations are part of services externalized to cloud infrastructures. Over the past years, however, many concerns have been raised regarding the security guarantees offered in such environments. Some solutions relying on cryptography were proposed for countering threats but these typically imply a high computational overhead. Intel, the largest manufacturer of commodity CPUs, recently introduced SGX (software guard extensions), a set of hardware instructions that support execution of code in an isolated secure environment. In this paper, we explore the use of Intel SGX for providing privacy guarantees for MapReduce operations, and based on our evaluation we conclude that it represents a viable alternative to a cryptographic mechanism. We present results based on the widely used k-means clustering algorithm, but our implementation can be generalized to other applications that can be expressed using MapReduce model.","","Electronic:978-1-5090-6611-7; POD:978-1-5090-5980-5","10.1109/CCGRID.2017.129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7973819","MapReduce;distributed processing;security","Data models;Data privacy;Encryption;Program processors;Routing","cloud computing;cryptography;data handling;learning (artificial intelligence);parallel processing;pattern clustering","CPU;SGX;cloud infrastructures;cryptography;distributed environments;k-means clustering algorithm;lightweight MapReduce framework;machine learning;parallel data processing;secure processing;software guard extensions","","","","","","","","14-17 May 2017","","IEEE","IEEE Conferences"
"Clustering voluminous of heterogeneous data","M. Kumar; K. S. Rani; C. R. Rao","Engineer GGK Tech, Hyderabad, India","2017 International Conference on Computer Communication and Informatics (ICCCI)","20171123","2017","","","1","5","Clustering analysis is one of the most commonly used data processing algorithm. In this era of data explosion, clustering large volume of data is very challenging. If the data is heterogeneous, it brings more challenges. K-Prototype is an algorithm which aims at clustering mixed dataset which contains numerical as well as categorical data. This algorithm does not distinguish between nominal data and ordinal data. There is no solution available in literature for dealing with voluminous of data having numerical as well as categorical data. In this paper we extended the K-Prototypes algorithm by feature scaling and behavior of γ value. K-Prototypes algorithm integrates the K-Means and K-Modes algorithms for mixed numeric and categorical attributes. We conducted the experiments on benchmark dataset to demonstrate partitioning clustering algorithm.","","CD:978-1-4673-8853-5; Electronic:978-1-4673-8855-9; POD:978-1-4673-8856-6","10.1109/ICCCI.2017.8117706","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8117706","Feature Scaling;K-Prototypes;Partitional Clustering","Algorithm design and analysis;Benchmark testing;Clustering algorithms;Euclidean distance;Informatics;Machine learning algorithms;Partitioning algorithms","pattern clustering","categorical data;clustering analysis;data explosion;heterogeneous data;k-mode algorithms;k-prototype algorithm;mixed numeric attributes;nominal data;ordinal data;partitioning clustering algorithm","","","","","","","","5-7 Jan. 2017","","IEEE","IEEE Conferences"
"Leveraging large sensor streams for robust cloud control","A. Singh; E. Stephan; T. Elsethagen; M. MacDuff; B. Raju; M. Schram; K. K. van Dam; D. J. Kerbyson; I. Altintas","San Diego Supercomputer Center, UCSD, San Diego, CA","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","2115","2120","Today's dynamic computing deployment for commercial and scientific applications is propelling us to an era where minor inefficiencies can snowball into significant performance and operational bottlenecks. Data center operations is increasingly relying on sensors based control systems for key decision insights. The increased sampling frequencies, cheaper storage costs and prolific deployment of sensors is producing massive volumes of operational data. However, there is a lag between rapid development of analytical techniques and its widespread practical deployment. We present empirical evidence of the potential carried by analytical techniques for operations management in computing and data centers. Using Machine Learning modeling techniques on data from a real instrumented cluster, we demonstrate that predictive modeling on operational sensor data can directly reduce systems operations monitoring costs and improve system reliability.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840839","Cloud Operations Management;Machine Learning;Sensor Data Analytics;Time Series","Correlation;Data mining;Data models;Monitoring;Predictive models;Temperature sensors;Time series analysis","cloud computing;computer centres;data analysis;learning (artificial intelligence);sensor fusion","data analytical techniques;data center operations;dynamic computing deployment;instrumented cluster;large sensor streams;machine learning modeling;predictive operational sensor data modeling;robust cloud control;sampling frequencies;system operation monitoring cost reduction;system reliability","","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conferences"
"Research of Food Safety Event Detection Based on Multiple Data Sources","F. Li; Y. Lv; Q. Zhu; X. Lin","Eng. Res. Center of Intell. Process Syst. Eng., BUCT, Beijing, China","2015 International Conference on Cloud Computing and Big Data (CCBD)","20160409","2015","","","213","216","Online event detection techniques are usually used in single data source. This paper analyzes event detection in the perspective of multiple data sources, combining news reports and microblogs. Detect events from news, combining microblogs to do event monitoring and early warning. Also improve feature selection methods for multiple data sources event detection. Finally, the methods are applied to the detection of food safety events and the results of the research show that event detection with multiple data sources is meaningful and valuable.","","Electronic:978-1-4673-8350-9; POD:978-1-4673-8351-6","10.1109/CCBD.2015.36","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7450554","event detection;food safety;sentiment analysis;single-pass","Algorithm design and analysis;Clustering algorithms;Crawlers;Event detection;Machine learning algorithms;Safety;Sentiment analysis","Web sites;feature selection;food safety;sentiment analysis","early warning;event monitoring;feature selection method improvement;food safety event detection;microblogs;multiple data sources;news reports;online event detection techniques","","","","9","","","","4-6 Nov. 2015","","IEEE","IEEE Conferences"
"Big Data Analytics for User-Activity Analysis and User-Anomaly Detection in Mobile Wireless Network","M. S. Parwez; D. B. Rawat; M. Garuba","Department of Electrical Engineering and Computer Science, Howard University, Washington, DC, USA","IEEE Transactions on Industrial Informatics","20170803","2017","13","4","2058","2065","The next generation wireless networks are expected to operate in fully automated fashion to meet the burgeoning capacity demand and to serve users with superior quality of experience. Mobile wireless networks can leverage spatio-temporal information about user and network condition to embed the system with end-to-end visibility and intelligence. Big data analytics has emerged as a promising approach to unearth meaningful insights and to build artificially intelligent models with assistance of machine learning tools. Utilizing aforementioned tools and techniques, this paper contributes in two ways. First, we utilize mobile network data (Big Data)-call detail record-to analyze anomalous behavior of mobile wireless network. For anomaly detection purposes, we use unsupervised clustering techniques namely k-means clustering and hierarchical clustering. We compare the detected anomalies with ground truth information to verify their correctness. From the comparative analysis, we observe that when the network experiences abruptly high (unusual) traffic demand at any location and time, it identifies that as anomaly. This helps in identifying regions of interest in the network for special action such as resource allocation, fault avoidance solution, etc. Second, we train a neural-network-based prediction model with anomalous and anomaly-free data to highlight the effect of anomalies in data while training/building intelligent models. In this phase, we transform our anomalous data to anomaly-free and we observe that the error in prediction, while training the model with anomaly-free data has largely decreased as compared to the case when the model was trained with anomalous data.","1551-3203;15513203","","10.1109/TII.2017.2650206","10.13039/100000001 - National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7811244","5G;anomaly detection;call detail record (CDR);machine learning;network analytics;network behavior analysis;next generation wireless networks;wireless cellular network","5G mobile communication;Big data;Data models;Mobile computing;Predictive models;Wireless networks","Big Data;mobile computing;neural nets;next generation networks;pattern clustering;security of data;wireless sensor networks","Big Data analytics;anomalous behavior;anomalous data;anomaly-free data;artificially intelligent models;call detail record;hierarchical clustering;k-means clustering;machine learning tools;mobile network data;mobile wireless network;neural-network-based prediction model;next generation wireless networks;spatio-temporal information;unsupervised clustering;user-activity analysis;user-anomaly detection","","1","","","","","20170109","Aug. 2017","","IEEE","IEEE Journals & Magazines"
"Discretizing Numerical Attributes in Decision Tree for Big Data Analysis","Y. Zhang; Y. M. Cheung","Dept. of Comput. Sci., Hong Kong Baptist Univ., Hong Kong, China","2014 IEEE International Conference on Data Mining Workshop","20150129","2014","","","1150","1157","The decision tree induction learning is a typical machine learning approach which has been extensively applied for data mining and knowledge discovery. For numerical data and mixed data, discretization is an essential pre-processing step of decision tree learning. However, when coping with big data, most of the existing discretization approaches will not be quite efficient from the practical viewpoint. Accordingly, we propose a new discretization method based on windowing and hierarchical clustering to improve the performance of conventional decision tree for big data analysis. The proposed method not only provides a faster process of discretizing numerical attributes with the competent classification accuracy, but also reduces the size of the decision tree. Experiments show the efficacy of the proposed method on the real data sets.","2375-9232;23759232","Electronic:978-1-4799-4274-9; POD:978-1-4799-4273-2","10.1109/ICDMW.2014.103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7022725","Big Data;Discretization;Hierarchical Clustering;Noise;Numerical Attribute;Window","Big data;Data mining;Decision trees;Market research;Noise;Noise measurement","Big Data;data mining;decision trees;learning (artificial intelligence);pattern clustering","big data analysis;data mining;decision tree;discretization method;hierarchical clustering;induction learning;knowledge discovery;machine learning;numerical attribute;windowing method","","0","","16","","","","14-14 Dec. 2014","","IEEE","IEEE Conferences"
"A Big Data Analytics Framework for Supporting Multidimensional Mining over Big Healthcare Data","M. Bochicchio; A. Cuzzocrea; L. Vaira","DIE Dept., Univ. of Salento, Lecce, Italy","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","508","513","Nowadays, a great deal of attention is being devoted to big data analytics in complex healthcare environments. Fetal growth curves, which are a classical case of big healthcare data, are used in prenatal medicine to early detect potential fetal growth problems, estimate the perinatal outcome and promptly treat possible complications. However, the currently adopted curves and the related diagnostic techniques have been criticized because of their poor precision. New techniques, based on the idea of customized growth curves, have been proposed in literature. In this perspective, the problem of building customized or personalized fetal growth curves by means of big data techniques is discussed in this paper. The proposed framework introduces the idea of summarizing the massive amounts of (input) big data via multidimensional views on top of which well-known Data Mining methods like clustering and classification are applied. This overall defines a multidimensional mining approach, targeted to complex healthcare environments. A preliminary analysis on the effectiveness of the framework is also proposed.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838194","Mining Big Data; Big Healthcare Data; Healthcare Systems","Big data;Clustering algorithms;Data mining;Machine learning algorithms;Medical diagnostic imaging;Medical services;Production","Big Data;data mining;health care;medical information systems;obstetrics;pattern classification;pattern clustering","Big Data analytics framework;Big healthcare Data;complex healthcare environments;data classification;fetal growth curves;multidimensional mining;multidimensional views;pattern clustering;prenatal medicine","","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conferences"
"Understanding the behavior of in-memory computing workloads","T. Jiang; Q. Zhang; R. Hou; L. Chai; S. A. Mckee; Z. Jia; N. Sun","SKL Computer Architecture, ICT, CAS, Beijing, China","2014 IEEE International Symposium on Workload Characterization (IISWC)","20141215","2014","","","22","30","The increasing demands of big data applications have led researchers and practitioners to turn to in-memory computing to speed processing. For instance, the Apache Spark framework stores intermediate results in memory to deliver good performance on iterative machine learning and interactive data analysis tasks. To the best of our knowledge, though, little work has been done to understand Spark's architectural and microarchitectural behaviors. Furthermore, although conventional commodity processors have been well optimized for traditional desktops and HPC, their effectiveness for Spark workloads remains to be studied. To shed some light on the effectiveness of conventional generalpurpose processors on Spark workloads, we study their behavior in comparison to those of Hadoop, CloudSuite, SPEC CPU2006, TPC-C, and DesktopCloud. We evaluate the benchmarks on a 17-node Xeon cluster. Our performance results reveal that Spark workloads have significantly different characteristics from Hadoop and traditional HPC benchmarks. At the system level, Spark workloads have good memory bandwidth utilization (up to 50%), stable memory accesses, and high disk IO request frequency (200 per second). At the microarchitectural level, the cache and TLB are effective for Spark workloads, but the L2 cache miss rate is high. We hope this work yields insights for chip and datacenter system designers.","","Electronic:978-1-4799-6454-3; POD:978-1-4799-6455-0; USB:978-1-4799-6453-6","10.1109/IISWC.2014.6983036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983036","","Bandwidth;Benchmark testing;Big data;Hardware;Microarchitecture;Program processors;Sparks","Big Data;parallel processing;storage management","17-node Xeon cluster;Apache Spark framework;HPC benchmark;Hadoop;big data application;in-memory computing workload;interactive data analysis;iterative machine learning;memory access;memory bandwidth utilization;microarchitectural behavior","","12","","30","","","","26-28 Oct. 2014","","IEEE","IEEE Conferences"
"Return of experience on the mean-shift clustering for heterogeneous architecture use case","C. Cérin; J. L. Gaudiot; M. Lebbah; F. Yuehgoh","Universit&#x00E9; de Paris 13, LIPN, 99, avenue Jean-Baptiste Cl&#x00E9;ment, 93430 Villetaneuse, France","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","3499","3507","The exponential increment in data size poses new challenges for computer scientists, giving rise to a new set of methodologies under the term Big Data. Many efficient algorithms for machine learning have been proposed, facing up time and memory requirements. Nevertheless, with hardware acceleration, multiple software instructions can be integrated and executed into a single hardware die. Current researches aim at eliminating the burden for the user in using multiple processor types. In this paper we propose our return of experience on a new way of implementing machine learning algorithms on heterogeneous hardware. To explore our vision, we use a parallel Mean-shift algorithm, developed at LIPN as our case study to investigate issues in building efficient Machine Learning libraries for heterogeneous systems. The ultimate goal is to provide a core set of building blocks for Machine Learning programming that could serve either to build new applications on heterogeneous architectures or to control the evolution of the underlying platform. We thus examine the difficulties encountered during the implementation of the algorithm with the aim to discover methodologies for building systems based on heterogeneous hardware. We also discover issues and building blocks for solving concrete machine learning (ML) problems on the Chisel software stack we use for this purpose.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258339","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258339","Constructing Hardware in Scala Embedded Language (Chisel);Field Programmable Gate Array (FPGA);Heterogeneous Architecture;Machine Learning","Algorithm design and analysis;Field programmable gate arrays;Hardware;Machine learning algorithms;Memory management;Software algorithms","Big Data;learning (artificial intelligence);parallel processing;pattern clustering","Big Data;Chisel software stack;Machine Learning libraries;Machine Learning programming;building blocks;computer scientists;concrete machine learning problems;data size;hardware acceleration;heterogeneous architecture;heterogeneous architectures;heterogeneous hardware;heterogeneous systems;mean-shift clustering;memory requirements;multiple processor types;parallel Mean-shift algorithm;software instructions","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"From social media to public health surveillance: Word embedding based clustering method for twitter classification","X. Dai; M. Bikdash; B. Meyer","Department of Computational Science and Engineering, North Carolina A&T State University, Greensboro, USA","SoutheastCon 2017","20170511","2017","","","1","7","Social media provide a low-cost alternative source for public health surveillance and health-related classification plays an important role to identify useful information. In this paper, we summarized the recent classification methods using social media in public health. These methods rely on bag-of-words (BOW) model and have difficulty grasping the semantic meaning of texts. Unlike these methods, we present a word embedding based clustering method. Word embedding is one of the strongest trends in Natural Language Processing (NLP) at this moment. It learns the optimal vectors from surrounding words and the vectors can represent the semantic information of words. A tweet can be represented as a few vectors and divided into clusters of similar words. According to similarity measures of all the clusters, the tweet can then be classified as related or unrelated to a topic (e.g., influenza). Our simulations show a good performance and the best accuracy achieved was 87.1%. Moreover, the proposed method is unsupervised. It does not require labor to label training data and can be readily extended to other classification problems or other diseases.","","Electronic:978-1-5386-1539-3; POD:978-1-5386-1540-9","10.1109/SECON.2017.7925400","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7925400","Big data;Clustering Process;Machine learning;Natural Language Processing;Public Health;Similarity Measure;Social Network;Surveillance;Twitter;Unsupervised Classification;Word Embeddings;Word2Vec","Diseases;Natural language processing;Public healthcare;Support vector machines;Surveillance;Twitter","diseases;health care;learning (artificial intelligence);medical computing;natural language processing;pattern classification;pattern clustering;social networking (online);vectors;word processing","BOW model;NLP;Twitter classification;bag-of-words;diseases;health-related classification;natural language processing;optimal vector learning;public health surveillance;social media;text semantic meaning;word clusters;word embedding based clustering method","","","","","","","","March 30 2017-April 2 2017","","IEEE","IEEE Conferences"
"Big social data analytics for public health: Facebook engagement and performance","N. Straton; K. Hansen; R. R. Mukkamala; A. Hussain; T. M. Gronli; H. Langberg; R. Vatrapu","Centre for Business Data Analytics, Dept. of IT Management, Copenhagen Business School, Denmark","2016 IEEE 18th International Conference on e-Health Networking, Applications and Services (Healthcom)","20161121","2016","","","1","6","In recent years, social media has offered new opportunities for interaction and distribution of public health information within and across organisations. In this paper, we analysed data from Facebook walls of 153 public organisations using unsupervised machine learning techniques to understand the characteristics of user engagement and post performance. Our analysis indicates an increasing trend of user engagement on public health posts during recent years. Based on the clustering results, our analysis shows that Photo and Link type posts are most favourable for high and medium user engagement respectively.","","Electronic:978-1-5090-3370-6; POD:978-1-5090-3371-3","10.1109/HealthCom.2016.7749497","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7749497","","Clustering algorithms;Correlation;Diseases;Facebook;Public healthcare","Big Data;data analysis;health care;organisational aspects;pattern clustering;social networking (online);unsupervised learning","Big social data analytics;Facebook engagement;Facebook walls;clustering;link type posts;photo posts;postperformance;public health information distribution;public health information interaction;public health posts;public organisations;social media;unsupervised machine learning techniques;user engagement","","2","","","","","","14-16 Sept. 2016","","IEEE","IEEE Conferences"
"The feasibility of deep learning algorithms integration on a GPU-based ultrasound research scanner","P. Jarosik; M. Lewandowski","Laboratory of Professional Electronics, Institute of Fundamental Technological Research PAS, Warsaw, Poland","2017 IEEE International Ultrasonics Symposium (IUS)","20171102","2017","","","1","4","Ultrasound medical diagnostics is a real-time modality based on a doctor's interpretation of images. So far, automated Computer-Aided Diagnostic tools were not widely applied to ultrasound imaging. The emerging methods in Artificial Intelligence, namely deep learning, gave rise to new applications in medical imaging modalities. The work's objective was to show the feasibility of implementing deep learning algorithms directly on a research scanner with GPU software beamforming. We have implemented and evaluated two deep neural network architectures as part of the signal processing pipeline on the ultrasound research platform USPlatform (us4us Ltd., Poland). The USPlatform is equipped with a GPU cluster, enabling full software-based channel data processing as well as the integration of open source Deep Learning frameworks. The first neural model (S-4-2) is a classical convolutional network for one-class classification of baby body parts. We propose a simple 6-layer network for this task. The model was trained and evaluated on a dataset consisting of 786 ultrasound images of a fetal training phantom. The second model (Gu-net) is a fully convolutional neural network for brachial plexus localisation. The model uses `U-net'-like architecture to compute the overall probability of target detection and the probability mask of possible target locations. The model was trained and evaluated on 5640 ultrasound B-mode frames. Both training and inference were performed on a multi-GPU (Nvidia Titan X) cluster integrated with the platform. As performance metrics we used: accuracy as a percentage of correct answers in classification, dice coefficient for object detection, and mean and std. dev. of a model's response time. The `S-4-2' model achieved 96% classification accuracy and a response time of 3 ms (334 predictions/s). This simple model makes accurate predictions in a short time. The `Gu-net' model achieved a 0.64 dice coefficient for object detection and a 76% target's - resence classification accuracy with a response time of 15 ms (65 predictions/s). The brachial plexus detection task is more challenging and requires more effort to find the right solution. The results show that deep learning methods can be successfully applied to ultrasound image analysis and integrated on a single advanced research platform.","","Electronic:978-1-5386-3383-0; POD:978-1-5386-3384-7; USB:978-1-5386-3382-3","10.1109/ULTSYM.2017.8091750","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8091750","","Convolution;Image segmentation;Kernel;Machine learning;Neural networks;Ultrasonic imaging","array signal processing;biomedical ultrasonics;cellular biophysics;graphics processing units;image classification;learning (artificial intelligence);medical image processing;neural nets;object detection;obstetrics;phantoms;physiological models;probability","GPU cluster;GPU software beamforming;GPU-based ultrasound research scanner;Gu-net model;artificial intelligence;automated computer-aided diagnostic tools;baby body parts;brachial plexus detection task;brachial plexus localisation;classical convolutional network;classification accuracy;convolutional neural network;deep learning algorithms integration;deep neural network architectures;fetal training phantom;neural model;object detection;one-class classification;probability mask;software-based channel data processing;ultrasound B-mode frames;ultrasound image analysis;ultrasound imaging;ultrasound medical diagnostics","","","","","","","","6-9 Sept. 2017","","IEEE","IEEE Conferences"
"Anticipation and alert system of congestion and accidents in VANET using Big Data analysis for Intelligent Transportation Systems","H. A. Najada; I. Mahgoub","Florida Atlantic University, 777 Glades Road, Boca Raton, 33431 USA","2016 IEEE Symposium Series on Computational Intelligence (SSCI)","20170213","2016","","","1","8","Vehicular Networks (VN) have a huge potential to increase roadway safety and traffic efficiency. Big Data analysis can be instrumental in realizing this potential and enhancing the Intelligent Transportation Systems (ITSs). We study the causes of road accidents using big real-time accidents data obtained from Florida Department of Transportation (FDOT) - District 4. The ultimate goal is to prevent or at least decrease traffic accidents and congestions. Our approach is based on dividing the roadway into segments, based on the infrastructure availability and the secondary accidents factors. We design a real-time Big Data system that receives online streamed data from vehicles on the road in addition to real-time average speed data from vehicles detectors on the road side to (1) Provide accurate Estimated Time of Arrival (ETA) using a Linear Regression (LR) model (2) Predict accidents and congestions before they happen using Naive Bayes (NB) and Distributed Random Forest (DRF) classifiers (3) Update ETA if an accident or a congestion takes place by predicting accurate clearance time. To make this system fast, accurate, and reliable we have implemented Lambda Architecture (LA) in our framework because of its speed, scalability, and fault tolerance. Furthermore, we have optimized the efficiency, the speed, and the accuracy of the designed model by securely selecting the most relevant and significant set of features required for the analysis.","","Electronic:978-1-5090-4240-1; POD:978-1-5090-4241-8","10.1109/SSCI.2016.7850097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7850097","Big Data;Intelligent Transportation Systems (ITSs);Lambda Architecture (LA);Machine Learning and Data Mining;Traffic Congestions Prediction;Traffic Crashes Prediction;Vehicular Ad-hoc Network (VANET)","Accidents;Big data;Computer architecture;Data models;Real-time systems;Roads;Vehicular ad hoc networks","Bayes methods;Big Data;data analysis;intelligent transportation systems;pattern classification;regression analysis;road accidents;road safety;road traffic;vehicular ad hoc networks","DRF classifiers;ETA;ITSs;LR model;NB;VANET;accident prediction;alert system;big data analysis;congestion prediction;distributed random forest classifiers;estimated time of arrival;fault tolerance;infrastructure availability;intelligent transportation systems;lambda architecture;linear regression model;naive Bayes;real-time Big Data system;road accidents;roadway safety;roadway traffic efficiency;traffic accidents;traffic congestions;vehicular networks","","1","","","","","","6-9 Dec. 2016","","IEEE","IEEE Conferences"
"Sanzu: A data science benchmark","A. Watson; D. S. V. Babu; S. Ray","Faculty of Computer Science, University of New Brunswick, Fredericton, Canada","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","263","272","The volume of data that is generated each day is rising rapidly. There is a need to analyze this data efficiently and produce results quickly. Data science offers a formal methodology for processing and analyzing data. It involves a work-flow with multiple stages, such as, data collection, data wrangling, statistical analysis and machine learning. In this paper, we look at data analytics systems that support the data science work-flow. The variety of current commercial and open-source data analytics systems differ significantly in terms of available features, functionality, and scalability. A benchmark can be used to evaluate the functionality and performance of a system. However, there is no standard benchmark for evaluating or comparing these data systems for doing data science. In this paper, we introduce a data science benchmark, Sanzu, to evaluate systems with data processing and analytics tasks. Our benchmark includes a micro and macro benchmark. The micro benchmark tests basic operations in isolation. It consists of task suites for reading and writing, data wrangling, statistical analysis, machine learning and time series analysis. Each macro workload evaluates an analytics application where a series of analysis or functions are based on a real world application. The macro benchmark focuses on sports and smart grid analytics. We evaluate these tasks on five different popular data science frameworks and systems: R, Anaconda Python, Dask, PostgreSQL (MADlib) and PySpark. For micro benchmark we generate synthetic datasets with 3 scale factors: 1, 10 and 100 (scale factor 1=1 million). The macro benchmark uses data generated from real-world data sources.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8257934","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8257934","","Benchmark testing;Big Data;Data models;Data science;Databases;Statistical analysis","benchmark testing;data analysis;learning (artificial intelligence);smart power grids;sport;statistical analysis;time series","Anaconda Python;Dask;MADlib;PostgreSQL;PySpark;R system;Sanzu;data collection;data processing;data science benchmark;data science work-flow;data systems;data wrangling;machine learning;macro benchmark;microbenchmark tests basic operations;open-source data analytics systems;real-world data sources;smart grid analytics;sports;statistical analysis;time series analysis","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"A novel density peak based semi-supervised clustering algorithm","W. F. Li; X. T. Li; Y. M. Ye; Y. Li; E. K. Wang","Department of Computer Science, Shenzhen Graduate School, Harbin Institute of Technology, 518055, China","2016 International Conference on Machine Learning and Cybernetics (ICMLC)","20170223","2016","1","","307","310","With the rapid development of technology, acquiring and storing big data from various fields is no longer a problem. Instead, how to utilize the data becomes an important and hot research topic. Clustering is one of the important tasks for big data utility. However, there exists one well-known challenge for the task, i.e. it is difficult to incorporate prior information into the clustering results. In this paper, we proposed a density peak based semi-supervised clustering algorithm, which is able to leverage label information of some seed objects for obtaining a better clustering result. Specifically, we first adopted a density based clustering algorithm to identify density peaks as the possible cluster centers for a dataset, and then proposed a graph-based algorithm to assign each center a class label by utilizing some given seed objects. Finally, we leveraged the label information of seed objects and identified centers to generate must-link and cannot-link constraints for clustering. Extensive experiments have been conducted on various publicly available data sets to verify the effectiveness of the proposed method, and the results showed that the proposed density-peak based semi-supervised algorithm outperforms the existing methods substantially.","","CD:978-1-5090-0388-4; Electronic:978-1-5090-0390-7; POD:978-1-5090-0391-4","10.1109/ICMLC.2016.7860919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7860919","Density peaks algorithm;Dijsktra algorithm;Seed object;Semi-supervised model","Clustering algorithms;Cybernetics;Fires;Heart;Machine learning algorithms;Measurement;Object recognition","Big Data;data mining;graph theory;learning (artificial intelligence);pattern clustering","Big Data utility;density peak;graph based algorithm;seed objects;semisupervised clustering algorithm","","","","","","","","10-13 July 2016","","IEEE","IEEE Conferences"
"Vehicular dataset for road assessment conditions","M. Antunes; D. Gomes; J. P. Barraca; R. L. Aguiar","Instituto de Telecomunica&#x00E7;&#x00F5;es Universidade de Aveiro Aveiro, Portugal","2017 International Smart Cities Conference (ISC2)","20171102","2017","","","1","4","The Internet of Things (IoT) is a very promising concept that by connecting numerous devices to the internet and extracting large sums of information (BigData) can enable the realisation of various futuristic scenarios. In order to develop and assess future applications and services, it is necessary the availability of datasets that can be used to train, test and cross validate. Project SCoT (Smart Cloud of Things) has developed an M2M platform capable of collecting information from heterogeneous devices and collide that information in a large data repository. During its pilot phase, the project made the assessment of the road conditions in the region of Aveiro, Portugal. In this work we make the dataset used on the previous mentioned pilot publicly available. With this dataset our road assessment algorithm reached 80% accuracy in the task of pothole detection, other scenarios (that take into account vehicular speed, position and acceleration) can also be explored. The dataset was not pre-processed in anyway, the only transformation was made to protect the identity of the volunteers.","","Electronic:978-1-5386-2524-8; POD:978-1-5386-2525-5; USB:978-1-5386-2523-1","10.1109/ISC2.2017.8090867","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8090867","Dataset;IoT;M2M;Machine Learning","Acceleration;Clustering algorithms;Global Positioning System;Machine-to-machine communications;Monitoring;Roads;Sensors","Internet of Things;cloud computing;road safety;telecommunication computing;telecommunication traffic;vehicular ad hoc networks","Big Data;IoT;M2M platform;Portugal;data repository;heterogeneous devices;pilot phase;pothole detection;project SCoT;road assessment algorithm;road assessment conditions;road conditions;smart Cloud of Things;vehicular dataset;vehicular speed","","","","","","","","14-17 Sept. 2017","","IEEE","IEEE Conferences"
"Optimization of Stacked Unsupervised Extreme Learning Machine to improve classifier performance","D. M. S. Arsa; M. A. Ma'sum; M. F. Rachmadi; W. Jatmiko","Faculty of Computer Science, Universitas Indonesia, Indonesia","2017 International Workshop on Big Data and Information Security (IWBIS)","20180201","2017","","","63","68","In the era of Big Data, data size and data security are issues that need to be solved. To address this problem, we may apply data compression technique or data encryption. On the other hand, the coding based method may be other solution. The coding based method may learn data distribution and reduce data dimension with minimized the loss of information from the original data. Stacked Unsupervised Extreme Learning Machine (Stacked US-ELM) is one of the fastest methods which can be used to address it. The problem is how many stacks we need to get the optimal performance of the classifier. In this research, we inspected the performance of Stacked Unsupervised Extreme Learning Machine (US-ELM) for enhancing classifier performance and proposed a new method. The proposed method is a loop scheme of Stacked US-ELM to optimize the number of stacks of US-ELM. We conducted the experiment using ECG-sleep dataset, synthetic dataset, and Glass dataset. To measure the performance of Stacked US-ELM and the proposed method, we conducted classification according to our data sets using Support Vector Machine (SVM). The 5-Folds Cross Validation is used to evaluate the performance. We compared the result of the classification without US-ELM with the fix Stacked US-ELM and the proposed method. The results showed the proposed method achieved the best performance over Stacked US-ELM in all dataset. The mean of accuracies of the proposed method are 64,39%, 76,24%, 63,22%, and 69, 61% on 4 class ECG-sleep dataset, 3 class ECG-sleep dataset, skewed synthetic dataset, and glass dataset.","","Electronic:978-1-5386-2038-0; POD:978-1-5386-2039-7; USB:978-1-5386-2037-3","10.1109/IWBIS.2017.8275104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8275104","","Big Data;Encoding;Machine learning;Support vector machines;Task analysis;Training;Training data","data compression;pattern classification;pattern clustering;regression analysis;support vector machines;unsupervised learning","Big Data;Stacked US-ELM;Stacked Unsupervised Extreme Learning Machine;coding based method;data compression;data dimension;data distribution;data security;data size","","","","","","","","23-24 Sept. 2017","","IEEE","IEEE Conferences"
"Feature Selection and Improving Classification Performance for Malware Detection","C. Cepeda; D. L. C. Tien; P. Ordóñez","Dept. of Comput. Sci., Kennesaw State Univ., Kennesaw, GA, USA","2016 IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking (SocialCom), Sustainable Computing and Communications (SustainCom) (BDCloud-SocialCom-SustainCom)","20161031","2016","","","560","566","After analyzing the advance of technology, it is clear that use of the Internet, computers, smart phones and tablets has become ubiquitous and therefore, the creation and proliferation of cyber threats and attacks has grown exponentially. Consequently, Anti-Virus companies and researchers have developed new approaches for dealing with discovering and classifying malware. Among these, machine learning and Big Data technologies have been used for feature extraction, detection, and clustering of cyber threats. In this paper, we created and analyzed a dataset of malware and clean files (goodware) from the static and dynamic features provided by the online framework VirusTotal. The purpose is to select the smallest number of features that keep the classification accuracy as high as possible given that the training execution time increase in polynomial time with respect to the number of features. In this research, we found that ""9"" features are enough to distinguish malware from ""goodware"" files within an accuracy of 99.60%. Selecting the most representative features for malware detection relies on the possibility of creating an embedded program that monitors the processes executed by the OS looking for the characteristics that match malware behavior. Thus, feature selection was made taking the most important features that keep the accuracy high and allows the creation of monitoring malware detection programs with a low overhead cost. In addition, classification algorithms such as Random Forest (RF), Support Vector Machine (SVM) and Neural Networks (NN) were used in a novel combination that not only showed an increase in accuracy, but also in the training speed from hours to just minutes.","","","10.1109/BDCloud-SocialCom-SustainCom.2016.87","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7723741","Big Data Analytics;Feature selection;Machine Learning;Malware detection;Neural Networks;Random Forest;Support Vector Machine;improving accuracy","Big data;Cloud computing;Conferences;Social computing","feature selection;invasive software;learning (artificial intelligence);neural nets;pattern classification;support vector machines","Big Data technologies;NN;RF;SVM;VirusTotal framework;classification performance;cyber attacks;cyber threats;feature clustering;feature detection;feature extraction;feature selection;machine learning;malware classification;malware detection;malware discovery;neural networks;random forest;support vector machine","","","","","","","","8-10 Oct. 2016","","IEEE","IEEE Conferences"
"A collaborative filtering based approach to performance prediction for parallel applications","Q. Shao; L. Pan; S. Liu; X. Liu","School of Computer Science and Technology, Shandong University Jinan 250101, China","2017 IEEE 21st International Conference on Computer Supported Cooperative Work in Design (CSCWD)","20171016","2017","","","331","336","Parallel application jobs account for a large population in current domain of cloud computing and Big Data processing services, whose execution time can be varied greatly with different runtime configurations. For efficiently scheduling resources and services to run parallel jobs, the ability to quickly and accurately estimate the performance of parallel applications is critical. Analytic predictive models based on traditional modeling techniques such as queuing systems are difficult to construct for parallel applications, due to the high complexity lying in the structures of parallel application models. Furthermore, due to the heterogeneity of resources computing capacities with a scalable computing environment such as a cloud computing platform, performance analytic and prediction becomes increasingly difficult for parallel applications. To address this problem, in this paper we propose a collaborative filtering based approach to quickly and accurately predict the execution time of parallel applications running in heterogenous resources. Particularly, we use the widely used Apache Spark platform as the running framework for parallel applications, and propose a bounds-based performance model to improve the prediction accuracy. Through extensive simulations and experiments on real Spark clusters and two large-scale machine learning applications as well as the simple but classic WordCount sample application, we show that the proposed Collaborative Filtering based approach and bounds-based performance model can accurately estimate the performance of parallel applications.","","Electronic:978-1-5090-6199-0; POD:978-1-5090-6200-3; USB:978-1-5090-6198-3","10.1109/CSCWD.2017.8066716","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8066716","Big Data;Spark;collaborative filtering;parallel application;performance prediction","Cloud computing;Collaboration;Computational modeling;Predictive models;Sparks;Time factors","Big Data;cloud computing;collaborative filtering;learning (artificial intelligence);parallel programming;resource allocation","Apache Spark platform;Big Data processing services;cloud computing;collaborative filtering based approach;large-scale machine learning applications;parallel application jobs;performance model","","","","","","","","26-28 April 2017","","IEEE","IEEE Conferences"
"Power Efficient MapReduce Workload Acceleration Using Integrated-GPU","S. Kim; J. Bottleson; J. Jin; P. Bindu; S. C. Sakhare; J. S. Spisak","Intel Corp., Folsom, CA, USA","2015 IEEE First International Conference on Big Data Computing Service and Applications","20150813","2015","","","162","169","With the pervasiveness of MapReduce - one of the most prominent programming models for data parallelism in Apache Hadoop-, many researchers and developers have spent tremendous effort attempting to boost the computational speed and energy efficiency of MapReduce-based big data processing. However, the scalable and fault-tolerant nature of MapReduce introduces additional costs in disk IO and data transfer, caused by streaming intermediate outputs to disk. In light of these issues, many interesting research projects have been initiated with the goal of improving the compute speed and power efficiency of compute-intensive cloud computing workloads, several with the addition of discrete GPUs. In this work, we present a modified MapReduce approach focused on the iterative clustering algorithms in the Apache Mahout machine learning library that leverage the acceleration potential of the Intel integrated GPU in a multi-node cluster environment. The accelerated framework shows varying levels of speed-up (≈45x for Map tasks-only, ≈4.37x for the entire K-means clustering) as evaluated using the HiBench benchmark suite. Based on various experiments and in-depth analysis, we find that utilizing the integrated GPU via OpenCL offers significant performance and power efficiency gains over the original CPU based approach. Further analysis is also done to understand the correlations between compute, IO and power efficiency. As such, our results show that embracing the integrated GPU in the Hadoop MapReduce framework represents a promising advance in adding cost and energy efficient compute parallelism to a data parallel multinode environment.","","Electronic:978-1-4799-8128-1; POD:978-1-4799-8129-8","10.1109/BigDataService.2015.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7184877","Big Data;GPGPU;Hadoop;Integrated Graphics;Machine Learning;Mahout;OpenCL","Acceleration;Graphics processing units;Java;Kernel;Optimization;Performance gain;Power demand","Big Data;graphics processing units;learning (artificial intelligence);parallel processing;pattern clustering;power aware computing","Apache Hadoop;Apache Mahout machine learning library;Hadoop MapReduce framework;HiBench benchmark suite;Intel integrated GPU;MapReduce-based big data processing;OpenCL;acceleration potential;computational speed;data parallel multinode environment;data parallelism;data transfer;discrete GPU;disk IO;energy efficiency;energy efficient compute parallelism;fault-tolerant nature;iterative clustering algorithms;multinode cluster environment;power efficient MapReduce workload acceleration;programming models","","0","","33","","","","March 30 2015-April 2 2015","","IEEE","IEEE Conferences"
"Big data processing: Is there a framework suitable for economists and statisticians?","G. Bruno; D. Condello; A. Falzone; A. Luciani","Bank of Italy, Economics and statistics Directorate, Rome, Italy","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","4204","4211","The emerging wave of Big Data applications is flooding all branches of scientific knowledge. Economic and statistical applied research carried out in central banks and policy advising institutions is no exception. In this paper we present one of the most promising platform providing a unifying framework for different researchers willing to harness their knowledge of popular and simple computing environment such as R and Python. Along with their Integrated Development Environment (IDE), these are two of the most used numerical computing framework which are open source, provide built-in capabilities for statistical analysis and include a wide array of user contributed packages for an ample set of analytical tools suitable for different scientific applications. In the Big Data framework, we show how to provide researchers with a suitable programming environment allowing them to tame the intrinsic complexity of a High Performance Computing Cluster. Here we provide few empirical applications based on classical econometric and machine learning modeling.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258446","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258446","Apache Spark;Cluster;Distributed Memory;Resource sharing","Big Data;Econometrics;Processor scheduling;Production;Software;Sparks;Tools","Big Data;learning (artificial intelligence);parallel processing;programming environments;statistical analysis","Big Data framework;Big data processing;central banks;high-performance computing cluster;integrated development environment;policy advising institutions;scientific knowledge;simple computing environment;statistical analysis;suitable programming environment","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Data partitioning strategies for graph workloads on heterogeneous clusters","M. LeBeane; S. Song; R. Panda; J. H. Ryoo; L. K. John","Dept. of Electr. & Comput. Eng., Univ. of Texas at AustinAustin, Austin, TX, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis","20170126","2015","","","1","12","Large scale graph analytics are an important class of problem in the modern data center. However, while data centers are trending towards a large number of heterogeneous processing nodes, graph analytics frameworks still operate under the assumption of uniform compute resources. In this paper, we develop heterogeneity-aware data ingress strategies for graph analytics workloads using the popular PowerGraph framework. We illustrate how simple estimates of relative node computational throughput can guide heterogeneity-aware data partitioning algorithms to provide balanced graph cutting decisions. Our work enhances five online data ingress strategies from a variety of sources to optimize application execution for throughput differences in heterogeneous data centers. The proposed partitioning algorithms improve the runtime of several popular machine learning and data mining applications by as much as a 65% and on average by 32% as compared to the default, balanced partitioning approaches.","","Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3","10.1145/2807591.2807632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832830","","Big data;Clustering algorithms;Computational modeling;Engines;Partitioning algorithms;Synchronization;Throughput","Big Data;cloud computing;computer centres;distributed programming;graph theory;network analysis;resource allocation","Big Data computing;PowerGraph framework;cloud computing;data partitioning strategies;distributed programming languages;graph analytics workloads;graph cutting decisions;heterogeneity-aware data partitioning algorithms;heterogeneous clusters;large scale graph analytics;load balancing;modern data center","","2","","","","","","15-20 Nov. 2015","","IEEE","IEEE Conferences"
"Air Quality Simulations Using Big Data Programming Models","H. Ayyalasomayajula; E. Gabriel; P. Lindner; D. Price","Dept. of Comput. Sci., Univ. of Houston, Houston, TX, USA","2016 IEEE Second International Conference on Big Data Computing Service and Applications (BigDataService)","20160523","2016","","","182","184","Forecasts of daily pollutant levels have become a standard part of weather predictions in television, on-line, and in newspapers. Research groups also need to analyze larger timeframes across more locations to correlate long term developments for different pollutants with multiple serious health effects such as asthma. This paper presents a comparison of the Hadoop MapReduce and Spark programing models for air quality simulations, guiding future code development for the research groups interested in these analyses. Two use cases have been used, namely (i) calculating the eight hour rolling average of pollutants in a restricted region, (ii) identifying clusters of sensors showing similar patterns in pollutant concentration over multiple years in the state of Texas. The data set used in this analysis is air pollution data collected over fifteen years at 179 monitor sites across the state of Texas for a variety of pollutants. Our results reveal 20-25% performance benefits for the Spark solutions over MapReduce. Furthermore, it documents performance benefits of the Spark MLlib machine learning library over the Mahout library which is based on the MapReduce programing model.","","Electronic:978-1-5090-2251-9; POD:978-1-5090-2252-6","10.1109/BigDataService.2016.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7474371","Air Quality Simulations;MapReduce;Spark","Air quality;Analytical models;Atmospheric modeling;Computational modeling;Data models;Sensors;Sparks","Big Data;air pollution;air quality;geophysics computing;learning (artificial intelligence);parallel programming;sensors;software libraries;weather forecasting","Big Data programming models;Hadoop MapReduce programming model;Spark MLlib machine learning library;Spark programming model;Texas;air pollution data;air quality simulations;code development;daily-pollutant level forecasting;pollutant concentration;pollutant rolling average;sensor cluster identification;weather predictions","","","","7","","","","March 29 2016-April 1 2016","","IEEE","IEEE Conferences"
"Towards Data-Driven Autonomics in Data Centers","A. Sîrbu; O. Babaoglu","Dept. of Comput. Sci. & Eng., Univ. of Bologna, Bologna, Italy","2015 International Conference on Cloud and Autonomic Computing","20151029","2015","","","45","56","Continued reliance on human operators for managing data centers is a major impediment for them from ever reaching extreme dimensions. Large computer systems in general, and data centers in particular, will ultimately be managed using predictive computational and executable models obtained through data-science tools, and at that point, the intervention of humans will be limited to setting high-level goals and policies rather than performing low-level operations. Data-driven autonomics, where management and control are based on holistic predictive models that are built and updated using generated data, opens one possible path towards limiting the role of operators in data centers. In this paper, we present a data-science study of a public Google dataset collected in a 12K-node cluster with the goal of building and evaluating a predictive model for node failures. We use BigQuery, the big data SQL platform from the Google Cloud suite, to process massive amounts of data and generate a rich feature set characterizing machine state over time. We describe how an ensemble classifier can be built out of many Random Forest classifiers each trained on these features, to predict if machines will fail in a future 24-hour window. Our evaluation reveals that if we limit false positive rates to 5%, we can achieve true positive rates between 27% and 88% with precision varying between 50% and 72%. We discuss the practicality of including our predictive model as the central component of a data-driven autonomic manager and operating it on-line with live data streams (rather than off-line on data logs). All of the scripts used for BigQuery and classification analyses are publicly available from the authors' website.","","Electronic:978-1-4673-9566-3; POD:978-1-4673-9567-0","10.1109/ICCAC.2015.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7312140","BigQuery;Data science;Google cluster trace;ensemble classifier;failure prediction;log data analysis;machine learning classification;predictive analytics;random forest","Correlation;Google;Predictive models;Radio frequency;Testing;Training;Training data","SQL;computer centres;fault tolerant computing;pattern classification;query processing;random processes","BigQuery;Google Cloud suite;autonomic manager;big data SQL platform;central component;classification analysis;computer system;data center;data log;data stream;data-driven autonomics;data-science study;data-science tool;holistic predictive model;human operator;low-level operation;node failure;predictive computational model;public Google dataset;random forest classifier","","4","","33","","","","21-25 Sept. 2015","","IEEE","IEEE Conferences"
"A Generative Model for Sparse Hyperparameter Determination","Z. Wan; H. He; B. Tang","Department of Electrical, Computer and Biomedical Engineering, University of Rhode Island, Kingston, RI","IEEE Transactions on Big Data","20180227","2018","4","1","2","10","Sparse autoencoder is an unsupervised feature extractor and has been widely used in the machine learning and data mining community. However, a sparse hyperparameter has to be determined to balance the trade-off between the reconstruction error and the sparsity of sparse autoencoder. Traditional sparse hyperparameter determination method is time-consuming, especially when the dataset is large. In this paper, we derive a generative model for sparse autoencoder. Based on this model, we derive a formulation to determine the sparse hyperparameter effectively and efficiently. The relationship between the sparse hyperparameter and the average activation of sparse autoencoder hidden units is also presented in this paper. Experimental results and comparative studies over numerous datasets demonstrate the effectiveness of our method to determine the sparse hyperparameter.","","","10.1109/TBDATA.2017.2689790","US National Science Foundation (NSF); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890419","Sparse autoencoder;feature extractor;generative model;sparse hyperparameter","Big Data;Data mining;Feature extraction;Image reconstruction;Probabilistic logic;Shape;Supervised learning","data mining;feature extraction;pattern clustering;unsupervised learning","data mining community;generative model;machine learning;sparse autoencoder hidden units;traditional sparse hyperparameter determination method","","","","","","","20170330","March 1 2018","","IEEE","IEEE Journals & Magazines"
"An Empirical Investigation of Mobile Network Traffic Data for Resource Management","M. Si; C. H. Lung; S. Ajila; W. Ding","Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, ON, Canada","2016 IEEE International Congress on Big Data (BigData Congress)","20161006","2016","","","291","298","Since the emergence of mobile networks, the number of mobile subscriptions has continued to increase year after year. To efficiently assign mobile network resources such as spectrum (which is expensive), the network operator needs to process and analyze information and statistics about each base station and the traffic that passes through it. This paper presents an application of data analytics by focusing on processing and analyzing two datasets from a commercial trial mobile network. A detailed description that uses Apache Hadoop and the Mahout machine learning library to process and analyze the datasets is presented. The analysis provides insights about the resource usage of network devices. This information is of great importance to network operators for efficient and effective management of resources and for supporting high-quality of user experience. Furthermore, an investigation has been conducted that evaluates the impact of executing the Mahout clustering algorithms with various system and workload parameters on a Hadoop cluster. The results demonstrate the value of performance data analysis. Specifically, the execution time can be significantly reduced using data pre-processing and some machine learning techniques, and Hadoop. The investigation provides useful information for the network operators for future real-time data analytics.","","","10.1109/BigDataCongress.2016.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7584950","Hadoop;Mahout;Mobile network traffic;Principal Component Analysis (PCA);clustering;real-time data analytics","Algorithm design and analysis;Base stations;Clustering algorithms;Data analysis;Mobile communication;Mobile computing;Telecommunication traffic","data analysis;learning (artificial intelligence);mobile computing;pattern clustering","Apache Hadoop;Mahout clustering algorithms;Mahout machine learning library;mobile network traffic data;mobile subscriptions;network devices resource usage;real-time data analytics;resource management","","","","","","","","June 27 2016-July 2 2016","","IEEE","IEEE Conferences"
"Evaluation of Machine Learning Frameworks on Bank Marketing and Higgs Datasets","B. M. Shashidhara; S. Jain; V. D. Rao; N. Patil; G. S. Raghavendra","Dept. of Inf. Technol., Nat. Inst. of Technol. Karnataka, Surathkal, India","2015 Second International Conference on Advances in Computing and Communication Engineering","20151026","2015","","","551","555","Big data is an emerging field with different datasets of various sizes are being analyzed for potential applications. In parallel, many frameworks are being introduced where these datasets can be fed into machine learning algorithms. Though some experiments have been done to compare different machine learning algorithms on different data, these experiments have not been tested out on different platforms. Our research aims to compare two selected machine learning algorithms on data sets of different sizes deployed on different platforms like Weka, Scikit-Learn and Apache Spark. They are evaluated based on Training time, Accuracy and Root mean squared error. This comparison helps us to decide what platform is best suited to work while applying computationally expensive selected machine learning algorithms on a particular size of data. Experiments suggested that Scikit-Learn would be optimal on data which can fit into memory. While working with huge, data Apache Spark would be optimal as it performs parallel computations by distributing the data over a cluster. Hence this study concludes that spark platform which has growing support for parallel implementation of machine learning algorithms could be optimal to analyze big data.","","Electronic:978-1-4799-1734-1; POD:978-1-4799-1735-8","10.1109/ICACCE.2015.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306745","Apache Spark;Big Data;Distributed Computing;Machine Learning Algorithms;Parallel Execution;Scikit-Learn;Weka","Accuracy;Algorithm design and analysis;Big data;Machine learning algorithms;Sparks;Support vector machines;Training","Big Data;data analysis;learning (artificial intelligence)","Apache Spark platform;Big Data analysis;Higgs datasets;Scikit-Learn platform;Weka platform;bank marketing datasets;machine learning frameworks","","","","16","","","","1-2 May 2015","","IEEE","IEEE Conferences"
"QoS Aware Resource Management for Apache Cassandra","Y. Kishore; N. H. V. Datta; K. V. Subramaniam; D. Sitaram","PESIT, Bangalore, India","2016 IEEE 23rd International Conference on High Performance Computing Workshops (HiPCW)","20170202","2016","","","3","10","Apache Cassandra is a distributed database of choice when it comes to big data management with zero downtime, linear scalability, and seamless multiple data center deployment. However, resource allocation for the system during deployment is a major issue which could either lead to bad performance or under-utilization of resources. In this paper we describe a QoS-aware architecture that manages resources for the distributed storage system to proactively and dynamically allocate resources for the distributed storage system to ensure that effective resource utilization and deliver performance according to the specified QoS. The architecture uses machine learning techniques to proactively predict and judge the performance of the system and make decisions for effective resource management for the database. In addition, we propose an adaptive sampling mechanism for classification to ensure that the architecture does not impose an unreasonable overhead on the system. We evaluate and provide results using Yahoo! Cloud Serving Benchmark (YCSB) on the modified Cassandra cluster. Our evaluation shows an increase of the overall utilization of the allocated resources without compromising on the required QoS latency for the overall cluster.","","Electronic:978-1-5090-5773-3; POD:978-1-5090-5774-0","10.1109/HiPCW.2016.009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7837042","Apache Cassandra;Machine Learning;QoS;Resource Management;Weka;YCSB","Cloud computing;Computer architecture;Databases;Engines;Peer-to-peer computing;Quality of service;Resource management","Big Data;cloud computing;computer centres;distributed databases;learning (artificial intelligence);quality of service;resource allocation;software architecture","Apache Cassandra;QoS aware resource management;QoS-aware architecture;YCSB;Yahoo! cloud serving benchmark;big data management;distributed database;linear scalability;machine learning;resource allocation;seamless multiple data center deployment","","","","","","","","19-22 Dec. 2016","","IEEE","IEEE Conferences"
"HeteroSpark: A heterogeneous CPU/GPU Spark platform for machine learning algorithms","Peilong Li; Yan Luo; Ning Zhang; Yu Cao","Dept. of Electrical and Computer Engineering, University of Massachusetts Lowell, USA","2015 IEEE International Conference on Networking, Architecture and Storage (NAS)","20150914","2015","","","347","348","Analytics algorithms on big data sets require tremendous computational capabilities. Spark is a recent development that addresses big data challenges with data and computation distribution and in-memory caching. However, as a CPU only framework, Spark cannot leverage GPUs and a growing set of GPU libraries to achieve better performance and energy efficiency. We present HeteroSpark, a GPU-accelerated heterogeneous architecture integrated with Spark, which combines the massive compute power of GPUs and scalability of CPUs and system memory resources for applications that are both data and compute intensive. We make the following contributions in this work: (1) we integrate the GPU accelerator into current Spark framework to further leverage data parallelism and achieve algorithm acceleration; (2) we provide a plug-n-play design by augmenting Spark platform so that current Spark applications can choose to enable/disable GPU acceleration; (3) application acceleration is transparent to developers, therefore existing Spark applications can be easily ported to this heterogeneous platform without code modifications. The evaluation of HeteroSpark demonstrates up to 18× speedup on a number of machine learning applications.","","Electronic:978-1-4673-7891-8; POD:978-1-4673-7892-5; USB:978-1-4673-7890-1","10.1109/NAS.2015.7255222","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7255222","","Acceleration;Big data;Computer architecture;Graphics processing units;Libraries;Machine learning algorithms;Sparks","Big Data;cache storage;graphics processing units;learning (artificial intelligence)","GPU libraries;GPU-accelerated heterogeneous architecture;HeteroSpark;Spark applications;analytics algorithms;big data sets;computation distribution;computational capabilities;data parallelism;energy efficiency;heterogeneous CPU-GPU spark platform;in-memory caching;machine learning algorithms;plug-n-play design;system memory resources","","2","","3","","","","6-7 Aug. 2015","","IEEE","IEEE Conferences"
"Consideration of parallel data processing over an apache spark cluster","K. Kato; A. Takefusa; H. Nakada; M. Oguchi","Ochanomizu University","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","4757","4759","The Spread of cameras and sensors and cloud technologies enable us to obtain life logs at ordinary homes and transmit the captured data to a cloud for life log analysis. However, the amount of processing for video data analysis in a cloud drastically increases when a very large number of homes send data to the cloud. In this research, we aim to improve the efficiency of distributed video data analysis processing by using the parallel deep learning framework Chainer [2] and the distribution processing platform Apache Spark [1] (Spark). In this paper, we construct a Spark cluster and investigate the performance of parallel data processing using Spark varying parameter settings.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258533","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258533","","Big Data;Conferences;Job shop scheduling;Machine learning;Sparks","cloud computing;data analysis;distributed processing;learning (artificial intelligence);parallel processing;pattern clustering;video signal processing","Apache Spark cluster;Chainer;cloud technologies;distributed video data analysis processing;distribution processing platform;life log analysis;parallel data processing;parallel deep learning framework","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Improved model for the spatial load forecasting of the Slovenian distribution network","M. Grabner; Z. Bregar; Š. Ivanjko; L. Valenčič","Slovenia","CIRED - Open Access Proceedings Journal","20180315","2017","2017","1","2354","2357","In accordance with the EU directive (EU 2009/72/EC) at least 80% of consumers will have to be equipped with smart meters until 2020. Therefore, the distribution companies are currently massively replacing old Ferraris meters with the new AMI (Advanced Metering Infrastructure) meters. The analysis of metering data from smart meters allows a better understanding of the network conditions in all operating states and help accurately assess the load of existing and new consumers. The paper presents a new analytics application based on big data from smart meters. Using unsupervised machine learning methods of grouping (clustering), the daily load profiles can be determined from a large amount of input data. By examining the load probability distribution in each cluster, consumers' stochastic models are made. The original daily load profiles are reproduced by using the Monte Carlo method, which allows very accurate analysis of LV and MV networks. The results obtained are used for spatial load forecasting. The paper briefly presents how it all fits together to evaluate the future load development for the entire considered area.","","","10.1049/oap-cired.2017.1020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8315571","","","Monte Carlo methods;cost-benefit analysis;data acquisition;geographic information systems;load forecasting;power distribution planning;probability;smart meters;stochastic processes;unsupervised learning","AMI;EU 2009-72-EC directive;Ferraris meter;LV network planning;MV network planning;Monte-Carlo method;Slovenian distribution network;advanced metering infrastructure;cost–benefit analysis;data acquisition;geographic information system;load probability distribution;smart meter;spatial load forecasting;stochastic model;unsupervised machine learning method","","","","","","","","10 2017","","IET","IET Journals & Magazines"
"[Front cover]","","","2016 International Conference on Machine Learning and Cybernetics (ICMLC)","20170223","2016","1","","1","1","The following topics are dealt with: fuzzy PID; compressive sensing; convolutional neural network; support vector classification; image classification; subspace clustering; smart phone; gesture recognition; machine learning; hidden Markov model; decision-theoretic rough set model; Bayesian network; clustering algorithm; differential evolution; Big Data; optimization; feature selection; and expression recognition.","","CD:978-1-5090-0388-4; Electronic:978-1-5090-0390-7; POD:978-1-5090-0391-4","10.1109/ICMLC.2016.7860862","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7860862","","","Big Data;belief networks;compressed sensing;decision theory;emotion recognition;evolutionary computation;feature selection;fuzzy control;fuzzy set theory;gesture recognition;hidden Markov models;image classification;learning (artificial intelligence);neural nets;optimisation;pattern clustering;rough set theory;smart phones;support vector machines;three-term control","Bayesian network;Big Data;clustering algorithm;compressive sensing;convolutional neural network;decision-theoretic rough set model;differential evolution;expression recognition;feature selection;fuzzy PID;gesture recognition;hidden Markov model;image classification;machine learning;optimization;smart phone;subspace clustering;support vector classification","","","","","","","","10-13 July 2016","","IEEE","IEEE Conferences"
"Hadoop cluster with FPGA-based hardware accelerators for K-means clustering algorithm","C. C. Chung; Y. H. Wang","Department of CSIE, National Chung Cheng University, Taiwan","2017 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-TW)","20170727","2017","","","143","144","In this paper, the implementation of the K-means clustering algorithm on a Hadoop cluster with FPGA-based hardware accelerators is presented. The proposed design follows MapReduce programming model and uses Hadoop distribution file system (HDFS) for storing large dataset. The proposed FPGA-based hardware accelerator for speed up the K-means clustering algorithm is implemented on Xilinx VC707 evaluation boards (EVBs). There are four computers in the proposed Hadoop cluster, one computer is Master Node, and the other three computers are Slave Nodes. The Slave Nodes communicate with VC707 EVBs through Gigabit Ethernet. The experimental results show that for clustering 125 million three-dimensional input dataset, the proposed design can achieve 4× speedup than the Hadoop cluster without FPGA-based hardware accelerators.","","Electronic:978-1-5090-4017-9; POD:978-1-5090-4018-6","10.1109/ICCE-China.2017.7991036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7991036","","Algorithm design and analysis;Big Data;Clustering algorithms;Computers;Field programmable gate arrays;Hardware;Machine learning algorithms","distributed databases;field programmable gate arrays;local area networks;parallel programming;pattern clustering","Ethernet;FPGA-based hardware accelerators;HDFS;Hadoop cluster;Hadoop distribution file system;K-means clustering algorithm;MapReduce programming model;VC707 EVB;Xilinx VC707 evaluation boards;master node;slave nodes;three-dimensional input dataset","","","","","","","","12-14 June 2017","","IEEE","IEEE Conferences"
"Classification and clustering for neuroinformatics: Assessing the efficacy on reverse-mapped NeuroNLP data using standard ML techniques","N. Melethadathil; P. Chellaiah; B. Nair; S. Diwakar","Amrita School of Biotechnology, Amrita Vishwa Vidyapeetham, (Amrita University), Kollam, Kerala, India","2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","20150928","2015","","","1065","1070","NeuroinformaticsNatural Language Processing (NeuroNLP) relies on clustering and classification for information categorization of biologically relevant extraction targets and for interconnections to knowledge-related patterns in event and text mined datasets. The accuracy of machine learning algorithms depended on quality of text-mined data while efficacy relied on the context of the choice of techniques. Although developments of automated keyword extraction methods have made differences in the quality of data selection, the efficacy of the Natural Language Processing (NLP) methods using verified keywords remain a challenge. In this paper, we studied the role of text classification and document clustering algorithms on datasets, where features were obtained by mapping to manually verified MESH terms published by National Library of Medicine (NLM). In this study, NLP data classification involved comparing 8techniques and unsupervised learning was performed with 6 clustering algorithms. Most classification techniques except meta-based algorithms namely stacking and vote, allowed 90% or higher training accuracy. Test accuracy was high (=>95%) probably due to limited test dataset. Logistic Model Trees had 30-fold higher runtime compared to other classification algorithms including Naive Bayes, AdaBoost, Hoeffding Tree. Grouped error rate in clustering was 0-4%. Runtime-wise, clustering was faster than classification algorithms on MESH-mapped NLP data suggesting clustering methods as adequate towards Medline-related datasets and text-mining big data analytic systems.","","Electronic:978-1-4799-8792-4; POD:978-1-4799-8793-1; USB:978-1-4799-8791-7","10.1109/ICACCI.2015.7275751","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7275751","NeuroNLP;Neuroinformatics;accuracy;classification;clustering","Accuracy;Biology;Classification algorithms;Clustering algorithms;Error analysis;Filtering algorithms;Text categorization","Big Data;bioinformatics;data mining;natural language processing;pattern classification;pattern clustering;text analysis;trees (mathematics);unsupervised learning","MESH terms;Medline-related datasets;NLP data classification;automated keyword extraction methods;data selection;document clustering algorithms;information categorization;knowledge-related patterns;logistic model trees;machine learning algorithms;meta-based algorithms;neuroinformatics natural language processing NeuroNLP;reverse-mapped NeuroNLP data;standard ML techniques;text classification;text mined datasets;text-mining big data analytic systems;unsupervised learning","","1","","28","","","","10-13 Aug. 2015","","IEEE","IEEE Conferences"
"Scalable and parallel machine learning algorithms for statistical data mining - Practice & experience","M. Riedel; M. Goetz; M. Richerzhagen; P. Glock; C. Bodenstein; A. S. Memon; M. S. Memon","Forschungszentrum Juelich, Juelich Supercomputing Centre, Germany","2015 38th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)","20150716","2015","","","204","209","Many scientific datasets (e.g. earth sciences, medical sciences, etc.) increase with respect to their volume or in terms of their dimensions due to the ever increasing quality of measurement devices. This contribution will specifically focus on how these datasets can take advantage of new `big data' technologies and frameworks that often are based on parallelization methods. Lessons learned with medical and earth science data applications that require parallel clustering and classification techniques such as support vector machines (SVMs) and density-based spatial clustering of applications with noise (DBSCAN) are a substantial part of the contribution. In addition, selected experiences of related `big data' approaches and concrete mining techniques (e.g. dimensionality reduction, feature selection, and extraction methods) will be addressed too. In order to overcome identified challenges, we outline an architecture framework design that we implement with open available tools in order to enable scalable and parallel machine learning applications in distributed systems.","","DVD:978-9-5323-3085-4; Electronic:978-9-5323-3082-3; POD:978-1-4799-8174-8","10.1109/MIPRO.2015.7160265","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7160265","","Algorithm design and analysis;Clustering algorithms;Concrete;Data mining;Machine learning algorithms;Standards;Support vector machines","Big Data;data mining;learning (artificial intelligence);parallel processing;pattern classification;pattern clustering;statistical analysis;support vector machines","Big Data technology;DBSCAN;SVMs;architecture framework design;classification techniques;concrete mining techniques;density-based spatial clustering of applications with noise;distributed systems;measurement device quality;parallel clustering;parallel machine learning algorithms;scalable machine learning algorithms;statistical data mining;support vector machines","","1","","27","","","","25-29 May 2015","","IEEE","IEEE Conferences"
"JVM characterization framework for workload generated as per machine learning benckmark and spark framework","S. Chidambaram; S. Saraswati; R. Ramachandra; J. B. Huttanagoudar; N. Hema; R. Roopalakshmi","Hewlett Packard (India) Software Operation Private Ltd, Bangalore - 560048, India","2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)","20170109","2016","","","1598","1602","Today there are plenty of frameworks to assist the development of Big-data applications. Computation and Storage are two major activities in these applications. Spark framework has replaced Map-Reduce in Hadoop, which is the preferred analytics engine for Big-data applications. Java Virtual Machine (JVM) is used as execution platform irrespective of which framework is used for development. In the production environment it is essential to monitor the health of application to gain better performance. The parameters like memory usage, CPU utilization and frequency of Garbage Collection etc., will help to decide on the health of application. In this paper a framework is proposed to characterize the JVM behavior to monitor the health of application. Workload generated by running Machine Learning algorithms available in Spark Benchmark Suite.","","Electronic:978-1-5090-0774-5; POD:978-1-5090-0775-2","10.1109/RTEICT.2016.7808102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7808102","Apache Spark;Big-Data;In-Memory Analytics;Java Virtual Machine;Machine Learnig","Benchmark testing;Clustering algorithms;Java;Machine learning algorithms;Monitoring;Sparks","Java;learning (artificial intelligence);virtual machines","JVM characterization framework;Java virtual machine;Spark framework;application health monitoring;machine learning benckmark","","","","","","","","20-21 May 2016","","IEEE","IEEE Conferences"
"A Novel Big Data Modeling Method for Improving Driving Range Estimation of EVs","C. H. Lee; C. H. Wu","Electrical Engineering Department, National Kaohsiung University of Applied Sciences, Kaohsiung, Taiwan","IEEE Access","20151028","2015","3","","1980","1993","In this paper, we address a big-data analysis method for estimating the driving range of an electric vehicle (EV), allowing drivers to overcome range anxiety. First, we present an estimating approach to project the life of battery pack for 1600 cycles (i.e., 8 years/160 000 km) based on the data collected from a cycle-life test. This approach has the merit of simplicity. In addition, it considers several critical issues that occur inside battery packs, such as the dependence of internal resistance and the state-of-health. Subsequently, we describe our work on driving pattern analysis of an EV, using a machine-learning approach, namely growing hierarchical self-organizing maps, to cluster the collected EV big data. This paper contains the analysis of energy consumption and driving range estimation for EVs, including powertrain simulation and driving behavior analysis. The experimental results, including both simulating battery degradation and analysis of driving behaviors, demonstrate a feasible solution for improving driving range estimation by the EV big data.","","","10.1109/ACCESS.2015.2492923","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300375","EV big-data;Electric vehicle;battery modeling method;data mining;electric vehicle;range estimation","Batteries;Big data;Data mining;Driving range;Electric vehicles;Modeling","Big Data;battery powered vehicles;data analysis;data mining;estimation theory;learning (artificial intelligence);pattern clustering;traffic engineering computing","Big Data modeling method;EV big data clustering;EV driving pattern analysis;battery packs;big-data analysis method;driving behavior analysis;electric vehicle driving range estimation;energy consumption;growing hierarchical self-organizing maps;machine-learning approach;powertrain simulation","","5","","35","","","20151019","2015","","IEEE","IEEE Journals & Magazines"
"A Scalable Data Science Workflow Approach for Big Data Bayesian Network Learning","J. Wang; Y. Tang; M. Nguyen; I. Altintas","San Diego Supercomput. Center, Univ. of California, San Diego, La Jolla, CA, USA","2014 IEEE/ACM International Symposium on Big Data Computing","20151109","2014","","","16","25","In the Big Data era, machine learning has more potential to discover valuable insights from the data. As an important machine learning technique, Bayesian Network (BN) has been widely used to model probabilistic relationships among variables. To deal with the challenges of Big Data PN learning, we apply the techniques in distributed data-parallelism (DDP) and scientific workflow to the BN learning process. We first propose an intelligent Big Data pre-processing approach and a data quality score to measure and ensure the data quality and data faithfulness. Then, a new weight based ensemble algorithm is proposed to learn a BN structure from an ensemble of local results. To easily integrate the algorithm with DDP engines, such as Hadoop, we employ Kepler scientific workflow to build the whole learning process. We demonstrate how Kepler can facilitate building and running our Big Data BN learning application. Our experiments show good scalability and learning accuracy when running the application in real distributed environments.","","Electronic:978-1-4799-1897-3; POD:978-1-4799-1898-0; USB:978-1-4799-1896-6","10.1109/BDC.2014.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7321725","Bayesian network;Big Data;Distributed computing;Ensemble learning;Hadoop;Kepler;Scientific workflow","Accuracy;Algorithm design and analysis;Bayes methods;Big data;Distributed databases;Engines;Partitioning algorithms","Big Data;belief networks;learning (artificial intelligence)","BN learning process;Big Data Bayesian network learning;Big Data PN learning;DDP;Kepler scientific workflow;data faithfulness;data quality score;distributed data parallelism;distributed environments;machine learning technique;probabilistic relationships;scalable data science workflow approach;scientific workflow","","7","","35","","","","8-11 Dec. 2014","","IEEE","IEEE Conferences"
"Applying bounded fuzzy possibilistic method on critical objects","H. Yazdani; D. Ortiz-Arroyo; K. Choros; H. Kwasnicka","Wroclaw University of Technology, Aalborg University","2016 IEEE 17th International Symposium on Computational Intelligence and Informatics (CINTI)","20170209","2016","","","000271","000276","Providing a flexible environment to process data objects is a desirable goal of machine learning algorithms. In fuzzy and possibilistic methods, the relevance of data objects is evaluated and a membership degree is assigned. However, some critical objects objects have the potential ability to affect the performance of the clustering algorithms if they remain in a specific cluster or they are moved into another. In this paper we analyze how critical objects affect the behaviour of fuzzy possibilistic methods in several data sets. The paper also compares the accuracy of Bounded fuzzy possibilistic method (BFPM) with conventional fuzzy possibilistic methods. The comparison is based on the accuracy and ability of learning methods to provide a proper searching space for data objects. The membership functions used by each method when dealing with critical objects is also evaluated. Our results show that relaxing the conditions of participation for data objects in as many partitions as they can, is beneficial.","","Electronic:978-1-5090-3909-8; POD:978-1-5090-3910-4; USB:978-1-5090-3908-1","10.1109/CINTI.2016.7846417","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846417","Bounded Fuzzy Possibilistic Method;Classification;Clustering;Critical Objects;Data Object;Membership Function","Big data;Clustering algorithms;Computational intelligence;Informatics;Learning systems;Partitioning algorithms;Search problems","data handling;fuzzy set theory;learning (artificial intelligence);search problems","BFPM;bounded fuzzy possibilistic method;critical objects;data objects;flexible environment;fuzzy possibilistic methods;machine learning algorithms;searching space","","1","","","","","","17-19 Nov. 2016","","IEEE","IEEE Conferences"
"Table of contents","","","2014 International Conference on Data Science and Advanced Analytics (DSAA)","20150312","2014","","","1","6","The following topics are dealt with: machine learning; data retrieval; information query; data analytics; recommendation system; data classification; data clustering; data privacy; influence analysis; data science application; complex data analysis; cloud computing; parallel computing; data mining; Big Data; and exploratory computing.","","Electronic:978-1-4799-6991-3; POD:978-1-4799-6982-1","10.1109/DSAA.2014.7058043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058043","","","Big Data;cloud computing;data analysis;data mining;data privacy;information retrieval;learning (artificial intelligence);parallel processing;pattern classification;pattern clustering;recommender systems","Big Data;cloud computing;complex data analysis;data analytics;data classification;data clustering;data mining;data privacy;data retrieval;data science application;exploratory computing;influence analysis;information query;machine learning;parallel computing;recommendation system","","0","","","","","","Oct. 30 2014-Nov. 1 2014","","IEEE","IEEE Conferences"
"A Framework for Mixed-Type Multioutcome Prediction With Applications in Healthcare","B. Saha; S. Gupta; D. Phung; S. Venkatesh","Pattern Recognition and Data Analytics, School of Information Technology, Deakin University, Warun Ponds, Highton, Australia","IEEE Journal of Biomedical and Health Informatics","20170630","2017","21","4","1182","1191","Health analysis often involves prediction of multiple outcomes of mixed type. The existing work is restrictive to either a limited number or specific outcome types. We propose a framework for mixed-type multioutcome prediction. Our proposed framework proposes a cumulative loss function composed of a specific loss function for each outcome type-as an example, least square (continuous outcome), hinge (binary outcome), Poisson (count outcome), and exponential (nonnegative outcome). To model these outcomes jointly, we impose a commonality across the prediction parameters through a common matrix normal prior. The framework is formulated as iterative optimization problems and solved using an efficient block-coordinate descent method. We empirically demonstrate both scalability and convergence. We apply the proposed model to a synthetic dataset and then on two real-world cohorts: a cancer cohort and an acute myocardial infarction cohort collected over a two-year period. We predict multiple emergency-related outcomes-as example, future emergency presentations (binary), emergency admissions (count), emergency length of stay days (nonnegative), and emergency time to next admission day (nonnegative). We show that the predictive performance of the proposed model is better than several state-of-the-art baselines.","2168-2194;21682194","","10.1109/JBHI.2017.2681799","Telstra-Deakin Centre of Excellence in Big Data and Machine Learning; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7879827","Health information management;Multi-task learning;Optimization","Correlation;Covariance matrices;Data models;Fasteners;Hospitals;Mathematical model;Predictive models","cancer;health care;iterative methods;least squares approximations;optimisation","acute myocardial infarction cohort;block-coordinate descent method;cancer cohort;cumulative loss function;healthcare;iterative optimization;mixed-type multioutcome prediction;specific loss function","Computational Biology;Health Information Management;Humans;Machine Learning;Medical Informatics Applications;Models, Statistical;Regression Analysis","","","","","","20170316","July 2017","","IEEE","IEEE Journals & Magazines"
"Efficient parallelization for big data collaborative recommendation decisions","E. O. Aboagye; G. Jianbin; A. A. Emmanuel","UESTC, Chengdu, China","2018 IEEE 8th Annual Computing and Communication Workshop and Conference (CCWC)","20180226","2018","","","268","274","This paper proposes a novel Sentiment-Based Probabilistic Tensor Analysis technique {senti-PTF} to address the information overload problem through information filtering. The proposed framework first applies a Natural Language Processing (NLP) technique to perform sentiment analysis taking advantage of the huge sums of textual data generated in from the social media are predominantly left untouched. Although some current studies do employ review texts, many of them do not consider how sentiments in reviews influence recommendation algorithm for prediction. Existing works concentrate only on rating matrix which are often sparse. There is therefore this big data text analytics gap whose modeling is computationally expensive. Probabilistic Tensor Factorization (PTF) is a standard technique for such large scale processing. From our experiments, our novel machine learning sentiment-based probabilistic tensor analysis (senti-PTF) is computationally less expensive, scalable and addresses the scalability problem and cold-start problems, for optimal recommendation decision making as shown from our error detection evaluation metrics.","","Electronic:978-1-5386-4649-6; POD:978-1-5386-4650-2","10.1109/CCWC.2018.8301706","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8301706","Probabilistic Tensor Factorization;data partitioning;rat-PTF;senti-PTF","Data models;Mathematical model;Partitioning algorithms;Prediction algorithms;Probabilistic logic;Scalability;Tensile stress","Big Data;data mining;decision making;information filtering;learning (artificial intelligence);natural language processing;recommender systems;sentiment analysis;social networking (online);tensors","Natural Language Processing technique;Probabilistic Tensor Factorization;big data collaborative recommendation decisions;big data text analytics gap;cold-start problems;efficient parallelization;information filtering;information overload problem;novel Sentiment-Based Probabilistic Tensor Analysis technique;novel machine learning sentiment;optimal recommendation decision;recommendation algorithm;review texts;scalability problem;senti-PTF;sentiment analysis;social media;standard technique;textual data","","","","","","","","8-10 Jan. 2018","","IEEE","IEEE Conferences"
"A quick view on current techniques and machine learning algorithms for big data analytics","J. L. Berral-García","Barcelona Supercomputing Center, Jordi Girona 29-31, 08034, Spain","2016 18th International Conference on Transparent Optical Networks (ICTON)","20160825","2016","","","1","4","Big-data is an excellent source of knowledge and information from our systems and clients, but dealing with such amount of data requires automation, and this brings us to data mining and machine learning techniques. In the ICT sector, as in many other sectors of research and industry, platforms and tools are being served and developed in order to help professionals to treat their data and learn from it automatically; most of those platforms coming from big companies like Google or Microsoft, or from incubators at the Apache Foundation. This brief review explains the basics of machine learning with some ICT examples, and enumerates some (but not all) of the most used tools for analyzing and modelling big-data.","","Electronic:978-1-5090-1467-5; POD:978-1-5090-1468-2; USB:978-1-5090-1466-8","10.1109/ICTON.2016.7550517","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7550517","analytics;big data;frameworks;knowledge discovery;machine learning","Classification algorithms;Clustering algorithms;Data mining;Data models;Libraries;Machine learning algorithms;Prediction algorithms","Big Data;data analysis;data mining;learning (artificial intelligence)","Apache Foundation;Big data analytics;Big data modelling;Google;ICT;ICT sector;Microsoft;data mining;knowledge source;machine learning","","","","","","","","10-14 July 2016","","IEEE","IEEE Conferences"
"Learning the Distribution of Data for Embedding","Y. Shen; P. Ren; T. Zhang; Y. Y. Tang","Dept. of Comput. Sci., Chongqing Univ., Chongqing, China","2016 7th International Conference on Cloud Computing and Big Data (CCBD)","20170717","2016","","","46","51","One of the central problems in machine learning and pattern recognition is how to deal with high-dimensional data either for visualization or for classification and clustering. Most of dimensionality reduction technologies, designed to cope with the curse of dimensionality, are based on Euclidean distance metric. In this work, we propose an unsupervised nonlinear dimensionality reduction method which attempt to preserve the distribution of input data, called distribution preserving embedding (DPE). It is done by minimizing the dissimilarity between the densities estimated in the original and embedded spaces. In theory, patterns in data can effectively be described by the distribution of the data. Therefore, DPE is able to discover the intrinsic pattern (structure) of data, including the global structures and the local structures. Additionally, DPE can be extended to cope with out-of-sample problem naturally. Extensive experiments on different data sets compared with other competing methods are reported to demonstrate the effectiveness of the proposed approach.","","Electronic:978-1-5090-3555-7; POD:978-1-5090-3556-4","10.1109/CCBD.2016.020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7979878","Dimensionality reduction;Kernel density estimation;distribution preserving embedding","Bandwidth;Estimation;Kernel;Manifolds;Optimization;Principal component analysis;Symmetric matrices","data reduction;data visualisation;learning (artificial intelligence);minimisation;pattern classification;pattern clustering","DPE;Euclidean distance metric;data classification;data clustering;data visualization;dissimilarity minimization;distribution preserving embedding;embedded spaces;global structures;high-dimensional data;input data distribution;intrinsic pattern;local structures;machine learning;pattern recognition;unsupervised nonlinear dimensionality reduction","","","","","","","","16-18 Nov. 2016","","IEEE","IEEE Conferences"
"Towards scalable kernel machines for streaming data analytics","S. D. Bopardikar; G. S. E. Ekladious","United Technologies Research Center","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","4730","4732","Kernel methods for machine learning have a strong mathematical basis and a proven modeling power. However, scalability of these methods is limited by the intensive computations they require. More specifically, for Gaussian Process, the covariance matrix needs to be inverted to compute the posterior distribution and to optimize the hyperparameters of the employed kernel. Scalability requirement of the matrix inversion computations grows with the number of points in the training data, which hinders applicability of the method to streaming and big data analytics applications. In this paper, we briefly review our recent sequential Gaussian process approach. Then, we propose an incremental hyperparameter optimization algorithm for polynomial kernels.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258523","Gaussian Process;Kernel Machines;Matrix Factorization;Streaming Analytics;hyperparameter optimization","Big Data;Covariance matrices;Gaussian processes;Kernel;Optimization;Scalability;Training data","Big Data;Gaussian processes;covariance matrices;data analysis;learning (artificial intelligence);matrix inversion;optimisation;polynomial matrices;statistical distributions","big data analytics applications;covariance matrix;incremental hyperparameter optimization algorithm;kernel methods;machine learning;matrix inversion computations;polynomial kernels;posterior distribution;scalable kernel machines;sequential Gaussian process approach;streaming data analytics applications","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Ensemble learning for network data stream classification using similarity and online genetic algorithm classifiers","M. A. M. Raja; S. Swamynathan","Department of Information Science and Technology, College of Engineering, Guindy Anna University, Chennai","2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","20161103","2016","","","1601","1607","The data are generated very rapidly from different information sources. These generation of data is increasing day by day from various sources such as automated data collection tools, database systems, e-commerce and social media websites. There is an explosive growth of data from terabytes to petabytes. It is essential to extract valuable knowledge from these large data. Since large amount of data is available, people look for valuable knowledge from the available data. Several mining algorithms are used to extract interesting patterns from the data stored in a repository. Traditional data sources are static in nature that the content are not generated very rapidly. Data streams are the streams of information that are generated at very rapid rate. After the evolution of data streams, the need arises to think of a new algorithm to process it. There are various data stream algorithms used for mining the data streams with different requirements. In this work, the ensemble of classifiers model has been developed for mining the data streams by combining stream mining classifiers such as Similarity-based Data Stream Classifier (SimC) and Online Genetic Algorithm (OGA) classifier. The performance of ensemble based classifiers show improved classification accuracy and less classification error rate under various circumstances.","","","10.1109/ICACCI.2016.7732277","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7732277","accuracy;classification;clustering;data stream;ensemble of classifiers;machine learning","Classification algorithms;Clustering algorithms;Computer architecture;Data mining;Error analysis;Genetic algorithms;Informatics","Big Data;database management systems;learning (artificial intelligence);pattern classification","Big data;OGA classifier;automated data collection tools;classification error rate;data sources;data stream mining;database systems;e-commerce;ensemble learning;information sources;network data stream classification;online genetic algorithm classifiers;similarity genetic algorithm classifiers;social media websites;valuable knowledge","","","","","","","","21-24 Sept. 2016","","IEEE","IEEE Conferences"
"Criminal shortlisting and crime forecasting based on modus operandi","M. Munasinghe; S. Udeshini; H. Perera; R. Weerasinghe","University of Colombo School of Computing, No.35, Reid Avenue, Colombo 07, Sri Lanka","2014 14th International Conference on Advances in ICT for Emerging Regions (ICTer)","20150416","2014","","","265","265","Crime detection and prevention is a very crucial wok which is in the hands of police, law enforcement agencies and local government. Experts in crime analyzing use crime scene evidences to capture unique ways a criminal has acted during a crime, which is also called as Modus Operandi(MO). Using MO as the main focus, the efforts taken in this research is to shortlist and predict criminals and criminal activities with the support of machine learning based algorithms.","","CD-ROM:978-1-4799-7729-1; Electronic:978-1-4799-7732-1; POD:978-1-4799-7733-8","10.1109/ICTER.2014.7083923","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7083923","Criminal Profiling;Machine Learning;Modus Operandi","Accuracy;Clustering algorithms;Feature extraction;Forecasting;Joining processes;Machine learning algorithms;Prediction algorithms","criminal law;forecasting theory;learning (artificial intelligence);local government;police data processing","MO;crime detection;crime forecasting;crime prevention;crime scene evidences;criminal shortlisting;law enforcement agencies;local government;machine learning based algorithms;modus operandi;police","","0","","","","","","10-13 Dec. 2014","","IEEE","IEEE Conferences"
"Machine-Learning-Based Identification of Defect Patterns in Semiconductor Wafer Maps: An Overview and Proposal","F. Adly; P. D. Yoo; S. Muhaidat; Y. Al-Hammadi","Dept. ECE, Khalifa Univ., Abu Dhabi, United Arab Emirates","2014 IEEE International Parallel & Distributed Processing Symposium Workshops","20141204","2014","","","420","429","Wafers are formed from very thin layers of a semiconductor material, hence, they are highly susceptible to various kinds of defects. The defects are most likely to occur during the lengthy and complex fabrication process, which can include hundreds of steps. Wafer defects are generally caused by machine inaccuracy, chemical stains, physical damages, human mistakes, and atmospheric conditions. The defective chips tend to have several unique spatial patterns across the wafer, namely ring, spot, repetitive and cluster patterns. To locate such defect patterns, wafer maps are used to visualize and ultimately lead to better understanding of what happened during the process failure. To identify the unique patterns of defects and to find the point of manufacturing process that causes such defects accurately, nature-inspired model-free machine-learning techniques have been well accepted. This paper thus reviews the theoretical and experimental literature of such models with a focus on model learnability and efficiency-related issues involving data reduction and transformation techniques, which could be seen as the key model properties to deal with big data applications.","","Electronic:978-1-4799-4116-2; POD:978-1-4799-4115-5","10.1109/IPDPSW.2014.54","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6969418","and classification;nature-inspired machine-learning;wafer defect patterns;wafer map","Accuracy;Classification algorithms;Feature extraction;Pattern matching;Semiconductor device modeling;Support vector machines;Vectors","crystal defects;learning (artificial intelligence);semiconductor materials;semiconductor technology","defect patterns;machine learning based identification;process failure;semiconductor material;semiconductor wafer maps;wafer defects","","1","","26","","","","19-23 May 2014","","IEEE","IEEE Conferences"
"Customer segmentation using centroid based and density based clustering algorithms","A. S. M. S. Hossain","Department of Computer Science & Engineering, Rajshahi University of Engineering & Technology, Rajshahi - 6204, Bangladesh","2017 3rd International Conference on Electrical Information and Communication Technology (EICT)","20180201","2017","","","1","6","In recent years, customer segmentation has become one of the most significant and useful tools for e-commerce. It plays a vital role in online product recommendation system and also helps to understand local and global wholesale or retail market. Customer segmentation refers to grouping customers into different categories based on shared characteristics such as age, location, spending habit and so on. Similarly, clustering means putting things together in such a way that similar type of things remain in the same group. Due to having similarities between these two terms, it is possible to apply clustering algorithms for ensuring satisfactory and automatic customer segmentation. Among different types of clustering algorithms, centroid based and density based are the most popular. This paper illustrates the idea of applying density based algorithms for customer segmentation beside using centroid based algorithms like k-means. Applying DBSCAN (Density-Based Spatial Clustering of Applications with Noise) algorithm as one of the density based algorithms results in a meaningful customer segmentation.","","Electronic:978-1-5386-2307-7; POD:978-1-5386-2308-4; Paper:978-1-5386-2305-3; USB:978-1-5386-2306-0","10.1109/EICT.2017.8275249","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8275249","Centroid based clustering;Clustering algorithms;Customer segmentation;Density based clustering;Market segmentation","Algorithm design and analysis;Clustering algorithms;Euclidean distance;Focusing;Machine learning algorithms;Mathematical model","electronic commerce;marketing data processing;pattern clustering;recommender systems","DBSCAN algorithm;Density-Based Spatial Clustering of Applications with Noise algorithm;automatic customer segmentation;centroid based algorithms;clustering algorithms;density based algorithms;e-commerce;global wholesale market;local market;market segmentation;online product recommendation system;retail market;satisfactory customer segmentation","","","","","","","","7-9 Dec. 2017","","IEEE","IEEE Conferences"
"An Effective Integrated Method for Learning Big Imbalanced Data","M. Ghanavati; R. K. Wong; F. Chen; Y. Wang; C. S. Perng","Sch. of Comput. Sci. & Eng., Univ. of New South Wales, Sydney, NSW, Australia","2014 IEEE International Congress on Big Data","20140925","2014","","","691","698","The imbalance of data has great effects on the performance of learning algorithms due to the presence of under-represented data and severe class distribution skews. This is one of the new challenges of machine learning data mining. Choosing a suitable metric that addresses the properties and domain characteristics of learning real-world data is critical for achieving a good result in most of machine learning and data mining algorithms. When the dataset is big and imbalanced, even with an accurate metric, it is extremely difficult to achieve good learning performance. This paper proposes an integrated method for learning large imbalanced datasets. In particular, a combination of metric learning algorithms and balancing techniques are experimented. Their performances are compared based on a set of evaluation metrics running on bootstrap datasets of different sizes. The best combination is then selected for learning the full imbalanced datasets. Experiments using the water pipeline datasets collected from various Australia regions in the past two decades show that our proposed method is both practical and effective.","2379-7703;23797703","Electronic:978-1-4799-5057-7; POD:978-1-4799-5058-4","10.1109/BigData.Congress.2014.102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906846","Classification;Imbalanced data;Large Margin Nearest Neighbour;Metric Learning","Clustering algorithms;Equations;Error analysis;Learning systems;Mathematical model;Measurement;Training","data mining;learning (artificial intelligence)","balancing techniques;bootstrap datasets;evaluation metrics;large imbalanced dataset learning;machine learning data mining;metric learning algorithms","","2","","31","","","","June 27 2014-July 2 2014","","IEEE","IEEE Conferences"
"Machine learning and its applications: A review","S. Angra; S. Ahuja","Chitkara University, India","2017 International Conference on Big Data Analytics and Computational Intelligence (ICBDAC)","20171019","2017","","","57","60","Nowadays, large amount of data is available everywhere. Therefore, it is very important to analyze this data in order to extract some useful information and to develop an algorithm based on this analysis. This can be achieved through data mining and machine learning. Machine learning is an integral part of artificial intelligence, which is used to design algorithms based on the data trends and historical relationships between data. Machine learning is used in various fields such as bioinformatics, intrusion detection, Information retrieval, game playing, marketing, malware detection, image deconvolution and so on. This paper presents the work done by various authors in the field of machine learning in various application areas.","","Electronic:978-1-5090-6400-7; POD:978-1-5090-6401-4; USB:978-1-5090-6399-4","10.1109/ICBDACI.2017.8070809","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8070809","Machine Learning;SVM;classification;clustering;decision tress;feature selection;logistic regression","Big Data;Computational intelligence;Conferences;Hafnium;Handheld computers","data mining;learning (artificial intelligence)","artificial intelligence;bioinformatics;data mining;data trends;design algorithm;game playing;historical relationships;image deconvolution;information retrieval;intrusion detection;machine learning;malware detection;marketing","","","","","","","","23-25 March 2017","","IEEE","IEEE Conferences"
"Scalable Overlapping Community Detection","I. El-Helw; R. Hofman; W. Li; S. Ahn; M. Welling; H. Bal","Vrije Univ. Amsterdam, Amsterdam, Netherlands","2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)","20160804","2016","","","1463","1472","Recent advancements in machine learning algorithms have transformed the data analytics domain and provided innovative solutions to inherently difficult problems. However, training models at scale over large data sets remains a daunting challenge. One such problem is the detection of overlapping communities within graphs. For example, a social network can be modeled as a graph where the vertices and edges represent individuals and their relationships. As opposed to the problem of graph partitioning or clustering, an individual can be part of multiple communities which significantly increases the problem complexity. In this paper, we present and evaluate an efficient parallel and distributed implementation of a Stochastic Gradient Markov Chain Monte Carlo algorithm that solves the overlapping community detection problem. We show that the algorithm can scale and process graphs consisting of billions of edges and tens of millions of vertices on a compute cluster of 65 nodes. To the best of our knowledge, this is the first time that the problem of deducing overlapping communities has been learned for problems of such a large scale.","","Electronic:978-1-5090-3682-0; POD:978-1-5090-3683-7","10.1109/IPDPSW.2016.165","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7530037","Distributed computing;High performance computing;Machine learning;Parallel programming;Performance analysis","Algorithm design and analysis;Clustering algorithms;Electronic mail;Heuristic algorithms;Monte Carlo methods;Parallel processing;Stochastic processes","Markov processes;Monte Carlo methods;data analysis;graph theory;learning (artificial intelligence);parallel processing","data analytics domain;distributed implementation;machine learning algorithms;parallel implementation;scalable overlapping community detection;stochastic gradient Markov chain Monte Carlo algorithm","","1","","","","","","23-27 May 2016","","IEEE","IEEE Conferences"
"Extraction of latent patterns and contexts from social honest signals using hierarchical Dirichlet processes","T. Nguyen; D. Phung; S. Gupta; S. Venkatesh","Centre for Pattern Recognition and Data Analytics, Deakin University, Australia","2013 IEEE International Conference on Pervasive Computing and Communications (PerCom)","20130610","2013","","","47","55","A fundamental task in pervasive computing is reliable acquisition of contexts from sensor data. This is crucial to the operation of smart pervasive systems and services so that they might behave efficiently and appropriately upon a given context. Simple forms of context can often be extracted directly from raw data. Equally important, or more, is the hidden context and pattern buried inside the data, which is more challenging to discover. Most of existing approaches borrow methods and techniques from machine learning, dominantly employ parametric unsupervised learning and clustering techniques. Being parametric, a severe drawback of these methods is the requirement to specify the number of latent patterns in advance. In this paper, we explore the use of Bayesian nonparametric methods, a recent data modelling framework in machine learning, to infer latent patterns from sensor data acquired in a pervasive setting. Under this formalism, nonparametric prior distributions are used for data generative process, and thus, they allow the number of latent patterns to be learned automatically and grow with the data - as more data comes in, the model complexity can grow to explain new and unseen patterns. In particular, we make use of the hierarchical Dirichlet processes (HDP) to infer atomic activities and interaction patterns from honest signals collected from sociometric badges. We show how data from these sensors can be represented and learned with HDP. We illustrate insights into atomic patterns learned by the model and use them to achieve high-performance clustering. We also demonstrate the framework on the popular Reality Mining dataset, illustrating the ability of the model to automatically infer typical social groups in this dataset. Finally, our framework is generic and applicable to a much wider range of problems in pervasive computing where one needs to infer high-level, latent patterns and contexts from sensor data.","","Electronic:978-1-4673-4575-0; POD:978-1-4673-4573-6; USB:978-1-4673-4574-3","10.1109/PerCom.2013.6526713","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6526713","","Accelerometers;Bayes methods;Bluetooth;Context;Data mining;Data models;Hidden Markov models","data mining;learning (artificial intelligence);nonparametric statistics;pattern classification;pattern clustering;statistical distributions;ubiquitous computing","Bayesian nonparametric methods;HDP;atomic activity;atomic patterns;clustering techniques;data generative process;data modelling framework;fundamental task;hierarchical Dirichlet processes;high-performance clustering;interaction patterns;latent pattern extraction;machine learning;nonparametric prior distributions;parametric unsupervised learning;pervasive computing;pervasive setting;raw data;reality mining dataset;sensor data;smart pervasive services;smart pervasive systems;social groups;social honest signals;sociometric badges","","7","","37","","","","18-22 March 2013","","IEEE","IEEE Conferences"
"Nonlinear Dimensionality Reduction by Unit Ball Embedding (UBE) and Its Application to Image Clustering","B. H. Soleimani; S. Matwin","Inst. for Big Data Analytics, Dalhousie Univ., Halifax, NS, Canada","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","983","988","The paper presents an unsupervised nonlinear dimensionality reduction algorithm called Unit Ball Embedding (UBE). Many high-dimensional data, such as object or face images, lie on a union of low-dimensional subspaces which are often called manifolds. The proposed method is able to learn the structure of these manifolds by exploiting the local neighborhood arrangement around each point. It tries to preserve the local structure by minimizing a cost function that measures the discrepancy between similarities of points in the high-dimensional data and similarities of points in the low-dimensional embedding. The cost function is proposed in a way that it provides a hyper-spherical representation of points in the low-dimensional embedding. Visualizations of our method on different datasets show that it creates large gaps between the manifolds and maximizes the separability of them. As a result, it notably improves the quality of unsupervised machine learning tasks (e.g. clustering). UBE is successfully applied on image datasets such as faces, handwritten digits, and objects and the results of clustering on the low-dimensional embedding show significant improvement over existing dimensionality reduction methods.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0177","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838281","Dimensionality Reduction;Feature Reduction;Low-Dimensional Embedding;Manifold Learning;Nonlinear Dimensionality Reduction;Representation Learning;Unit Ball Embedding;Unsupervised Learning","Algorithm design and analysis;Big data;Data visualization;Laplace equations;Manifolds;Optimization;Sparse matrices","image representation;pattern clustering;unsupervised learning","UBE;cost function minimization;high-dimensional data;hyper-spherical representation;image clustering;image datasets;local neighborhood arrangement;low-dimensional embedding;low-dimensional subspaces;unit ball embedding;unsupervised machine learning tasks;unsupervised nonlinear dimensionality reduction algorithm","","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conferences"
"Detecting Malicious URLs: A Semi-Supervised Machine Learning System Approach","A. D. Gabriel; D. T. Gavrilut; B. I. Alexandru; P. A. Stefan","Bitdefender Lab., Al.I. Cuza Univ., Iasi, Romania","2016 18th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)","20170126","2016","","","233","239","As malware industry grows, so does the means of infecting a computer or device evolve. One of the most common infection vector is to use the Internet as an entry point. Not only that this method is easy to use, but due to the fact that URLs come in different forms and shapes, it is really difficult to distinguish a malicious URL from a benign one. Furthermore, every system that tries to classify or detect URLs must work on a real time stream and needs to provide a fast response for every URL that is submitted for analysis (in our context a fast response means less than 300-400 milliseconds/URL). From a malware creator point of view, it is really easy to change such URLs multiple times in one day. As a general observation, malicious URLs tend to have a short life (they appear, serve malicious content for several hours and then they are shut down usually by the ISP where they reside in). This paper aims to present a system that analyzes URLs in network traffic that is also capable of adjusting its detection models to adapt to new malicious content. Every correctly classified URL is reused as part of a new dataset that acts as the backbone for new detection models. The system also uses different clustering techniques in order to identify the lack of features on malicious URLs, thus creating a way to improve detection for this kind of threats.","2470-881X;2470881X","Electronic:978-1-5090-5707-8; POD:978-1-5090-5708-5","10.1109/SYNASC.2016.045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829617","big data;data streams;malicious URLs;semi-supervised learning","Adaptation models;Classification algorithms;Data mining;Electronic mail;Feature extraction;Malware;Uniform resource locators","Internet;invasive software;learning (artificial intelligence);pattern clustering","Internet;clustering techniques;infection vector;malicious URL detection;malware industry;network traffic;semisupervised machine learning system;threats","","","","","","","","24-27 Sept. 2016","","IEEE","IEEE Conferences"
"Hadoop cluster monitoring and fault analysis in real time","J. Pinto; P. Jain; T. Kumar","Indian Institute of Information Technology Kota, India","2016 International Conference on Recent Advances and Innovations in Engineering (ICRAIE)","20170608","2016","","","1","6","Failure of a task running on a Hadoop cluster is highly expensive in terms of computational time. A failure occurring even at the end phase of the task may cause the need to redo the entire task. Thus is really important to deploy fault tolerant techniques. Hadoop deploys a technique of checkpointing to prevent data loss. However, computational time-loss still pose a grim threat to critical applications. Hence a solution is proposed that uses SVM models trained with normal cluster resource usage statistics to predict and detect faults occurring in the cluster. The prediction engine can detect faults with minimal time delay and give sufficient time to implement fault tolerance and recovery measures.","5090-2806;50902806","Electronic:978-1-5090-2807-8; POD:978-1-5090-2808-5","10.1109/ICRAIE.2016.7939506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7939506","Big data;Fault tolerance;Hadoop;Machine Learning;SVM","Computational modeling;Fault diagnosis;Fault tolerance;Fault tolerant systems;Monitoring;Support vector machines;Training","checkpointing;data handling;fault tolerant computing;parallel processing;real-time systems;support vector machines;system monitoring","Hadoop cluster fault analysis;Hadoop cluster monitoring analysis;SVM model;data loss prevention;fault detection;fault recovery;fault tolerant technique;normal cluster resource usage statistics","","","","","","","","23-25 Dec. 2016","","IEEE","IEEE Conferences"
"PRIVAaaS: Privacy Approach for a Distributed Cloud-Based Data Analytics Platforms","T. Basso; R. Moraes; N. Antunes; M. Vieira; W. Santos; W. Meira","Univ. of Campinas, Limeira, Brazil","2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)","20170713","2017","","","1108","1116","Data privacy is a key challenge that is exacerbated by Big Data storage and analytics processing requirements. Big Data and Cloud Computing are related and allow the users to access data from any device, making data privacy essential as the data sets are exposed through the web. Organizations care about data privacy as it directly affects the confidence that clients have that their personal data are safe. This paper presents a data privacy approach - PRIVAaaS - and its inte-gration to the LEMONADE Web-based platform, developed to compose ETL (Extract, Transform, Load) process and Machine Learning workflows. The 3-level approach of PRIVAaaS, based on data anonymization policies, is implemented in a software toolkit that provides a set of libraries and tools which allows controlling and reducing data leakage in the context of Big Data processing.","","Electronic:978-1-5090-6611-7; POD:978-1-5090-5980-5","10.1109/CCGRID.2017.136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7973820","LEMONADE;anonymization;cloud-based data analytics platform;data privacy","Big Data;Cloud computing;Data analysis;Data models;Data privacy;Privacy;Tools","Big Data;cloud computing;data analysis;data privacy;learning (artificial intelligence);organisational aspects;storage management","Big Data analytics;Big Data storage;ETL;LEMONADE Web-based platform;PRIVAaaS;cloud computing;data anonymization policies;data leakage reduction;data privacy;distributed cloud;extract transform load;machine learning;organizations care","","","","","","","","14-17 May 2017","","IEEE","IEEE Conferences"
"An insight into tree based machine learning techniques for big data analytics using Apache Spark","A. Sheshasaayee; J. V. N. Lakshmi","Department of Computer Science, Quaid - E - Millath Govt College for Women, Chennai, India","2017 International Conference on Intelligent Computing, Instrumentation and Control Technologies (ICICICT)","20180423","2017","","","1740","1743","Data Analysis, Classification and Regression on Big Data using machine learning algorithms in a novel approach used in various science and medical streams. Map Reduce is widely used frame work to parallelize machine learning algorithms. These algorithms are tuned to attain best outcomes. This techniques uses lots of time for processing map reduce model multiple times by tuning the parameters as per the requirement. To achieve the shortest time consumption for tuning the jobs, Apache Spark based model is proposed for optimizing the assignments. This model is to predict the temperature from existing data by training using tree based machine learning techniques. This model replaces the map reduce by spark for implementing the best prediction result. The prediction outcomes computed are compared on tree structured ML methods with respect to time and space utilization.","","Electronic:978-1-5090-6106-8; POD:978-1-5090-6107-5","10.1109/ICICICT1.2017.8342833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8342833","Apache Spark;Big Data;Data Analysis;Decision Tree;Gradient Boosted tree;Machine Learning;Prediction;Random Forest","Big Data;Computational modeling;Decision trees;Machine learning;Machine learning algorithms;Sparks;Vegetation","Big Data;cluster computing;data analysis;learning (artificial intelligence);parallel processing;trees (mathematics)","Apache Spark based model;Data Analysis;MapReduce;big data analytics;space utilization;tree based machine learning techniques","","","","","","","","6-7 July 2017","","IEEE","IEEE Conferences"
"Real-time predictive maintenance for wind turbines using Big Data frameworks","M. Canizo; E. Onieva; A. Conde; S. Charramendieta; S. Trujillo","IK4-Ikerlan Technology Research Centre, Big Data Architectures Area, Po. J. M. Arizmendiarrieta, 2. 20500 Arrasate-Mondragn, Spain","2017 IEEE International Conference on Prognostics and Health Management (ICPHM)","20170803","2017","","","70","77","This work presents the evolution of a solution for predictive maintenance to a Big Data environment. The proposed adaptation aims for predicting failures on wind turbines using a data-driven solution deployed in the cloud and which is composed by three main modules. (i) A predictive model generator which generates predictive models for each monitored wind turbine by means of Random Forest algorithm. (ii) A monitoring agent that makes predictions every 10 minutes about failures in wind turbines during the next hour. Finally, (iii) a dashboard where given predictions can be visualized. To implement the solution Apache Spark, Apache Kafka, Apache Mesos and HDFS have been used. Therefore, we have improved the previous work in terms of data process speed, scalability and automation. In addition, we have provided fault-tolerant functionality with a centralized access point from where the status of all the wind turbines of a company localized all over the world can be monitored, reducing O&M costs.","","Electronic:978-1-5090-5710-8; POD:978-1-5090-5711-5","10.1109/ICPHM.2017.7998308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7998308","Big Data architectures;Cloud computing;Industry 4.0;Machine learning;Wind power","Big Data;Cloud computing;Companies;Predictive maintenance;Predictive models;Wind turbines","Big Data;cloud computing;failure analysis;learning (artificial intelligence);maintenance engineering;power engineering computing;wind turbines","Apache Kafka;Apache Mesos;Apache Spark;Big Data environment;HDFS;O and M cost reduction;centralized access point;data process speed;data-driven solution;fault-tolerant functionality;monitoring agent;predictive model generator;random forest algorithm;real-time predictive maintenance;wind turbines","","","","","","","","19-21 June 2017","","IEEE","IEEE Conferences"
"Survey on Big data Analytics for digital world","S. Gaikwad; P. Nale; R. Bachate","Department of Computer Engineering, JSCOE, Hadapsar, Pune, India","2016 IEEE International Conference on Advances in Electronics, Communication and Computer Technology (ICAECCT)","20170608","2016","","","180","186","Every day quintillion bytes of data is generated; 90% of which has been created in the last two years alone, from this it can predict the amount of data that will be generated in future. It is necessary, to introduce some techniques for analyzing such a huge amount data to face the challenges and to deal with limitation of `Big data Analysis'. Such Big Data analysis can be used nearly in every aspect of our modern society, including mobile services, retail and consumer services, manufacturing, business intelligence, financial services and robotics. The motive of this paper is to understand how big data is generated and the necessity of analyzing such data. This paper also gives a short glimpse of Big Data analysis implications in the real world and its role in every field along with challenges and advantages. This paper also explores various techniques, algorithms, systems of big data analytics in various sectors of digital world.","","Electronic:978-1-5090-3662-2; POD:978-1-5090-3663-9","10.1109/ICAECCT.2016.7942579","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7942579","Big Data;Big Data Analysis;Data Mining;Informatics;Security;Tools","Algorithm design and analysis;Big Data;Clustering algorithms;Computers;Data mining;Machine learning algorithms;Tools","Big Data;data analysis","Big Data analytics;business intelligence;consumer services;digital world;financial services;manufacturing services;mobile services;retail services;robotics","","","","","","","","2-3 Dec. 2016","","IEEE","IEEE Conferences"
"A novel approach for feature selection based on MapReduce for biomarker discovery","A. Kourid; M. Batouche","Computer Science Department, College of NTIC, Constantine University 2, 25000 Constantine, Algeria","International Conference on Computer Vision and Image Analysis Applications","20151210","2015","","","1","11","Scale feature selection is one of the most important fields in the big data domain that can solve real data problems, such as bioinformatics, when it is necessary to process huge amount of data. The efficiency of existing feature selection algorithms significantly downgrades, if not totally inapplicable, when data size exceeds hundreds of gigabytes, because most feature selection algorithms are designed for centralized computing architecture. For that distributed computing techniques, such as MapReduce can be applied to handle very large data. Our approach is to scale the existing method for feature selection, Kmeans clustering and Signal to Noise Ratio (SNR) combined with optimization technique as Binary Particle Swarm Optimization (BPSO). The proposed method is divided into two stages. In the first stage, we have used parallel Kmeans on MapReduce for clustering features, and then we have applied iterative MapReduce that implement parallel SNR ranking for each cluster, after we have selected the top ranked feature from each cluster. The top scored features from each cluster are gathered and a new feature subset is generated. In the second stage the new feature subset is used as input to the novel BPSO proposed based on MapReduce and optimized feature subset is being produced. The proposed method is implemented in a distributed environment, and its efficiency is illustrated through analyzing practical problems such as biomarker discovery.","","Electronic:978-1-4799-7186-2; POD:978-1-4799-7187-9","10.1109/ICCVIA.2015.7351888","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351888","Big Data;Bioinformatics;Biomarker Discovery;Feature selection;Scale Machine Learning","Bioinformatics;Clustering algorithms;Computational modeling;Data models;Signal to noise ratio;Support vector machines;Systems architecture","Big Data;bioinformatics;feature selection;learning (artificial intelligence);parallel processing;particle swarm optimisation","BPSO;Big data domain;K-means clustering;MapReduce;SNR;binary particle swarm optimization;biomarker discovery;centralized computing architecture;distributed computing technique;feature selection algorithm;optimization technique;parallel Kmeans clustering features;parallel SNR ranking;signal-to-noise ratio","","","","24","","","","18-20 Jan. 2015","","IEEE","IEEE Conferences"
"Email Spam filtering using BPNN classification algorithm","S. K. Tuteja; N. Bogiri","Dept. of Computer Engineering, K.J. College of Engineering and Management Research, Pune, India","2016 International Conference on Automatic Control and Dynamic Optimization Techniques (ICACDOT)","20170316","2016","","","915","919","Millions of people use email correspondence for communication across the globe and it is a critically vital application for many businesses. Considerable amount of unsolicited mail flows into user's mail boxes on a daily basis. A major negative aspect since the past decade has been bulk spam or phishing mail. Besides such unsolicited spam emails being wearisome for many email users, it also puts pressure on the IT infrastructure of organizations and costs businesses billions of dollars in lost efficiency. Increasing need of effectively filtering spam has become vital. We thus use BPNN filtering algorithm i.e. Artificial Neural Network Feed forward with Back Propagation, which is based on text classification to classify significant emails from unsolicited ones.","","Electronic:978-1-5090-2080-5; POD:978-1-5090-2081-2","10.1109/ICACDOT.2016.7877720","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877720","Spam email detection;feature selection;machine learning","Clustering algorithms;Feature extraction;Neural networks;Neurons;Training;Unsolicited electronic mail","backpropagation;business data processing;computer crime;e-mail filters;feature selection;feedforward neural nets;pattern classification;unsolicited e-mail","BPNN classification;BPNN filtering algorithm;artificial feedforward neural network;bulk spam;email spam filtering;feature selection;machine learning;phishing mail;unsolicited mail","","","","","","","","9-10 Sept. 2016","","IEEE","IEEE Conferences"
"Anomaly detection in network traffic using K-mean clustering","R. Kumari; Sheetanshu; M. K. Singh; R. Jha; N. K. Singh","Dept. of Computer Sc. & Engineering, B.I.T, Mesra, Ranchi, India","2016 3rd International Conference on Recent Advances in Information Technology (RAIT)","20160709","2016","","","387","393","With the advancement of digital age and internet technologies cyber-attacks increasingly have been prompting the news headlines. These attacks exploit a network's short comings to gain unauthorized access to the sensitive information or sometimes just create a flood to prevent legitimate users from accessing it. In any case intrusion of the network plays a key role before the execution of any attack. In this paper we will discuss a how these intrusions can be detected with k-means clustering based machine learning approach using big data analytical techniques and put forward the experimental results to prevent attacks at it's very core.","","Electronic:978-1-4799-8579-1; POD:978-1-4799-8580-7","10.1109/RAIT.2016.7507933","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7507933","Anomaly detection;Big data;Spark;k-mean;network intrusion","Clustering algorithms;Computers;Information technology;Security;Sparks;Telecommunication traffic;Unsupervised learning","Big Data;learning (artificial intelligence);pattern clustering;security of data","K-mean clustering;anomaly detection;big data analytical techniques;internet technologies cyber-attacks;machine learning approach;network traffic;sensitive information;unauthorized access","","1","","","","","","3-5 March 2016","","IEEE","IEEE Conferences"
"A Proposal of a Methodological Framework with Experimental Guidelines to Investigate Clustering Stability on Financial Time Series","G. Marti; P. Very; P. Donnat; F. Nielsen","Hellebore Capital Manage., France","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","32","37","We present in this paper an empirical framework motivated by the practitioner point of view on stability. The goal is to both assess clustering validity and yield market insights by providing through the data perturbations we propose a multi-view of the assets' clustering behaviour. The perturbation framework is illustrated on an extensive credit default swap time series database available online at www.datagrapple.com.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424282","Clustering;Credit Default Swap;Financial Markets;Stability;Time Series","Clustering algorithms;Correlation;Databases;Portfolios;Stability criteria;Time series analysis","financial data processing;pattern clustering;time series","assets clustering behaviour;clustering stability;clustering validity assessment;credit default swap time series database;data perturbations;experimental guidelines;financial time series;methodological framework;yield market insights assessment","","2","","22","","","","9-11 Dec. 2015","","IEEE","IEEE Conferences"
"Benchmarking Harp-DAAL: High Performance Hadoop on KNL Clusters","L. Chen; B. Peng; B. Zhang; T. Liu; Y. Zou; L. Jiang; R. Henschel; C. Stewart; Z. Zhang; E. McCallum; Z. Tom; O. Jon; J. Qiu","Sch. of Inf. & Comput., Indiana Univ., Bloomington, IN, USA","2017 IEEE 10th International Conference on Cloud Computing (CLOUD)","20170911","2017","","","82","89","Data analytics is undergoing a revolution in many scientific domains, and demands cost-effective parallel data analysis techniques. Traditional Java-based Big Data processing tools like Hadoop MapReduce are designed for commodity CPUs. In contrast, emerging manycore processors like the Xeon Phi have an order of magnitude greater computation power and memory bandwidth. To harness their computing capabilities, we propose the Harp-DAAL framework. We show that enhanced versions of MapReduce can be replaced by Harp, a Hadoop plug-in, that offers useful data abstractions for both high-performance iterative computation and MPI-quality communication, as well as drive Intel's native DAAL library. We select a subset of three machine learning algorithms and implement them within Harp-DAAL. Our scalability benchmarks ran on Knights Landing (KNL) clusters and achieved up to 2.5 times speedup of performance over the HPC solution in NOMAD and 15 to 40 times speedup over Java-based solutions in Spark. We further quantify the workloads on single node KNL with a performance breakdown at the micro-architecture level.","","Electronic:978-1-5386-1993-3; POD:978-1-5386-1994-0; USB:978-1-5386-1992-6","10.1109/CLOUD.2017.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030575","BigData;HPC;Xeon Phi","Data conversion;Data structures;Instruction sets;Java;Kernel;Libraries;Machine learning algorithms","Big Data;Java;data analysis;iterative methods;learning (artificial intelligence);message passing;multiprocessing systems;parallel processing","Data analytics;HPC solution;Hadoop MapReduce;Harp-DAAL framework;Intel native DAAL library;Java-based Big Data processing;KNL clusters;Knights Landing clusters;MPI-quality communication;NOMAD;Spark;Xeon Phi;commodity CPUs;cost-effective parallel data analysis techniques;data abstractions;high performance Hadoop;iterative computation;machine learning algorithms;manycore processors;memory bandwidth;performance breakdown;scalability benchmarks","","","","","","","","25-30 June 2017","","IEEE","IEEE Conferences"
"Batch clustering algorithm for big data sets","R. Alguliyev; R. Aliguliyev; A. Bagirov; R. Karimov","Institute of Information Technology of Azerbaijan National Academy of Sciences, Baku, Azerbaijan","2016 IEEE 10th International Conference on Application of Information and Communication Technologies (AICT)","20170727","2016","","","1","4","Vast spread of computing technologies has led to abundance of large data sets. Today tech companies like, Google, Facebook, Twitter and Amazon handle big data sets and log terabytes, if not petabytes, of data per day. Thus, there is a need to find similarities and define groupings among the elements of these big data sets. One of the ways to find these similarities is data clustering. Currently, there exist several data clustering algorithms which differ by their application area and efficiency. Increase in computational power and algorithmic improvements have reduced the time for clustering of big data sets. But it usually happens that big data sets can't be processed whole due to hardware and computational restrictions. In this paper, the classic k-means clustering algorithm is compared to the proposed batch clustering (BC) algorithm for the required computation time and objective function. The BC algorithm is designed to cluster large data sets in batches but maintain the efficiency and quality. Several experiments confirm that batch clustering algorithm for big data sets is more efficient in using computational power, data storage and results in better clustering compared to k-means algorithm. The experiments are conducted with the data set of 2 (two) million two-dimensional data points.","","Electronic:978-1-5090-1841-3; POD:978-1-5090-1842-0","10.1109/ICAICT.2016.7991657","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7991657","Batch Clustering;Big Data;Big Data Clustering;Clustering Algorithms;k-means","Algorithm design and analysis;Big Data;Classification algorithms;Clustering algorithms;Linear programming;Machine learning algorithms;Partitioning algorithms","Big Data;pattern clustering;storage management","Amazon;BC;Facebook;Google;Twitter;algorithmic improvements;batch clustering algorithm;big data sets;computational power;computational restrictions;computing technologies;data clustering;data storage;k-means clustering algorithm;tech companies;terabytes;two-dimensional data points","","","","","","","","12-14 Oct. 2016","","IEEE","IEEE Conferences"
"Algebraic conditions for generating accurate adjacency arrays","K. Dibert; H. Jansen; J. Kepner","MIT Lincoln Laboratory, Lexington, MA","2015 IEEE MIT Undergraduate Research Technology Conference (URTC)","20160912","2015","","","1","4","Data processing systems impose multiple views on data as it is processed by the system. These views include spreadsheets, databases, matrices, and graphs. Associative arrays unify and simplify these different approaches into a common two-dimensional view of data. Graph construction, a fundamental operation in the data processing pipeline, is typically done by multiplying the incidence array representations of a graph, Ein and Eout, to produce an adjacency matrix of the graph that can be processed with a variety of machine learning clustering techniques. This work focuses on establishing the mathematical criteria to ensure that the matrix product E out Ein is the adjacency array of the graph. It will then be shown that these criteria are also necessary and sufficient for the remaining nonzero product of incidence arrays, E in Eout to be the adjacency matrices of the reversed graph. Algebraic structures that comply with the criteria will be identified and discussed.","","","10.1109/URTC.2015.7563745","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7563745","","Additives;Arrays;Data processing;Databases;Mathematics;Pipelines;Standards","data handling;data structures;graph theory;learning (artificial intelligence);matrix algebra;pattern clustering","adjacency array;adjacency matrix;algebraic condition;data processing system;graph construction;incidence array representation;machine learning clustering technique;matrix product","","1","","","","","","7-8 Nov. 2015","","IEEE","IEEE Conferences"
"Prediction of wind turbine blades icing based on MBK-SMOTE and random forest in imbalanced data set","Y. Ge; D. Yue; L. Chen","School of Automation Engineering, Nanjing University of Posts and Telecommunications, Nanjing, 210023, P. R. China","2017 IEEE Conference on Energy Internet and Energy System Integration (EI2)","20180104","2017","","","1","6","The icing problem of wind turbine blades caused by low temperature environment poses a serious threat to the power generation performance of wind turbines. The serious im-balanced real-time data of the wind turbine makes the traditional machine learning algorithm unable to make a good prediction for blades icing problem. To solve the problem, this paper proposes an improved SMOTE algorithm (MBK-SMOTE) which is based on the standard SMOTE algorithm. MBK-SMOTE makes samples distribution balanced by introducing the concept of sample's density and dividing the concentration area. Then we use the Random Forest algorithm to predict the icing event of wind turbine blades. Comparing the experiments among MBK-SMOTE+RF, standard SMOTE+RF and improved SMOTE+RF, we finally found that MBK-SMOTE+RF is more effective, and the prediction effect of MBK-SMOTE+RF is superior to other methods.","","Electronic:978-1-5386-1427-3; POD:978-1-5386-1428-0","10.1109/EI2.2017.8245530","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8245530","Im-balanced data processing;MBK-SMOTE;MiniBatchK-means;Random Forest algorithm;SMOTE;Wind turbine blades frozen","Blades;Clustering algorithms;Interpolation;Machine learning algorithms;Standards;Training;Wind turbines","blades;ice;learning (artificial intelligence);mechanical engineering computing;pattern classification;sampling methods;wind turbines","MBK-SMOTE+RF;icing problem;improved SMOTE algorithm;over-sampling method;random forest algorithm;standard SMOTE algorithm;wind turbine blades","","","","","","","","26-28 Nov. 2017","","IEEE","IEEE Conferences"
"A Parallel and Incremental Approach for Data-Intensive Learning of Bayesian Networks","K. Yue; Q. Fang; X. Wang; J. Li; W. Liu","Department of Computer Science and EngineeringSchool of Information Science and Engineering, Yunnan University, Kunming, China","IEEE Transactions on Cybernetics","20170520","2015","45","12","2890","2904","Bayesian network (BN) has been adopted as the underlying model for representing and inferring uncertain knowledge. As the basis of realistic applications centered on probabilistic inferences, learning a BN from data is a critical subject of machine learning, artificial intelligence, and big data paradigms. Currently, it is necessary to extend the classical methods for learning BNs with respect to data-intensive computing or in cloud environments. In this paper, we propose a parallel and incremental approach for data-intensive learning of BNs from massive, distributed, and dynamically changing data by extending the classical scoring and search algorithm and using MapReduce. First, we adopt the minimum description length as the scoring metric and give the two-pass MapReduce-based algorithms for computing the required marginal probabilities and scoring the candidate graphical model from sample data. Then, we give the corresponding strategy for extending the classical hill-climbing algorithm to obtain the optimal structure, as well as that for storing a BN by <;key, value> pairs. Further, in view of the dynamic characteristics of the changing data, we give the concept of influence degree to measure the coincidence of the current BN with new data, and then propose the corresponding two-pass MapReduce-based algorithms for BNs incremental learning. Experimental results show the efficiency, scalability, and effectiveness of our methods.","2168-2267;21682267","","10.1109/TCYB.2015.2388791","Key Program of Natural Science Foundation of Yunnan Province; National Basic Research (973) Program of China; Program for Innovative Research Team in Yunnan University; Program for New Century Excellent Talents in China; 10.13039/501100001809 - National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7018001","Bayesian network learning;MapReduce;data-intensive computing;incremental learning;parallel algorithm;uncertain knowledge","Algorithm design and analysis;Computational modeling;Data models;Distributed databases;Heuristic algorithms;Parallel algorithms;Probability","Big Data;belief networks;inference mechanisms;learning (artificial intelligence);parallel algorithms;probability","BN incremental learning;Bayesian networks;artificial intelligence;big data;candidate graphical model;classical hill-climbing algorithm;cloud environment;data-intensive computing;data-intensive learning;distributed data;dynamically changing data;incremental approach;knowledge inference;machine learning;marginal probability;massive data;minimum description length;parallel approach;probabilistic inference;scoring algorithm;scoring metric;search algorithm;two-pass MapReduce-based algorithm;uncertain knowledge representation","","10","","40","","","20150122","Dec. 2015","","IEEE","IEEE Journals & Magazines"
"A survey on evolutionary machine learning algorithms for multi-dimensional data classification","Swapna C; R. S. Shaji","Department of Computer Applications, Noorul Islam University, Tamil Nadu, India","2015 International Conference on Control, Instrumentation, Communication and Computational Technologies (ICCICCT)","20160523","2015","","","781","785","The paper presents an analysis of various knowledge discovery estimation methods performed using different methods. Knowledge discovery approach addresses the problem of estimating outlier materials in the presence of abnormal information within the case that very little previous knowledge is offered concerning the nature of the information, the distortion, or the noise. The paper describes a detailed study on various techniques for outlier analysis and the issues associated with individual operations. In some data set an object may be a single point. The distribution of data in such objects is not taken into account, in traditional clustering algorithms. In this paper, divergence approaches are applied for comparing similarity between uncertain objects in continuous and discrete cases. To cluster uncertain objects, integration is done in density-based and partitioning clustering strategies. The proposed paper outlines basic concepts behind several developments, their assumptions and identifiably conditions needed by these approaches along with the algorithm characteristics. The proposed paper illustrates the comparison between approaches and strategies to estimate the novel outlier analysis algorithm for very large database analysis.","","Electronic:978-1-4673-9825-1; POD:978-1-4673-9826-8","10.1109/ICCICCT.2015.7475385","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7475385","Abnormal Information;Big Data analysis;Computational approach;Distortion;Outlier","Algorithm design and analysis;Brain modeling;Clustering algorithms;Context;Diseases;Medical diagnostic imaging","data mining;evolutionary computation;learning (artificial intelligence);pattern classification;pattern clustering","clustering algorithms;continuous cases;density-based strategies;discrete cases;evolutionary machine learning algorithms;knowledge discovery estimation methods;multidimensional data classification;outlier analysis algorithm;outlier materials;partitioning clustering strategies","","","","19","","","","18-19 Dec. 2015","","IEEE","IEEE Conferences"
"Nomadic Computing for Big Data Analytics","H. F. Yu; C. J. Hsieh; H. Yun; S. V. N. Vishwanathan; I. Dhillon","University of Texas at Austin","Computer","20160413","2016","49","4","52","60","Analyzing the massive datasets of today's applications will require scalable and sophisticated machine-learning methods. NOMAD, a novel nomadic framework, combines two common approaches: stochastic optimization and distributed computing.","0018-9162;00189162","","10.1109/MC.2016.116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7452314","Web analytics;big data;computing;distributed algorithms;nomadic algorithms","Big data;Charge coupled devices;Inference algorithms;Motion pictures;Partitioning algorithms;Scalability;Stochastic processes","Big Data;data analysis;distributed processing;learning (artificial intelligence);stochastic programming","big data analytics;distributed computing;machine-learning methods;nomadic computing;stochastic optimization","","1","","15","","","","Apr. 2016","","IEEE","IEEE Journals & Magazines"
"An improved optimized clustering technique for crime detection","N. Tomar; A. K. Manjhvar","Dept. of CSE/IT, MITS Gwalior, India","2016 Symposium on Colossal Data Analysis and Networking (CDAN)","20160919","2016","","","1","5","Data mining automates the finding predictive records procedure in big databases. Clustering is a most famous method in data mining and is an important methodology that is performed based on the similarity principle. The segregation of a big database is a stimulating and task of time consuming. It concludes two different stages: first, feature extraction maps all documents or record to a point in the space of high-dimensional, then algorithms for clustering automatically grouping the points into a cluster hierarchy. Clustering has various applications in different fields. Few of the fields include criminology, text mining, image resolution, machine learning. Crime detection has become one of the most attractive field as the crime rate in India and whole world is increasing at a greater pace. We as citizens of a country have to contribute towards its detection and removal. Thus, a comprehensive survey carried about the basics of clustering has given in this paper. Moreover, proposed work was given that gives the idea of the work going to be done in the upcoming time.","","","10.1109/CDAN.2016.7570880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7570880","Clustering;crime;data mining;k-means method optimization;partitioning","Algorithm design and analysis;Clustering algorithms;Clustering methods;Data analysis;Data mining;Databases;Partitioning algorithms","data mining;feature extraction;pattern clustering;police data processing","India;big databases;cluster hierarchy;crime detection;crime rate;criminology;data mining;feature extraction;high-dimensional space;image resolution;machine learning;optimized clustering;similarity principle;text mining","","","","","","","","18-19 March 2016","","IEEE","IEEE Conferences"
"Patient-individual morphological anomaly detection in multi-lead electrocardiography data streams","A. Acker; F. Schmidt; A. Gulenko; R. Kietzmann; O. Kao","Technische Universit&#x00E4;t Berlin (TU Berlin, Complex and Distributed IT Systems (CIT)), 10587 Berlin, Germany","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","3841","3846","Cardiac diseases like myocardial infarction, which possibly result in cardiac death, are still a relevant topic. To achieve recognitions in early stages, long term ECG monitoring devices are used. Such devices produce large amounts of data, either directly streamed or stored in databases. Manually analysing this data by experts is inefficient. Thus, automated preprocessing methods are needed to minimize the temporal effort dedicated to the inspection. The proposed method helps to identify morphological anomalies within the ECG data stream. It determines a set of meaningful time series features based on a Kolmogorov-Smirnov test (KST) and after that, applies the BICO online clustering algorithm. Thereby, the system learns the patient-individual PQRST-complex segment morphologies and after that, uses the learned models for detecting anomalies within the ECG data stream. For evaluation, real world patient data was used, which was previously tagged by electrophysiologists. As a result, the KST selected set of features was revealed to be especially suitable for analysing ECG data streams, resulting in average sensitivity rates of 98.82% and average specificity rates of 98.13%.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258387","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258387","ECG data streams;anomaly detection;clustering;machine learning;time seriesx","Anomaly detection;Cardiac disease;Electrocardiography;Feature extraction;Monitoring;Morphology;Time series analysis","diseases;electrocardiography;feature extraction;medical signal detection;medical signal processing;muscle;patient monitoring;pattern clustering;time series","BICO online clustering algorithm;ECG data stream;Kolmogorov-Smirnov test;automated preprocessing methods;cardiac death;cardiac diseases;electrophysiologists;long term ECG monitoring devices;morphological anomalies;multilead electrocardiography data streams;myocardial infarction;patient-individual PQRST-complex segment morphologies;patient-individual morphological anomaly detection;time series;world patient data","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Using Big Data Analytics for Authorship Authentication of Arabic Tweets","J. Albadarneh; B. Talafha; M. Al-Ayyoub; B. Zaqaibeh; M. Al-Smadi; Y. Jararweh; E. Benkhelifa","Jordan Univ. of Sci. & Technol., Irbid, Jordan","2015 IEEE/ACM 8th International Conference on Utility and Cloud Computing (UCC)","20160314","2015","","","448","452","Authorship authentication of a certain text is concerned with correctly attributing it to its author based on its contents. It is a very important problem with deep root in history as many classical texts have doubtful attributions. The information age and ubiquitous use of the Internet is further complicating this problem and adding more dimensions to it. We are interested in the modern version of this problem where the text whose authorship needs authentication is an online text found in online social networks. Specifically, we are interested in the authorship authentication of tweets. This is not the only challenging aspect we consider here. Another challenging aspect is the language of the tweets. Most current works and existing tools support English. We chose to focus on the very important, yet largely understudied, Arabic language. Finally, we add another challenging aspect to the problem at hand by addressing it at a very large scale. We present our effort to employ big data analytics to address the authorship authentication problem of Arabic tweets. We start by crawling a dataset of more than 53K tweets distributed across 20 authors. We then use preprocessing steps to clean the data and prepare it for analysis. The next step is to compute the feature vectors of each tweet. We use the Bag-Of-Words (BOW) approach and compute the weights using the Term Frequency-Inverse Document Frequency (TF-IDF). Then, we feed the dataset to a Naive Bayes classifier implemented on a parallel and distributed computing framework known as Hadoop. To the best of our knowledge, none of the previous works on authorship authentication of Arabic text addressed the unique challenges associated with (1) tweets and (2) large-scale datasets. This makes our work unique on many levels. The results show that the testing accuracy is not very high (61.6%), which is expected in the very challenging setting that we consider.","","Electronic:978-0-7695-5697-0; POD:978-1-5090-0343-3","10.1109/UCC.2015.80","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7431455","","Algorithm design and analysis;Authentication;Big data;Clustering algorithms;Electronic mail;Machine learning algorithms;Support vector machines","Big Data;natural language processing;social networking (online);text analysis","Arabic language;Arabic text;Arabic tweets;BOW approach;Bag-Of-Words approach;Big Data analytics;Hadoop;Internet;Naive Bayes classifier;TF-IDF;authorship authentication problem;classical texts;distributed computing framework;information age;online social networks;online text;term frequency-inverse document frequency;ubiquitous use","","1","","29","","","","7-10 Dec. 2015","","IEEE","IEEE Conferences"
"Scalable developments for big data analytics in remote sensing","G. Cavallaro; M. Riedel; C. Bodenstein; P. Glock; M. Richerzhagen; M. Goetz; J. A. Benediktsson","Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","20151112","2015","","","1366","1369","Big Data Analytics methods take advantage of techniques from the fields of data mining, machine learning, or statistics with a focus on analysing large quantities of data (aka `big datasets') with modern technologies. Big data sets appear in remote sensing in the sense of large volumes, but also in the sense of an ever increasing amount of spectral bands (i.e., high-dimensional data). The remote sensing has traditionally used the above described techniques for a wide variety of application such as classification (e.g., land cover analysis using different spectral bands from satellite data), but more recently scalability challenges occur when using traditional (often serial) methods. This paper addresses observed scalability limits when using support vector machines (SVMs) for classification and discusses scalable and parallel developments used in concrete application areas of remote sensing. Different approaches that are based on massively parallel methods are discussed as well as recent developments in parallel methods.","2153-6996;21536996","Electronic:978-1-4799-7929-5; POD:978-1-4799-7930-1; USB:978-1-4799-7928-8","10.1109/IGARSS.2015.7326030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326030","Classification;Parallel Computing;Remote Sensing;Scalability;Support Vector Machines","Accuracy;Big data;Graphics processing units;Kernel;Remote sensing;Scalability;Support vector machines","Big Data;data mining;learning (artificial intelligence);parallel processing;remote sensing;statistical analysis;support vector machines","Big data analytic method;SVM;data mining;machine learning;parallel method;remote sensing;scalable development;statistical analysis;support vector machine","","2","","19","","","","26-31 July 2015","","IEEE","IEEE Conferences"
"Implementation of a self-adaptive real time recommendation system using spark machine learning libraries","B. K. Sunny; P. S. Janardhanan; A. B. Francis; R. Murali","Department of Computer Science and Engineering, Rajiv Gandhi Institute of Technology, Kottayam, Kerala","2017 IEEE International Conference on Signal Processing, Informatics, Communication and Energy Systems (SPICES)","20171102","2017","","","1","7","Real time recommendation systems have become an essential component of e-commerce web applications. With increasing volume and velocity of data handled by these applications, known as the bigdata problem, traditional recommendation systems that analyze data and update models at regular time intervals would not be able to satisfy this requirement. With the evolution of technologies for processing bigdata in real time, it has become fairly easy to implement real time recommendation systems. Stream-computing is a new computing paradigm for handling the velocity attribute of bigdata which makes it possible to develop real time bigdata applications. This paper gives the details of implementation of a real time recommendation system using Apache Spark, a widely used platform for stream computing. This system is implemented for recommending TV channels to viewers in real time. This becomes a challenging task due to continuous changes in the set of available channels and the context dependent preference of viewers. In channel recommendation scenario, characterized by its dynamic nature, volume of data, and tight time constraints, traditional approaches cannot be used. We have implemented a highly scalable TV channel recommendation system optimized for the processing of real-time data streams originating from set-top boxes. The proposed system implements a self-adaptive approach for model building. The system effectively uses distributed processing power of Apache Spark to make recommendations in real time with scalability to meet the real time constraints with increasing load. The Spark Machine Learning Libraries (Spark MLLib) provide several algorithms which were used for developing the proposed recommendation system. The large amount of data in the system is efficiently managed by the data processing method of Lambda Architecture.","","Electronic:978-1-5386-3864-4; POD:978-1-5386-3865-1","10.1109/SPICES.2017.8091310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8091310","Machine Learning;Real-Time Recommendation Engine;Spark Streaming","Data models;Libraries;Machine learning algorithms;Real-time systems;Sparks;TV;Training","Big Data;data analysis;electronic commerce;learning (artificial intelligence);real-time systems;recommender systems","Apache Spark;TV channel recommendation system;big data applications;channel recommendation scenario;e-commerce Web applications;real-time data streams;self-adaptive real time recommendation system;spark machine learning libraries;stream-computing","","","","","","","","8-10 Aug. 2017","","IEEE","IEEE Conferences"
"A Comparison of Distributed Machine Learning Platforms","K. Zhang; S. Alqahtani; M. Demirbas","Dept. of Comput. Sci. & Eng., Univ. at Buffalo, New York, NY, USA","2017 26th International Conference on Computer Communication and Networks (ICCCN)","20170918","2017","","","1","9","The proliferation of big data and big computing boosted the adoption of machine learning across many application domains. Several distributed machine learning platforms emerged recently. We investigate the architectural design of these distributed machine learning platforms, as the design decisions inevitably affect the performance, scalability, and availability of those platforms. We study Spark as a representative dataflow system, PMLS as a parameter- server system, and TensorFlow and MXNet as examples of more advanced dataflow systems. We take a distributed systems perspective, and analyze the communication and control bottlenecks for these approaches. We also consider fault-tolerance and ease-of-development in these platforms. In order to provide a quantitative evaluation, we evaluate the performance of these three systems with basic machine learning tasks: logistic regression, and an image classification example on the MNIST dataset.","","Electronic:978-1-5090-2991-4; POD:978-1-5090-2992-1; USB:978-1-5090-2990-7","10.1109/ICCCN.2017.8038464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8038464","","Computational modeling;Computer architecture;Data models;Machine learning algorithms;Servers;Sparks;Training","distributed processing;learning (artificial intelligence);software architecture;software fault tolerance","MXNet;PMLS;Spark;TensorFlow;architectural design;dataflow system;design decisions;distributed machine learning;fault-tolerance;parameter- server system","","","","","","","","July 31 2017-Aug. 3 2017","","IEEE","IEEE Conferences"
"Sparse Multigraph Embedding for Multimodal Feature Representation","S. Wang; W. Guo","College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China","IEEE Transactions on Multimedia","20170615","2017","19","7","1454","1466","Data fusion is used to integrate features from heterogeneous data sources into a consistent and accurate representation for certain learning tasks. As an effective technique for data fusion, unsupervised multimodal feature representation aims to learn discriminative features, indicating the improvement of classification and clustering performance of learning algorithms. However, it is a challenging issue since varying modality favors different structural learning. In this paper, we propose an efficient feature learning method to represent multimodal images as a sparse multigraph structure embedding problem. First, an effective algorithm is proposed to learn a sparse multigraph construction from multimodal data, where each modality corresponds to one regularized graph structure. Second, incorporating the learned multigraph structure, the feature learning problem for multimodal images is formulated as a form of matrix factorization. An efficient corresponding algorithm is developed to optimize the problem and its convergence is also proved. Finally, the proposed method is compared with several state-of-the-art single-modal and multimodal feature learning techniques in eight publicly available face image datasets. Comprehensive experimental results demonstrate that the proposed method outperforms the existing ones in terms of clustering performance for all tested datasets.","1520-9210;15209210","","10.1109/TMM.2017.2663324","Fujian Collaborative Innovation Center for Big Data Application in Governments; Technology Innovation Platform Project of Fujian Province; 10.13039/501100001809 - National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839979","Feature fusion;graph embedding;machine learning;multimodal data;sparse representation","Clustering algorithms;Correlation;Data integration;Feature extraction;Learning systems;Optimization;Sparse matrices","graph theory;image fusion;image representation;learning (artificial intelligence);matrix decomposition","data fusion;feature learning;heterogeneous data sources;learning tasks;matrix factorization;multimodal data;multimodal image representation;multimodal images;regularized graph structure;sparse multigraph construction;sparse multigraph embedding;sparse multigraph structure embedding problem;structural learning;unsupervised multimodal feature representation;varying modality","","2","","","","","20170202","July 2017","","IEEE","IEEE Journals & Magazines"
"Sparse Computation for Large-Scale Data Mining","D. S. Hochbaum; P. Baumann","Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA","IEEE Transactions on Big Data","20170520","2016","2","2","151","174","Leading machine learning techniques rely on inputs in the form of pairwise similarities between objects in the data set. The number of pairwise similarities grows quadratically in the size of the data set which poses a challenge in terms of scalability. One way to achieve practical efficiency for similarity-based techniques is to sparsify the similarity matrix. However, existing sparsification approaches consider the complete similarity matrix and remove some of the non-zero entries. This requires quadratic time and storage and is thus intractable for large-scale data sets. We introduce here a method called sparse computation that generates a sparse similarity matrix which contains only relevant similarities without computing first all pairwise similarities. The relevant similarities are identified by projecting the data onto a low-dimensional space in which groups of objects that share the same grid neighborhood are deemed of potential high similarity whereas pairs of objects that do not share a neighborhood are considered to be dissimilar and thus their similarities are not computed. The projection is performed efficiently even for massively large data sets. We apply sparse computation for the K-nearest neighbors algorithm (KNN), for graph-based machine learning techniques of supervised normalized cut and K-supervised normalized cut (SNC and KSNC) and for support vector machines with radial basis function kernels (SVM), on realworld classification problems. Our empirical results show that the approach achieves a significant reduction in the density of the similarity matrix, resulting in a substantial reduction in tuning and testing times, while having a minimal effect (and often none) on accuracy. The low-dimensional projection is of further use in massively large data sets where the grid structure allows to easily identify groups of “almost identical” objects. Such groups of objects are then replaced by representatives, thus- reducing the size of the matrix. This approach is effective, as illustrated here for data sets comprising up to 8.5 million objects.","","","10.1109/TBDATA.2016.2576470","10.13039/100006445 - Center for Hierarchical Manufacturing National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486113","$K$ -nearest neighbor algorithm;Big data;data mining;similarity-based machine learning;sparsification;supervised normalized cut;support vector machines","Big data;Clustering algorithms;Kernel;Machine learning algorithms;Sparse matrices;Support vector machines;Training","data analysis;data mining;learning (artificial intelligence);pattern classification;pattern matching;radial basis function networks;sparse matrices;support vector machines","K-nearest neighbor algorithm;K-supervised normalized cut;KNN;KSNC;graph-based machine learning techniques;grid structure;large-scale data mining;low-dimensional projection;low-dimensional space;pairwise similarities;radial basis function kernels;similarity-based techniques;sparse computation;sparse similarity matrix;support vector machines","","2","","41","","","20160607","June 1 2016","","IEEE","IEEE Journals & Magazines"
"Cloud-Based Machine Learning Tools for Enhanced Big Data Applications","A. Cuzzocrea; E. Mumolo; P. Corona","ICAR, Univ. of Calabria, Cosenza, Italy","2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing","20150709","2015","","","908","914","We propose Cloud-based machine learning tools for enhanced Big Data applications, where the main idea is that of predicting the ""next"" workload occurring against the target Cloud infrastructure via an innovative ensemble-based approach that combine the effectiveness of different well-known classifiers in order to enhance the whole accuracy of the final classification, which is very relevant at now in the specific context of Big Data. So-called workload categorization problem plays a critical role towards improving the efficiency and the reliability of Cloud-based big data applications. Implementation-wise, our method proposes deploying Cloud entities that participate to the distributed classification approach on top of virtual machines, which represent classical ""commodity"" settings for Cloud-based big data applications. Preliminary experimental assessment and analysis clearly confirm the benefits deriving from our classification framework.","","Electronic:978-1-4799-8006-2; POD:978-1-4799-8007-9","10.1109/CCGrid.2015.170","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7152575","","Benchmark testing;Big data;Discrete cosine transforms;Hidden Markov models;Machine learning algorithms;Training;Virtual machining","Big Data;cloud computing;learning (artificial intelligence);pattern classification;reliability;virtual machines","cloud entities;cloud infrastructure;cloud-based big data applications;cloud-based machine learning tools;enhanced Big Data applications;innovative ensemble-based approach;virtual machines;workload categorization problem","","0","","50","","","","4-7 May 2015","","IEEE","IEEE Conferences"
"Particle swarm optimization for large-scale clustering on apache spark","M. Sherar; F. Zulkernine","School of Computing, Queen's University, Kington, Canada","2017 IEEE Symposium Series on Computational Intelligence (SSCI)","20180208","2017","","","1","8","We present a particle swarm optimization (PSO) clustering algorithm implemented in Apache Spark to achieve parallel big data clustering. Apache Spark is an in-memory big data analytics framework which uses parallel distributed processing to analyze large amount of data faster than most other existing data analytic tools. Spark's library of data analytic functions does not include the PSO algorithm. PSO is an evolutionary computing technique that has shown to produce more compact clusters than other partitional clustering techniques for a wide range of data. In addition PSO is a paralellizable and customizable algorithm well suited for multi-objective clustering problems. In this paper we present our implementation of a hybrid K-Means PSO (KMPSO) clustering algorithm in Apache Spark and demonstrate the performance gained in Spark by comparing our implementation with an implementation of KMPSO in MATLAB. We demonstrate that KMPSO can produce better clustering results than Spark's built-in clustering algorithms, and that Apache Spark enables efficient scaling of resources to handle large and complex workloads.","","Electronic:978-1-5386-2726-6; POD:978-1-5386-2727-3","10.1109/SSCI.2017.8285208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8285208","","Algorithm design and analysis;Clustering algorithms;Convergence;Machine learning algorithms;Optimization;Particle swarm optimization;Sparks","Big Data;Matlab;data analysis;evolutionary computation;parallel processing;particle swarm optimisation;pattern clustering","Apache Spark;KMPSO;MATLAB;PSO clustering algorithm;Spark library;clustering results;compact clusters;data analytic functions;data analytic tools;evolutionary computing technique;in-memory big data analytics framework;large-scale clustering;multiobjective clustering problems;parallel big data clustering;parallel distributed processing;particle swarm optimization clustering algorithm;partitional clustering techniques","","","","","","","","Nov. 27 2017-Dec. 1 2017","","IEEE","IEEE Conferences"
"An Improvement to Data Service in Cloud Computing with Content Sensitive Transaction Analysis and Adaptation","C. W. Lu; C. M. Hsieh; C. H. Chang; C. T. Yang","Hsiuping Univ. of Sci. & Technol., Taichung, Taiwan","2013 IEEE 37th Annual Computer Software and Applications Conference Workshops","20130923","2013","","","463","468","Currently, cloud computing is one of the significant focuses to the modern ICT technology and service for enterprise applications. Through the advantage of usage of resource visualization, parallel processing, access control, and data service integration with scalable virtual machines, cloud computing can not only reduces the cost and barrier for the automation and computerization to the individuals and enterprises, but also promise lower IT cost, efficient management, high capability for data and user accesses. The virtual machine management and reduction to the corresponding operation overhead, related to virtual machine deploying and clustering, has become the essential issue to the cloud computing comprehensively. And effective and efficient data service has become the key to the bottleneck problem, especially in the cloud environment intermixing with the nature of big data. In this paper, we propose an improvement to data Service in cloud computing with content sensitive transaction analysis and adaptation, which is named ADSC (Adaptive Data Service Coordinator). ADSC manages and monitors the query sequence consisting of data requirement transactions collected from clients/users to the data service virtual machines with big data. Through analyzing with a machine learning awareness algorithm using theory of Fuzzy ART, ADSC detects the similarity, redundancy, and localization of the data accesses, then improve the following transactions by reordering the query sequence or even the virtual machine service re-deployment. ADSC is proposed to benefit enterprise cloud application with more efficient big data and Big Table operation.","","Electronic:978-1-4799-2159-1; POD:978-1-4799-2160-7","10.1109/COMPSACW.2013.72","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605834","DaaS;Fuzzy ART;big data;cloud computing;content sensitive Analysis;data service virtual machine","Algorithm design and analysis;Cloud computing;Data handling;Data storage systems;Information management;Subspace constraints;Virtual machining","cloud computing;data analysis;learning (artificial intelligence);virtual machines","ADSC;Big Table operation;ICT technology;access control;adaptive data service coordinator;cloud computing;content sensitive transaction analysis;data requirement transactions;data service;data service integration;fuzzy ART;parallel processing;query sequence;resource visualization;virtual machine management","","3","","18","","","","22-26 July 2013","","IEEE","IEEE Conferences"
"A multilevel deep learning method for big data analysis and emergency management of power system","X. Z. Wang; J. Zhou; Z. L. Huang; X. L. Bi; Z. Q. Ge; L. Li","East China Electric Power Dispatching and Control Center, East China Grid Co., Ltd., Shanghai 200120, China","2016 IEEE International Conference on Big Data Analysis (ICBDA)","20160714","2016","","","1","5","The general focus of this study is to design a multilevel deep learning model that provides big data analytics and emergency management knowledge. A big data covariance analysis approach has been used to find multilevel representations of data based on prior knowledge from large scale power systems. For purpose of meeting requirements of incremental knowledge discovery, an adaptive regression algorithm is presented. Given the multilevel operating status and development trend of power system, the emergency management techniques are then proposed to produce intelligent decision making support. In this paper, a multilevel clustered hidden Markov model based global optimization approach is considered for power system emergency management problem, which is an extension of the conventional optimal power flow problem. The objective is defined to generate operation mode that minimizes multilevel cost while satisfying different constraints. To demonstrate the effectiveness of the presented approach, this paper carefully compared the discriminatory power of knowledge discovery models that utilize deep learning with dimensionality reduction based method and machine learning without dimensionality reduction based method. The experimental results showed that the proposed multilevel deep learning approach consistently outperformed the traditional machine learning method. The emergency management of large scale power system may also benefit from the modified hidden Markov model and global optimization.","","CD-ROM:978-1-4673-9589-2; Electronic:978-1-4673-9591-5; POD:978-1-4673-9592-2","10.1109/ICBDA.2016.7509811","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7509811","Big data;Emergency management;Machine learning;Power system modeling;Smart grid","Algorithm design and analysis;Big data;Emergency services;Hidden Markov models;Kernel;Machine learning;Power system stability","Big Data;covariance analysis;data analysis;data mining;decision making;emergency management;hidden Markov models;learning (artificial intelligence);load flow;minimisation;power engineering computing;power systems;regression analysis","adaptive regression algorithm;big data covariance analysis approach;incremental knowledge discovery;intelligent decision making support;multilevel clustered hidden Markov model based global optimization;multilevel cost minimization;multilevel deep learning method;optimal power flow problem;power system emergency management;power system multilevel operating status","","","","","","","","12-14 March 2016","","IEEE","IEEE Conferences"
"Understanding daily mobility patterns in urban road networks using traffic flow analytics","I. Laña; J. Del Ser; I. I. Olabarrieta","TECNALIA RESEARCH & INNOVATION, 48170 Derio, Bizkaia, Spain","NOMS 2016 - 2016 IEEE/IFIP Network Operations and Management Symposium","20160704","2016","","","1157","1162","The MoveUs project funded by the European Commission aims to foster sustainable eco-friendly mobility habits in cities. In this context predicting the traffic flow is useful for managers to optimize the configuration of the road network towards reducing the congestions and ultimately, the pollution. With the explosion of the so-called Big Data concept and its application to traffic data, a wide range of traffic flow prediction methods has been reported in the related literature. However, most of the efforts in this field have been hitherto focused on short-term prediction models. This paper analyzes how to properly characterize traffic flow in urban road scenarios with an emphasis on the long term. To this end a clustering stage is utilized to discover typicalities or patterns within the traffic flow data registered by each road sensor, which permits building prediction models for each of such discovered patterns. These individual prediction models are intended to become part of the MoveUs platform, which will provide the technical means 1) for traffic managers to analyze in depth the status of the road network, and 2) for road users to better plan their trips.","","Electronic:978-1-5090-0223-8; POD:978-1-5090-0224-5","10.1109/NOMS.2016.7502980","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7502980","Long-term traffic flow prediction;machine learning;mobility patterns","Biological system modeling;Conferences;Data models;Predictive models;Roads;Urban areas","Big Data;intelligent transportation systems;road traffic","Big Data concept;European Commission;MoveUs project;clustering stage;daily mobility patterns;road sensor;sustainable eco-friendly mobility habits;traffic data;traffic flow analytics;traffic flow prediction methods;traffic managers;urban road networks","","","","","","","","25-29 April 2016","","IEEE","IEEE Conferences"
"Nonparametric statistical structuring of knowledge systems using binary feature matches","M. Mørup; F. K. Glückstad; T. Herlau; M. N. Schmidt","Technical University of Denmark, Lyngby, Denmark","2014 IEEE International Workshop on Machine Learning for Signal Processing (MLSP)","20141120","2014","","","1","6","Structuring knowledge systems with binary features is often based on imposing a similarity measure and clustering objects according to this similarity. Unfortunately, such analyses can be heavily influenced by the choice of similarity measure. Furthermore, it is unclear at which level clusters have statistical support and how this approach generalizes to the structuring and alignment of knowledge systems. We propose a non-parametric Bayesian generative model for structuring binary feature data that does not depend on a specific choice of similarity measure. We jointly model all combinations of binary matches and structure the data into groups at the level in which they have statistical support. The model naturally extends to structuring and aligning an arbitrary number of systems. We analyze three datasets on educational concepts and their features and demonstrate how the proposed model can both be used to structure each system separately or to jointly align two or more systems. The proposed method forms a promising new framework for the statistical modeling and alignment of structure across an arbitrary number of systems.","1551-2541;15512541","Electronic:978-1-4799-3694-6; POD:978-1-4799-3695-3","10.1109/MLSP.2014.6958905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958905","Bayesian non-parametrics;binary similarity;knowledge structuring;relational modeling","Abstracts;Computers;Market research;Periodic structures","Bayes methods;educational administrative data processing;knowledge representation;pattern clustering;pattern matching","binary feature matches;educational concepts;knowledge systems;nonparametric Bayesian generative model;object clustering;similarity measure","","1","","20","","","","21-24 Sept. 2014","","IEEE","IEEE Conferences"
"Kylix: A Sparse Allreduce for Commodity Clusters","H. Zhao; J. Canny","Comput. Sci. Div., Univ. of California, Berkeley, Berkeley, CA, USA","2014 43rd International Conference on Parallel Processing","20141120","2014","","","273","282","Allreduce is a basic building block for parallel computing. Our target here is ""Big Data"" processing on commodity clusters (mostly sparse power-law data). Allreduce can be used to synchronize models, to maintain distributed datasets, and to perform operations on distributed data such as sparse matrix multiply. We first review a key constraint on cluster communication, the minimum efficient packet size, which hampers the use of direct all-to-all protocols on large networks. Our allreduce network is a nested, heterogeneous-degree butterfly. We show that communication volume in lower layers is typically much less than the top layer, and total communication across all layers a small constant larger than the top layer, which is close to optimal. A chart of network communication volume across layers has a characteristic ""Kylix"" shape, which gives the method its name. For optimum performance, the butterfly degrees also decrease down the layers. Furthermore, to efficiently route sparse updates to the nodes that need them, the network must be nested. While the approach is amenable to various kinds of sparse data, almost all ""Big Data"" sets show power-law statistics, and from the properties of these, we derive methods for optimal network design. Finally, we present experiments showing with Kylix on Amazon EC2 and demonstrating significant improvements over existing systems such as PowerGraph and Hadoop.","0190-3918;01903918","Electronic:978-1-4799-5618-0; POD:978-1-4799-7858-8","10.1109/ICPP.2014.36","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6957236","","Benchmark testing;Big data;Fault tolerance;Machine learning algorithms;Sparse matrices;Topology;Vectors","Big Data;data mining;graph theory;learning (artificial intelligence);matrix multiplication;network theory (graphs);parallel processing;sparse matrices;synchronisation;tree data structures","Amazon EC2;Big Data processing;characteristic Kylix shape;cluster communication;commodity clusters;communication volume;direct all-to-all protocols;distributed datasets;distributed graph mining;low-layers;machine learning;minimum efficient packet size;model synchronization;nested-heterogeneous-degree butterfly;network communication volume;optimal network design;optimum performance;parallel computing;power-law statistics;sparse allreduce;sparse matrix multiply;sparse power-law data;sparse updates;top-layer","","2","","33","","","","9-12 Sept. 2014","","IEEE","IEEE Conferences"
"A novel recommender system for E-commerce","P. M. Chu; S. J. Lee","Department of Electrical Engineering National Sun Yat-sen University Kaohsiung 80424, Taiwan","2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)","20180226","2017","","","1","5","Recommender systems play an important role in human lives nowadays. They have been used in many electronic commercial activities, and are growing more popularly due to the development of IOT (Internet of Things), big data analysis, and machine learning techniques. However, most recommender systems are only based on the user-item rating matrices which are usually very sparse. The lack of information leads to bad recommendation results. Furthermore, a large number of users and items are usually involved in the recommendation process. These extremely high dimensional data may thwart the efficiency of a recommender system. In this paper, we propose a novel method to overcome these problems. Word2Vec is adopted to extract information from the comments users have made on the items bought. Then dimensionality reduction is applied to project the acquired data into a lower dimension space. A clustering algorithm is then used to group the involved items to a small number of clusters. Finally, the recommendation results are generated for each user. The effectiveness of our proposed method is demonstrated by the results of experiments with some real world data sets.","","Electronic:978-1-5386-1937-7; POD:978-1-5386-1938-4; USB:978-1-5386-1936-0","10.1109/CISP-BMEI.2017.8302310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8302310","E-commerce;Machine Learning;Word2Vec;clustering;recommender System","Clustering algorithms;Data mining;Dimensionality reduction;Internet of Things;Principal component analysis;Recommender systems;Sparse matrices","Big Data;data analysis;electronic commerce;learning (artificial intelligence);matrix algebra;pattern clustering;recommender systems;word processing","Word2Vec;big data analysis;clustering algorithm;dimensionality reduction;electronic commercial activities;information extraction;machine learning;recommender system;user-item rating matrices","","","","","","","","14-16 Oct. 2017","","IEEE","IEEE Conferences"
"Feature Ranking Based on Information Gain for Large Classification Problems with MapReduce","E. Zdravevski; P. Lameski; A. Kulakov; B. Jakimovski; S. Filiposka; D. Trajanov","Fac. of Comput. Sci. & Eng., Ss.Cyril & Methodius Univ., Skopje, Macedonia","2015 IEEE Trustcom/BigDataSE/ISPA","20151203","2015","2","","186","191","In classification problems the large number of features can pose a significant challenge from many aspects. This is particularly the case in the context of Big Data. In order to address this issue we propose a distributed and parallel computation of information gain based on MapReduce. The proposed implementation on Hadoop can be used for ranking features of large datasets and furthermore for feature selection. The data-parallelism is achieved by uniformly distributing it using HBase tables with proper row keys. Performance evaluations are made by estimation of the speed-up of multi-node clusters against a one-node cluster. The framework was deployed on a on-premises Hadoop cluster. The results show that by parallelization and distribution of the computations on a cluster significant speedup can be achieved. The main contribution of this paper is that we have demonstrated how the higher level scripting language Pig Latin can be used for writing MapReduce jobs instead of directly writing a separate map and reduce function. Additionally, we have proposed the use of manually pre-splitted HBase tables instead of HDFS files for data fragmentation in order to set the degree of parallelism on a higher level.","","Electronic:978-1-4673-7952-6; POD:978-1-4673-7953-3","10.1109/Trustcom.2015.580","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7345493","HBase;Hadoop;MapReduce;feature ranking;information gain;parallelization","Context;Entropy;Machine learning algorithms;Mathematical model;Measurement;Servers;Writing","Big Data;feature selection;parallel processing;pattern classification;pattern clustering","Big Data;HBase tables;HDFS files;Hadoop cluster;MapReduce jobs;Pig Latin;classification problems;data fragmentation;data-parallelism;distributed computation;feature ranking;feature selection;higher level scripting language;information gain;large datasets;multinode clusters;one-node cluster;parallel computation;parallelization","","3","","31","","","","20-22 Aug. 2015","","IEEE","IEEE Conferences"
"Distributed Adaptive Importance Sampling on graphical models using MapReduce","A. Haque; S. Chandra; L. Khan; C. Aggarwal","The University of Texas at Dallas, Richardson TX, USA","2014 IEEE International Conference on Big Data (Big Data)","20150108","2014","","","597","602","In the case of a graphical model, machine learning algorithms used to evaluate a query can be broadly classified into exact and approximate inference algorithms. Exact inference algorithms use only network parameters to evaluate a query. However, these algorithms are typically intractable on large networks due to exponential time and space complexity. Approximate inference algorithms are widely used in practice to overcome this constraint, with a trade-off in accuracy. It includes sampling and propagation-based algorithms. These approximate algorithms may also suffer from scalability issues if applied on large networks, for achieving higher accuracy. To address this challenge, we have designed and implemented several MapReduce-based distributed versions of a specific type of approximate inference algorithm called Adaptive Importance Sampling (AIS). We compare and evaluate the proposed approaches using benchmark networks. Experimental results show that our proposed approaches achieve significant scaleup and speedup compared to the sequential method, while achieving similar accuracy asymptotically.","","Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8","10.1109/BigData.2014.7004280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004280","Adaptive Importance Sampling;Approximate Inference;MapReduce","Accuracy;Approximation algorithms;Graphical models;Indexes;Inference algorithms;Monte Carlo methods;Proposals","importance sampling;inference mechanisms;learning (artificial intelligence)","AIS;MapReduce;approximate inference algorithms;benchmark networks;distributed adaptive importance sampling;exact inference algorithms;exponential time;graphical models;machine learning algorithms;propagation-based algorithms;sequential method;space complexity","","1","","25","","","","27-30 Oct. 2014","","IEEE","IEEE Conferences"
"Genomic variant analysis using distributed in-memory computation framework","T. Döngel; Y. Timar","T&#x00DC;B&#x0130;TAK B&#x0130;LGEM, Bili&#x015F;im Teknolojileri Enstit&#x00FC;s&#x00FC;, Gebze, Kocaeli","2017 International Conference on Computer Science and Engineering (UBMK)","20171102","2017","","","961","966","In bioinformatics studies (eg. rare diseases, population genetics, etc.) variation files are used for high volumes of genetic data. A system that allows the transfer of high-volume genomic variation information, the search for variations on this data, filtering, prioritization, complex queries related to genotypes and hereditary properties will enable bioinformatics researchers to work efficiently on high volume data. We have developed a platform called B3SafirBiyo using scalable, distributed, in-memory computation techniques which enables users to do genomic variant analysis operations to be combined with a user-friendly interface to work on large-scale data and shorten the run-time. This framework also forms a infrastructure for machine learning studies on big genomic variation datasets.","","Electronic:978-1-5386-0930-9; POD:978-1-5386-0931-6","10.1109/UBMK.2017.8093585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8093585","Apache Spark;big data;distributed computing;genomic variation analysis;in-memory computing;scalability","Bioinformatics;DNA;Genomics;Pipelines;Sparks;Yarn","bioinformatics;diseases;genetics;genomics;learning (artificial intelligence);query processing","B3SafirBiyo;big genomic variation datasets;bioinformatics;complex queries;distributed in-memory computation framework;high-volume genomic variation information;large-scale data;machine learning studies;user-friendly interface","","","","","","","","5-8 Oct. 2017","","IEEE","IEEE Conferences"
"Hybrid clustering algorithm for time series data — A literature survey","T. Rajesh; Y. S. Devi; K. V. Rao","JNTUH","2017 International Conference on Big Data Analytics and Computational Intelligence (ICBDAC)","20171019","2017","","","343","347","Mining of Time Series data has an impressive growth of interest in today's world. To provide an indication various implementation mechanisms are studied and summarized the different types of problems identified in existing applications. Clustering time series data is a trouble that has applications in an extensive variety of areas and has recently evoked a large amount of research. Time series data may contain large and outliers. In addition, time series data is a one kind of special data set where attributes have a temporal ordering. Therefore clustering of time series data is a good issue in the data mining process. Different techniques and various clustering algorithms have been proposed to assist clustering of time series data sets also different kinds of non-developmentary optimization techniques and for clustering multivariate in some applications, usually they produces poor efficient results due to the dependency on the initial set of values and their poor performance in manipulating multiple objectives. Sometimes Time series data doesn't contain same length and they usually have missing values, the basic measure Euclidean distance and dynamic time warping cannot be applied for such datasets to measure the similarity of objects.","","Electronic:978-1-5090-6400-7; POD:978-1-5090-6401-4; USB:978-1-5090-6399-4","10.1109/ICBDACI.2017.8070861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8070861","Clustering techniques;Machine Learning Algorithms;Time Series Data;Unsupervised Learning","Big Data;Computational intelligence;Conferences;Handheld computers","data mining;pattern clustering;time series","data mining process;hybrid clustering algorithm;nondevelopmentary optimization techniques;time series data sets","","","","","","","","23-25 March 2017","","IEEE","IEEE Conferences"
"The ARM Scalable Vector Extension","N. Stephens; S. Biles; M. Boettcher; J. Eapen; M. Eyole; G. Gabrielli; M. Horsnell; G. Magklis; A. Martinez; N. Premillieu; A. Reid; A. Rico; P. Walker","ARM","IEEE Micro","20170510","2017","37","2","26","39","This article describes the ARM Scalable Vector Extension (SVE). Several goals guided the design of the architecture. First was the need to extend the vector processing capability associated with the ARM AArch64 execution state to better address the computational requirements in domains such as high-performance computing, data analytics, computer vision, and machine learning. Second was the desire to introduce an extension that can scale across multiple implementations, both now and into the future, allowing CPU designers to choose the vector length most suitable for their power, performance, and area targets. Finally, the architecture should avoid imposing a software development cost as the vector length changes and where possible reduce it by improving the reach of compiler auto-vectorization technologies. SVE achieves these goals. It allows implementations to choose a vector register length between 128 and 2,048 bits. It supports a vector-length agnostic programming model that lets code run and scale automatically across all vector lengths without recompilation. Finally, it introduces several innovative features that begin to overcome some of the traditional barriers to autovectorization.","0272-1732;02721732","","10.1109/MM.2017.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7924233","ARM;HPC;SIMD;SVE;Scalable Vector Extension;VLA;Vector Length Agnostic;autovectorization;data parallelism;high-performance computing;instruction set architecture;predication;scalable vector architecture;vector length agnostic","Computer architecture;Encoding;High performance computing;Programming;Scalability;Software engineering;Vectors","program compilers;programming;vectors","ARM AArch64 execution state;ARM scalable vector extension;CPU designers;SVE;area targets;compiler autovectorization technologies;computational requirements;computer vision;data analytics;high-performance computing;machine learning;performance targets;power targets;vector length;vector processing capability;vector-length agnostic programming model","","1","","","","","","Mar.-Apr. 2017","","IEEE","IEEE Journals & Magazines"
"Large scale author name disambiguation in digital libraries","M. Khabsa; P. Treeratpituk; C. L. Giles","Computer Science and Engineering, The Pennsylvania State University, University Park, PA 16802, USA","2014 IEEE International Conference on Big Data (Big Data)","20150108","2014","","","41","42","Person name disambiguation is essential to distinguish between persons that share the same name where unique identifiers are not present. In many domains this is a common problem including digital libraries where the same name can refer to multiple unique authors. Correctly attributing work and citations requires the digital library's database to be disambiguated. In this work we describe a large scale framework for disambiguating author names efficiently and effectively. The framework uses a density based clustering algorithm with a random forest based distance function to clusters unique authors. Effective use of blocking functions allows the clustering algorithm to be run in parallel. In our experiments we show that the framework disambiguates authors of more than 4 million papers in 24 hours.","","Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8","10.1109/BigData.2014.7004487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004487","Clustering;Disambiguation","Clustering algorithms;Computer science;Databases;Educational institutions;Libraries;Machine learning algorithms;Noise","digital libraries;pattern clustering","author name disambiguation;blocking functions;clustering algorithm;digital libraries;distance function","","4","","5","","","","27-30 Oct. 2014","","IEEE","IEEE Conferences"
"Encrypted Gradient Descent Protocol for Outsourced Data Mining","F. Liu; W. K. Ng; W. Zhang","Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore","2015 IEEE 29th International Conference on Advanced Information Networking and Applications","20150430","2015","","","339","346","With the push of cloud computing which has both resource and compute scalability, data, which has been exploding in the past years, are often outsourced to a server. To this end, secure and efficient data processing and mining on outsourced private database becomes a primary concern for users. Among different secure data mining and machine learning algorithms, gradient descent method, as a widely used optimization paradigm, aims at approximating a target function to reach a local minimum, which is always deemed as a decision model to be discovered. In existing methods, users are assumed to hold and process their own data, and all users follow a secure protocol to perform gradient descent algorithm. However, such methods are not applicable to a cloud platform since that data is outsourced to a centralized server after encryption. To address this problem, we propose an Encrypted Gradient Descent Protocol (EGDP) in this paper. In EGDP, both users and server perform collaborative operations to learn and approximate the target function without violating data privacy. We formally proved that EGDP is secure and can return correct result.","1550-445X;1550445X","Electronic:978-1-4799-7905-9; POD:978-1-4799-7906-6","10.1109/AINA.2015.204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7097989","cloud computing;gradient descent method;outsourced data;protocol;secure data mining;stochastic approach","Data mining;Data models;Encryption;Protocols;Public key;Servers","cloud computing;cryptographic protocols;data mining;data privacy;gradient methods;learning (artificial intelligence);optimisation","EGDP;centralized server;cloud computing;cloud platform;collaborative operations;compute scalability;encrypted gradient descent protocol;gradient descent method;machine learning algorithms;optimization paradigm;outsourced data mining;outsourced private database;resource scalability;secure data mining;secure protocol","","0","1","25","","","","24-27 March 2015","","IEEE","IEEE Conferences"
"Mobile Multi-agent Systems for the Internet-of-Things and Clouds Using the JavaScript Agent Machine Platform and Machine Learning as a Service","S. Bosse","Dept. of Math. & Comput. Sci., Univ. of Bremen, Bremen, Germany","2016 IEEE 4th International Conference on Future Internet of Things and Cloud (FiCloud)","20160926","2016","","","244","253","The Internet-of-Things (IoT) gets real in today's life and is becoming part of pervasive and ubiquitous computing networks offering distributed and transparent services. A unified and common data processing and communication methodology is required to merge the IoT, sensor networks, and Cloud-based environments seamless, which can be fulfilled by the mobile agent-based computing paradigm, discussed in this work. Currently, portability, resource constraints, security, and scalability of Agent Processing Platforms (APP) are essential issues for the deployment of Multi-agent Systems (MAS) in strong heterogeneous networks including the Internet, addressed in this work. To simplify the development and deployment of MAS it would be desirable to implement agents directly in JavaScript, which is a well known and public widespread used programming language, and JS VMs are available on all host platforms including WEB browsers. The novel proposed JS Agent Machine (JAM) is capable to execute AgentJS agents in a sandbox environment with full run-time protection and Machine learning as a service. Agents can migrate between different JAM nodes seamless preserving their data and control state by using a on-the-fly code-to-text transformation in an extended JSON+ format. A Distributed Organization System (DOS) layer provides JAM node connectivity and security in the Internet, completed by a Directory-Name Service offering an organizational graph structure. Agent authorization and platform security is ensured with capability-based access and different agent privilege levels.","","","10.1109/FiCloud.2016.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7575871","Agent Platforms;Agents;Cloud Computing;IoT","Cloud computing;Computer architecture;Internet of things;Mobile communication;Security;Sensors","Internet of Things;Java;authorisation;cloud computing;data protection;graph theory;learning (artificial intelligence);mobile agents;mobile computing;multi-agent systems;online front-ends;virtual machines","APP;AgentJS agents;DOS layer;Internet-of-Things;IoT;JAM node connectivity;JAM node security;JS VM;JS agent machine;JavaScript agent machine platform;MAS;Web browsers;agent authorization;agent privilege levels;agent processing platforms;capability-based access;cloud-based environment;communication methodology;control state;data preservation;data processing;directory-name service;distributed organization system layer;extended JSON+ format;heterogeneous networks;host platforms;machine learning-as-a-service;mobile multiagent systems;on-the-fly code-to-text transformation;organizational graph structure;pervasive computing network;platform security;portability constraint;resource constraint;run-time protection;sandbox environment;scalability constraint;security constraint;sensor networks;ubiquitous computing network","","2","","","","","","22-24 Aug. 2016","","IEEE","IEEE Conferences"
"Performance evaluation of in-memory computing on scale-up and scale-out cluster","Taekyung Yoo; Minsub Yim; Ilgyun Jeong; Yunsu Lee; Seung-Tae Chun","Datastreams Corp., Korea","2016 Eighth International Conference on Ubiquitous and Future Networks (ICUFN)","20160811","2016","","","456","461","Apache Spark framework, which is the implementation of Resilient Distributed Datasets(RDD), is used instead of MapReduce on recent data processing models of Hadoop ecosystem. In this paper, we evaluated the performance and resource usage of real world workloads on scale-up and scale-out clusters using the in-memory caching feature of Spark framework. In our experiments, scale-up processed data more efficiently than scale-out in write intensive workloads such as Sort and Scan, whereas scale-out had strength in those utilizing iterative algorithms such as Join, Pagerank and KMeans. Considering the efficiency in physical factors including performance per watt and the physical space each occupies, we show that it is more advantages to use scale up cluster than scale out.","","Electronic:978-1-4673-9991-3; POD:978-1-4673-9992-0; USB:978-1-4673-9990-6","10.1109/ICUFN.2016.7537070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7537070","","Bandwidth;Benchmark testing;Data models;Hardware;Machine learning algorithms;Sparks;Web search","cache storage;iterative methods;pattern clustering;performance evaluation;sorting;storage management","Apache Spark framework;Hadoop ecosystem data processing models;Join;KMeans;Pagerank;RDD;Scan;Sort;in-memory caching feature;in-memory computing;iterative algorithms;performance evaluation;resilient distributed datasets;scale-out cluster;scale-up cluster;write intensive workloads","","","","","","","","5-8 July 2016","","IEEE","IEEE Conferences"
"Driver performance detection & recommender system in vehicular environment using video streaming analytics","H. Venkataraman; R. Assfalg","Center for Smart Cities, Indian Institute of Information Technology (IIIT) Chittoor, Sricity - a research institute of national excellence, India","2017 IEEE International Conference on Advanced Intelligent Mechatronics (AIM)","20170824","2017","","","1","3","This tutorial deals with how the overall human performance while working can be detected, under different conditions and scenarios. The tutorial will talk about how regular and real-time monitoring of people can be carried out through eye-tracking and how it can be integrated with other environment factors to develop a recommender system. Particularly, the speakers would articulate a vehicular driving scenario and explain how a combined use of eye-tracking and face-tracking can not only help the drivers but also significantly assist in reducing the road accidents, thereby increase the road safety. Finally, the speakers will present the use of existing eye-trackers along with the integrated eye-tracker and face-tracker that is under indigenous development. This tutorial is intended for a wide section of audience - ranging from clusters of automobile designers/manufacturers, researchers in mechatronics and students researching on different aspects of connected cars, computer vision, signal processing, image processing and machine learning and data analytics.","","Electronic:978-1-5090-6000-9; POD:978-1-5090-6001-6; Paper:978-1-5090-5998-0; USB:978-1-5090-5999-7","10.1109/AIM.2017.8013985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8013985","","Automobiles;Autonomous automobiles;Engines;Real-time systems;Recommender systems;Tutorials","driver information systems;gaze tracking;real-time systems;recommender systems;road accidents;road safety;video streaming","driver performance detection;environment factors;eye-tracking;face-tracking;human performance;real-time monitoring;recommender system;road accidents;road safety;vehicular driving scenario;vehicular environment;video streaming analytics","","","","","","","","3-7 July 2017","","IEEE","IEEE Conferences"
"Data analytics in smart distribution networks: Applications and challenges","F. C. L. Trindade; L. F. Ochoa; W. Freitas","University of Campinas, Campinas, Brazil","2016 IEEE Innovative Smart Grid Technologies - Asia (ISGT-Asia)","20161226","2016","","","574","579","The large volumes of data that will be produced by ubiquitous sensors and meters in future smart distribution networks represent an opportunity for the use of data analytics to extract valuable knowledge and, thus, improve Distribution Network Operator (DNO) planning and operation tasks. Indeed, applications ranging from outage management to detection of non-technical losses to asset management can potentially benefit from data analytics. However, despite all the benefits, each application presents DNOs with diverse data requirements and the need to define an adequate approach. Consequently, it is critical to understand the different interactions among applications, monitoring infrastructure and approaches involved in the use of data analytics in distribution networks. To assist DNOs in the decision making process, this work presents some of the potential applications where data analytics are likely to improve distribution network performance and the corresponding challenges involved in its implementation.","","Electronic:978-1-5090-4303-3; POD:978-1-5090-5228-8; USB:978-1-5090-4302-6","10.1109/ISGT-Asia.2016.7796448","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7796448","Data analytics;distribution networks;operation;planning","Classification algorithms;Clustering algorithms;Data analysis;Machine learning algorithms;Monitoring;Planning;Prediction algorithms","asset management;data analysis;power distribution economics;power distribution planning;power engineering computing","DNO operation task improvement;asset management;data analytics;decision making process;monitoring infrastructure;nontechnical loss detection;outage management;smart distribution network operator planning improvement;valuable knowledge extraction","","","","","","","","Nov. 28 2016-Dec. 1 2016","","IEEE","IEEE Conferences"
"An overview of free software tools for general data mining","A. Jovic; K. Brkic; N. Bogunovic","Dept. of Electron., Microelectron., Comput. & Intell. Syst., Univ. of Zagreb, Zagreb, Croatia","2014 37th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)","20140724","2014","","","1112","1117","This expert paper describes the characteristics of six most used free software tools for general data mining that are available today: RapidMiner, R, Weka, KNIME, Orange, and scikit-learn. The goal is to provide the interested researcher with all the important pros and cons regarding the use of a particular tool. A comparison of the implemented algorithms covering all areas of data mining (classification, regression, clustering, associative rules, feature selection, evaluation criteria, visualization, etc.) is provided. In addition, the tools' support for the more advanced and specialized research topics (big data, data streams, text mining, etc.) is outlined, where applicable. The tools are also compared with respect to the community support, based on the available sources. This multidimensional overview in the form of expert paper on data mining tools emphasizes the quality of RapidMiner, R, Weka, and KNIME platforms, but also acknowledges the significant advancements made in the other tools.","","CD-ROM:978-953-233-081-6; Electronic:978-953-233-077-9; POD:978-1-4799-5657-9","10.1109/MIPRO.2014.6859735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6859735","","Big data;Communities;Data mining;Data visualization;Graphical user interfaces;Machine learning algorithms;Vegetation","data mining;public domain software","DM;KNIME;Orange;R;RapidMiner;Weka;free software tools;general data mining;scikit-learn","","16","","9","","","","26-30 May 2014","","IEEE","IEEE Conferences"
"Machine learning approach for predicting end price of online auction","M. R. Khadge; M. V. Kulkarni","Dept of Computer Science and Engineering, Vishwakarma Institute of Technology, Pune, India","2016 International Conference on Inventive Computation Technologies (ICICT)","20170126","2016","3","","1","5","An online auction is an auction held over the internet. In today's era of the internet, economies have changed the way than they were a few years back. The scope and reach of online auctions have been propelled by internet to the prominent level because online auctions break down and remove the physical limitations of traditional auctions such as geography, presence, time, space, etc. As online auction has become one of the fastest growing modes of online commerce transaction, sellers and buyers have started preferring to go online for purchasing and selling products respectively. As eBay has been the leading online auction marketplace for nearly two decades, the proposed system collected huge auction data from eBay and machine learning algorithms are used to predict end price of auction items. The proposed system is trained with 70% of dataset and 30% of dataset is used for testing. The proposed system used Naive Bayes which gives 99.33% accuracy in classification of whether the item will sell or not and kernel mapping SVM gives 96.3% accuracy for predicting whether an item maximize profit or not.","","DVD:978-1-5090-1283-1; Electronic:978-1-5090-1285-5; POD:978-1-5090-1286-2","10.1109/INVENTIVE.2016.7830232","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7830232","Machine learning;Naive Bayes(NB);Support Vector Machine(SVM);Uniform Prior Naive Bayes(UPNB);k-Nearest Neighbour(k-NN)","Clustering algorithms;Decision trees;Feature extraction;Internet;Machine learning algorithms;Prediction algorithms;Support vector machines","Bayes methods;Internet;electronic commerce;learning (artificial intelligence);pricing;purchasing;retail data processing;support vector machines;transaction processing","Internet;end price prediction;kernel mapping SVM;machine learning;naive Bayes;online auction;online commerce transaction;product purchasing;product selling","","","","","","","","26-27 Aug. 2016","","IEEE","IEEE Conferences"
"Fraud detection in big data using supervised and semi-supervised learning techniques","G. E. Melo-Acosta; F. Duitama-Muñoz; J. D. Arias-Londoño","I&S Research Group, Universidad de Antioquia, Calle 67 No. 53 - 108, 050010, Medell&#x00ED;n, Colombia","2017 IEEE Colombian Conference on Communications and Computing (COLCOM)","20171030","2017","","","1","6","This paper addresses the credit card fraud detection problem in the context of Big Data, based on machine learning techniques. In the fraud detection task, typically the available datasets for ML training present some peculiarities, such as the unavoidable condition of a strong class imbalance, the existence of unlabeled transactions, and the large number of records that must be processed. The present paper aims to propose a methodology for automatic detection of fraudulent transactions, that tackle all these problems. The methodology is based on a Balanced Random Forest, that can be used in supervised and semi-supervised scenarios through a co-training approach. Two different schemes for the co-training approach are tested, in order to overcome the class imbalance problem. Moreover, a Spark platform and Hadoop file system support our solution, in order to enable the scalability of the proposed solution. The proposed approach achieves an absolute improvement of around 24% in terms of geometric mean in comparison to a standard random forest learning strategy.","","Electronic:978-1-5386-1060-2; POD:978-1-5386-1061-9; USB:978-1-5386-1059-6","10.1109/ColComCon.2017.8088206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8088206","Fraud Detection;Machine Learning;Random Forest;Semi-supervised Learning;Spark","Credit cards;Radio frequency;Semisupervised learning;Sparks;Support vector machines;Training","credit transactions;data handling;fraud;learning (artificial intelligence);random processes;security of data;transaction processing","Balanced Random Forest;Big Data;Hadoop file system support;ML training;Spark platform;class imbalance problem;credit card fraud detection problem;fraudulent transactions automatic detection;machine learning techniques;semisupervised learning techniques;semisupervised scenarios;standard random forest learning strategy;strong class imbalance;unlabeled transactions","","","","","","","","16-18 Aug. 2017","","IEEE","IEEE Conferences"
"Scaling historical text re-use","M. Büchler; G. Franzini; E. Franzini; M. Moritz","G&#x00F6;ttingen Centre for Digital Humanities, Georg-August-University G&#x00F6;ttingen, G&#x00F6;ttingen, Germany","2014 IEEE International Conference on Big Data (Big Data)","20150108","2014","","","23","31","Text re-use describes the spoken and written repetition of information. Historical text re-use, with its longer time span, embraces a larger set of morphological, linguistic, syntactic, semantic and copying variations, thus adding complication to text-reuse detection. Furthermore, it increases the chances of redundancy in a digital library. In Natural Language Processing it is crucial to remove these redundancies before we can apply any kind of machine learning techniques to the text. In Humanities, these redundancies foreground textual criticism and allow scholars to identify lines of transmission. Identification can be accomplished by way of automatic or semi-automatic methods. Text re-use algorithms, however, are of squared complexity and call for higher computational power. The present paper addresses this issue of complexity, with a particular focus on its algorithmic implications and solutions.","","Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8","10.1109/BigData.2014.7004449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004449","performance;scalability;text re-use","Big data;Complexity theory;Equations;Force;Joining processes;Libraries;Pragmatics","computational complexity;digital libraries;humanities;learning (artificial intelligence);natural language processing;text analysis","computational power;copying variations;digital library;historical text reuse scaling;humanities;machine learning techniques;natural language processing;semiautomatic methods;squared complexity;text-reuse detection;textual criticism","","0","","38","","","","27-30 Oct. 2014","","IEEE","IEEE Conferences"
"A non-negative multilinear block tensor decomposition approach to flow cytometry data analysis","D. Brie; S. Miron; P. Becuwe; S. Grandemange","Centre de Recherche en Automatique de Nancy, Universit&#x00E9; de Lorraine, CNRS, Campus Sciences B.P. 70239, F-54506 Vandoeuvre-l&#x00E8;s-Nancy, France","2014 IEEE International Workshop on Machine Learning for Signal Processing (MLSP)","20141120","2014","","","1","6","The paper presents a novel approach to the processing of flow cytometry data sequences. It consists in decomposing a sequence of multidimensional probability density functions by using multilinear block tensor decomposition approach [1]. The identifiability of the model is also addressed as well as the data processing. To illustrate the effectiveness of the approach, a study of the T47D cell line mitochondrial membrane potential as a function of the CCCP<sup>1</sup> decoupling agent concentration is performed. The cell sorting capacity of the method is significantly improved as compared to classical clustering methods.","1551-2541;15512541","Electronic:978-1-4799-3694-6; POD:978-1-4799-3695-3","10.1109/MLSP.2014.6958907","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958907","Flow cytometry;Mixture of multivariate probability density functions;non-negative block Candecomp/Parafac decomposition","Approximation methods;Data models;Histograms;Loading;Matrix decomposition;Tensile stress","biocomputing;biomembranes;cellular biophysics;data analysis;probability","CCCP;T47D cell line mitochondrial membrane potential;cell sorting capacity;clustering methods;data processing;decoupling agent concentration;flow cytometry data analysis;flow cytometry data sequences;multidimensional probability density functions;nonnegative multilinear block tensor decomposition approach","","0","","22","","","","21-24 Sept. 2014","","IEEE","IEEE Conferences"
"Automated detection of anomalous shipping manifests to identify illicit trade","A. Sanfilippo; S. Chikkagoudar","Comput. & Stat. Div., Pacific Northwest Nat. Lab., Richland, WA, USA","2013 IEEE International Conference on Technologies for Homeland Security (HST)","20140102","2013","","","529","534","We describe an approach to analyzing anomalies in trade data based on the identification of cluster outliers. The approach uses unsupervised machine learning methods to discover semantically coherent clusters of shipping records in large collections of trade data. Trade data with cluster annotations are then used as input to a supervised machine learning algorithm to train and evaluate a classification model capable of identifying members of each cluster. The evaluation of this classification model provides an assessment of cluster coherence. Outliers are identified for each cluster by measuring the Euclidean distance from each member of the cluster to the cluster centroid, and then selecting a percentile threshold to identify shipping records with extreme distances from the cluster centroid. We describe a specific application of this approach to a dataset of 2.36M records for containerized shipments, with specific reference to the detection of anomalies potentially related to nuclear smuggling. Results show that this approach succeeds in finding semantically coherent clusters of shipping records, and identifying outliers that may help facilitate the detection of illicit trade.","","CD-ROM:978-1-4799-3963-3; Electronic:978-1-4799-1535-4; POD:978-1-4799-3964-0","10.1109/THS.2013.6699059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6699059","classification;clustering;detection of radiological threat materials;illicit trafficking;nuclear smuggling;trade data;visual analytics","Clustering algorithms;Earth;Euclidean distance;Inspection;Materials;Ores;Rocks","business data processing;data analysis;pattern classification;pattern clustering;unsupervised learning","anomalous shipping manifests detection;classification model;cluster centroid;cluster outliers identification;data analysis;illicit trade identification;nuclear smuggling;percentile threshold;shipping records;supervised machine learning algorithm;trade data anomalies;unsupervised machine learning methods","","0","","21","","","","12-14 Nov. 2013","","IEEE","IEEE Conferences"
"Machine learning techniques in Hadoop environment: A survey","S. R. Suthar; V. K. Dabhi; H. B. Prajapati","Department of Information Technology, Dharmsinh Desai University, Nadiad, India","2017 Innovations in Power and Advanced Computing Technologies (i-PACT)","20180104","2017","","","1","8","A plenty of informative data is produced on social sites. Twitter is a widely used social site to express opinion and feelings related to any domain. Tweets can be used to extract relevant and reliable information. Data analysis is an important task for mining these types of large data. The classification of these tweets can be useful to human beings. However, storage and processing of plenty of tweets is a challenging problem. Hadoop Distributed File System (HDFS) can be used to store this data. To extract information and to classify tweets, various data mining techniques can be utilized. The well-known data mining techniques include Clustering and Classification. Traditionally, these algorithms run in the sequential environment. For improving computational performance and address scalability issue, these methods should run in distributed environment. However, distributed environment introduces issues like synchronization, communication overhead, appropriate division of tasks/data, and load imbalance. The problem of synchronization can be handled using Hadoop Map-reduce programming model. This paper aims to discuss parallelism of various data mining algorithms in Hadoop environment.","","Electronic:978-1-5090-5682-8; POD:978-1-5090-5683-5","10.1109/IPACT.2017.8245018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8245018","Big Data;Hadoop;Mahout;Map Reduce","Big Data;Data mining;File systems;Machine learning algorithms;Parallel processing;Support vector machines","data analysis;data mining;distributed databases;information retrieval;learning (artificial intelligence);parallel processing;pattern classification;pattern clustering;social networking (online)","HDFS;Hadoop Distributed File System;Hadoop Map-reduce programming model;Hadoop environment;Twitter;data analysis;data mining algorithms;data mining techniques;distributed environment;information extraction;informative data;machine learning techniques;sequential environment;social sites;tweets classification","","","","","","","","21-22 April 2017","","IEEE","IEEE Conferences"
"Distributed Information-Theoretic Metric Learning in Apache Spark","Y. Su; H. Yang; I. King; M. Lyu","Shenzhen Key Laboratory of Rich Media Big Data Analytics and Applications, Shenzhen Research Institute, The Chinese University of Hong Kong, China","2016 International Joint Conference on Neural Networks (IJCNN)","20161103","2016","","","3306","3313","Distance metric learning (DML) is an effective similarity learning tool to learn a distance function from examples to enhance the model performance in applications of classification, regression, and ranking, etc. Most DML algorithms need to learn a Mahalanobis matrix, a positive semidefinite matrix that scales quadratically with the number of dimensions of input data. This brings huge computational cost in the learning procedure, and makes all proposed algorithms infeasible for extremely high-dimensional data even with the low-rank approximation. Differently, in this paper, we take advantage of the power of parallel computation and propose a novel distributed distance metric learning algorithm based on a state-of-the-art DML algorithm, Information-Theoretic Metric Learning (ITML).More specifically, we utilize the property that each positive semidefinite matrix can be decomposed into a combination of rank-one and trace-one matrices and convert the original sequential training procedure into a parallel one. In most cases, the communication demands of the proposed method are also reduced from O(d<sup>2</sup>) to O(cd), where d is the number of dimensions of the data and c is the number of constraints in DML and can be smaller than d by appropriate selection. Moreover importantly, we present a rigorous theoretical analysis to upper bound the Bregman divergence between the sequential algorithm and the parallel algorithm, which guarantees the correctness and performance of the proposed algorithm. Our experiments on datasets with O(10<sup>5</sup>) features demonstrate the competitive scalability and the performance compared with the original ITML algorithm.","","","10.1109/IJCNN.2016.7727622","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727622","","Algorithm design and analysis;Approximation algorithms;Covariance matrices;Eigenvalues and eigenfunctions;Machine learning algorithms;Matrix decomposition;Measurement","information theory;learning (artificial intelligence);matrix algebra;parallel algorithms","Apache Spark;Bregman divergence;DML algorithms;ITML;Mahalanobis matrix learning;distance function learning;distributed distance metric learning;distributed information-theoretic metric learning;high-dimensional data;low-rank approximation;original sequential training procedure;parallel algorithm;parallel computation;positive semidefinite matrix;rank-one matrices;sequential algorithm;similarity learning tool;trace-one matrices","","","","","","","","24-29 July 2016","","IEEE","IEEE Conferences"
"Deriving Cognitive Map Concepts on the Basis of Social Media Data Clustering","V. S. Kireev","Inst. of Cyber Intell. Syst., Nat. Res. Nucl. Univ. MEPhI, Moscow, Russia","2017 5th International Conference on Future Internet of Things and Cloud Workshops (FiCloudW)","20171120","2017","","","37","40","With data storage and processing technology developing fast, there has been accumulated a great amount of open data that comes from everywhere including social media. One of the promising tools to analyze these data is fuzzy cognitive maps that help to describe connections and substances to reveal patterns, facts and knowledge. One of the problems when creating cognitive maps is the identification of concepts and relations between them, which is usually solved by the expert way. However, this approach is not always effective. This work tries to solve the topical issue of automatic cognitive map building based on cluster analysis methods. The article features both the proposed cognitive map building algorithm description and the results of Twitter data-based experiments.","","Electronic:978-1-5386-3281-9; POD:978-1-5386-3282-6","10.1109/FiCloudW.2017.103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8113767","automated data science;cluster analysis;cognitive maps concepts derivation;fuzzy cognitive maps;machine learning;social media;web mining","Clustering algorithms;Data mining;Data models;Fuzzy cognitive maps;Organizations;Twitter","cognition;data analysis;fuzzy set theory;pattern clustering;social networking (online)","Twitter data;cluster analysis methods;cognitive map building algorithm;cognitive map concepts;data analysis;data clustering;data processing;data storage;fuzzy cognitive maps;open data;social media","","","","","","","","21-23 Aug. 2017","","IEEE","IEEE Conferences"
"Sentiment classification on big data using Naïve bayes and logistic regression","A. Prabhat; V. Khullar","Department of Computer Sciences and Engineering, CT Institute of Engineering, Management and Technology, Shahpur, Jalandhar, India","2017 International Conference on Computer Communication and Informatics (ICCCI)","20171123","2017","","","1","5","The huge expansion of world wide web has involved a contemporary fashion of conveying the attitude or viewpoint of human being. It is a channel where anybody any visualize opinion and sentiments of different customers. It is also possible to see opinion classified into different categories and ratings given on different products. This information plays a supreme role in sentiment classification task. The huge amount of data stored online can be mined effectively to extract valuable information and do decision based on extracted information. The real time Twitter reviews are feed to different supervised machine learning classifier. After training the classification is carried out by various classifiers. The tweets as categorized as positive or, negative. In this paper we have used Naïve Bayes and Logistic Regression for the classification of Twitters reviews. The performance of algorithms has been evaluated on the basis of different parameter like accuracy, precision and throughput.","","CD:978-1-4673-8853-5; Electronic:978-1-4673-8855-9; POD:978-1-4673-8856-6","10.1109/ICCCI.2017.8117734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8117734","Big Data;Logistic Regression;Naïve Bayes;Sentiment Classification","Classification algorithms;Clustering algorithms;File systems;Logistics;Machine learning algorithms;Text categorization","Bayes methods;data mining;learning (artificial intelligence);pattern classification;regression analysis;social networking (online)","Big Data;Naive Bayes;Twitters reviews;World Wide Web;information extraction;logistic regression;opinion visualization;sentiment classification task;supervised machine learning classifier","","","","","","","","5-7 Jan. 2017","","IEEE","IEEE Conferences"
"B3SafirBiyo: Genomic variant analysis with big data technologies","T. Döngel; Y. Timar","T&#x00FC;B&#x0130;TAK B&#x0130;LGEM, Information Technologies Institute, Gebze, Kocaeli, Turkey","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","1","9","In this information age, the DNA information itself, as well as the genomic variations of individuals are popular examples of big data to be processed. In case of analyzing thousands of individuals, the size of the data set is getting so much larger which requires big data processing technologies. In order to support the studies in bioinformatics, specifically on genomic variants and population genetics, we have implemented B3SafirBiyo, a framework with the recent big data technologies; web-based user interfaces, Spark engine and machine learning libraries. We have demonstrated the efficiency of basic filtering, querying operations on large variant files. The performance of the population clustering on 1000 genome dataset is also presented in this work.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258137","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258137","Apache Spark;big data;distributed computing;genomic variation analysis;genomic variation clustering;in-memory computing;population genetics;scalability","Big Data;Bioinformatics;Genomics;Sociology;Sparks;Statistics","Big Data;DNA;bioinformatics;data mining;genetics;genomics;learning (artificial intelligence);medical computing;user interfaces","B3SafirBiyo;Big Data processing technologies;DNA information;data set;genome dataset;genomic variant analysis;genomic variants;information age;machine learning libraries;population genetics;variant files;web-based user interfaces","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
